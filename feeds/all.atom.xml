<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>NickNagel.com</title><link href="https://dr-nick-nagel.github.io/" rel="alternate"></link><link href="https://dr-nick-nagel.github.io/feeds/all.atom.xml" rel="self"></link><id>https://dr-nick-nagel.github.io/</id><updated>2025-04-07T00:00:00-04:00</updated><entry><title>Tracking Trump Administration Tariff Policy</title><link href="https://dr-nick-nagel.github.io/blog/trump-tariffs.html" rel="alternate"></link><published>2025-04-07T00:00:00-04:00</published><updated>2025-04-07T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2025-04-07:/blog/trump-tariffs.html</id><summary type="html">&lt;p&gt;My dashboard for tracking the economic impacts of Trump administration tariffs...&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Purpose&lt;/h1&gt;
&lt;p&gt;The purpose of this page is to track the ongoing impacts of the Trump administration tariffs which were announced in April 2025. &lt;/p&gt;
&lt;h1&gt;Rationale&lt;/h1&gt;
&lt;p&gt;On April 2, 2025, president Donald Trump signed an executive order imposing a minimum 10% tariff on all U.S. imports, with higher rates applied to upwards of 57  nations. This amounts to a huge perturbation to the &lt;em&gt;World Wide Economy&lt;/em&gt;. As shown in the following figure the trade-weighted average tariff rose from 2% to an estimated 24% in an incredibly small time period. This broad application of tariffs marked a significant escalation in U.S. trade protectionism the likes of which have not been seen since the Great Depression. &lt;/p&gt;
&lt;iframe title="US Average Effective Tariff Rate Since 1850" aria-label="Interactive line chart" id="datawrapper-chart-FXfCZ" src="https://datawrapper.dwcdn.net/FXfCZ/1/" scrolling="no" frameborder="0" style="width: 0; min-width: 100% !important; border: none;" height="389" data-external="1"&gt;&lt;/iframe&gt;
&lt;script type="text/javascript"&gt;!function(){"use strict";window.addEventListener("message",(function(a){if(void 0!==a.data["datawrapper-height"]){var e=document.querySelectorAll("iframe");for(var t in a.data["datawrapper-height"])for(var r,i=0;r=e[i];i++)if(r.contentWindow===a.source){var d=a.data["datawrapper-height"][t]+"px";r.style.height=d}}}))}();
&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; US average effective tariff rate since 1850. Source: &lt;a href="https://budgetlab.yale.edu/research/state-us-tariffs-week-april-7-2025"&gt;Yale Budget Lab&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;The formula used by the administration to apply the tariffs [first deduced by journalists and later published by the Office of the United States Trade Representative (OUSTR)] is as follows:&lt;/p&gt;
&lt;p&gt;$$
\Delta \tau _i = \frac{x_i - m_i}{\varepsilon \times \varphi \times m_i } 
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\Delta \tau$ = The amount of tariff increase (for each country)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$x_i$ = US exports to that country &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$m_i$ = US imports to that country &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\varepsilon$ = The price elasticity of import demand&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\varphi$ = The elasticity of import prices with respect to tariffs&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;According to the OUSTR the $\varepsilon$ and $\varphi$ values were set to 4 and 0.25 respectively which effectively cancel one another out (the product of the two amounts to 1) which has zero effect on the denominator. Also, the &lt;em&gt;minimum&lt;/em&gt; tariff for a given country was floored at 10% &lt;em&gt;regardless of the value determined by the formula&lt;/em&gt;. So, in essence, the formula amounts to imposing a tariff equivalent to the proportion of the trade deficit with any nation divided by overall imports from that nation, or, a flat 10% tariff where the proportion falls below 0.1. To me, this approach amounts to applying a rather blunt instrument to address concerns about the US trade deficit. &lt;/p&gt;
&lt;p&gt;The blanket approach to imposing tariffs world-wide is bound to affect &lt;em&gt;everybody everywhere&lt;/em&gt;. Economists have warned of inevitable price increases for US citizens and strong probabilities of trending the economy toward recession. And so, mainly out of concern for the consequences of this economic policy I've put together this page in order to track the economic impacts of the policy going forward. &lt;/p&gt;
&lt;h1&gt;TTT Dashboard&lt;/h1&gt;
&lt;p&gt;I'm not purporting to be an economist but off the top of my head it seems that a fair set of indicators to watch should include market reactions, inflation and GDP -- broad indicators of economic health. So to start off the analysis I've added feeds to display the S&amp;amp;P index, CPI and GDP. Later I'll come back and look at employment numbers and trade balances. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;S&amp;amp;P 500&lt;/strong&gt;. The S&amp;amp;P 500 is a commonly used indicator of U.S. Market performance embodying a broad market view with the 500 largest capitalized U.s. companies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CPI&lt;/strong&gt;. The CPI (Consumer Price Index) is a measure of change in prices over time ( $CPI_t = \frac{C_t}{C_0} \times 100$ ) and as such, a good indicator of inflation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GDP&lt;/strong&gt; . The GDP (Gross Domestic Product) is defined as the total monetary value of all the goods and services produced by a country over the course of a specified period. Here I'll be looking at quarterly GDP tied to 2017 USD.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For direct comparison I've clamped the starting point of the time series for the chosen indicators to January of 2018.&lt;/p&gt;
&lt;style&gt;
.Chart {
    margin-bottom: 10px
}
&lt;/style&gt;

&lt;div class='Chart'&gt;
  &lt;canvas id='snpChart' 
          width='400' 
          height='200' 

          &gt;
  &lt;/canvas&gt;
&lt;/div&gt;

&lt;div  class='Chart'&gt;
  &lt;canvas id='cpiChart' 
          width='400' 
          height='200' &gt;
  &lt;/canvas&gt;
&lt;/div&gt;

&lt;div  class='Chart'&gt;
  &lt;canvas id='gdpChart' 
          width='400' 
          height='200' &gt;
  &lt;/canvas&gt;
&lt;/div&gt;

&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;In the past I've been skeptical with regard to the extent to which U.S. presidents can impact the economy. The &lt;em&gt;World Wide Economy&lt;/em&gt; is a vastly complex system and it's hard to imagine that any one individual could have such significant impacts. But the Trump Tariffs have absolutely opened my eyes and proved my skepticism wrong. Here is a single individual who with a single order has imposed a policy that is bound to affect &lt;em&gt;everybody everywhere&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;Immediately out of the gate, as the S&amp;amp;P data shows, the markets don't seem to like the tariffs. Leading companies (e.g., Apple, Google, etc.) saw trillions of dollars evaporate almost overnight. Economists have warned of inevitable price increases for US citizens and strong possibilities of trending the economy toward recession. Initial market indications seem to bear those fears out as companies around the world reel from the implications of the 2025 trade wars. &lt;/p&gt;
&lt;p&gt;Will this perturbation result in increased economic difficulties for American households? In the near term almost certainly the answer is yes. As noted in a &lt;a href="https://budgetlab.yale.edu/research/state-us-tariffs-week-april-7-2025"&gt;Yale Budget Lab analysis&lt;/a&gt; &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tariffs are a regressive tax, especially in the short-run. This means that tariffs burden households at the bottom of the income ladder more than those at the top as a share of income. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tariffs are essentially taxes on materials and goods that have to be paid somewhere whether by producers or consumers of the materials or finished goods subject to the tariffs. And when tariffs are applied across the board in the manner executed on April 2nd it's hard to find a product that &lt;em&gt;isn't&lt;/em&gt; directly affected. &lt;/p&gt;
&lt;p&gt;But what will be the long term consequences? The answer is only time will tell. That's why I created this dashboard; it behooves us all to pay very careful attention to the indicators and tie them to the fiscal policy of an administration that appears determined to reinvigorate trade protectionism at any cost.&lt;/p&gt;
&lt;h1&gt;Resources and References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://ustr.gov/issue-areas/reciprocal-tariff-calculations"&gt;Office of the United States Trade Representative: Reciprocal Tariff Calculations &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://budgetlab.yale.edu/research/state-us-tariffs-week-april-7-2025"&gt;State of U.S. Tariffs: Week of April 7, 2025&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://budgetlab.yale.edu/research/state-us-tariffs-week-april-7-2025"&gt;Yale Budget Lab analysis&lt;/a&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.datawrapper.de/"&gt;Data Wrapper&lt;/a&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://fred.stlouisfed.org/docs/api/fred/"&gt;FRED &amp;reg; API&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/w/index.php?title=Tariffs_in_the_second_Trump_administration"&gt;Wikipedia: Tariffs in the second Trump administration&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.reuters.com/markets/us/jpmorgan-ceo-dimon-warns-tariffs-could-slow-us-growth-fuel-inflation-2025-04-07/"&gt;Dimon warns of economic impact of trade war, sees possible recession&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script src='https://cdn.jsdelivr.net/npm/chart.js' &gt;&lt;/script&gt;
&lt;script src="https://cdn.jsdelivr.net/npm/luxon@3/build/global/luxon.min.js"&gt;&lt;/script&gt;
&lt;script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-luxon@1"&gt;&lt;/script&gt;</content><category term="Published"></category><category term="economics"></category><category term="tariffs"></category><category term="trump administration"></category></entry><entry><title>SVG Artworks</title><link href="https://dr-nick-nagel.github.io/blog/svg-artworks.html" rel="alternate"></link><published>2025-04-01T00:00:00-04:00</published><updated>2025-04-01T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2025-04-01:/blog/svg-artworks.html</id><summary type="html">&lt;p&gt;Creating art with SVG&lt;/p&gt;</summary><content type="html">&lt;p&gt;&amp;lt;&amp;lt; INSERT THE N &amp;gt;&amp;gt;&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Recently I've had the good fortune to be able to devote effort to a long-time passion-project of mine revolving around art and SVG (Scalable Vector Graphics). Returning to this project (after somewhat of a hiatus) got me thinking about general principles of art and how they apply to SVG. This post is an attempt to synthesize some of these concepts.&lt;/p&gt;
&lt;p&gt;Some time ago I was regularly attending life drawing sessions conducted by a local artist, Freeman Burns. Every couple of weeks He'd invite models to his studio and local artists would come and participate in drawing sessions. To me, over and above the creation of the artwork itself, this kind of activity is &lt;em&gt;very&lt;/em&gt; important to train and exercise one's mind, and foster creativity more generally. &lt;/p&gt;
&lt;p&gt;While attending these sessions, Freeman helped me understand that all art is about making marks. Whether it's using charcoal, ink or oils -- or in the digital world pixels, brushes and strokes -- art is about getting your image from your mind to your medium. And one of the key insights I took away from these sessions was a christallization of the core elements of art itself: &lt;em&gt;shape&lt;/em&gt;, &lt;em&gt;line&lt;/em&gt; and &lt;em&gt;color&lt;/em&gt;. This post is about applying these core elements to create artworks using the medium of SVG. &lt;/p&gt;
&lt;h1&gt;SVG as an Artistic Medium&lt;/h1&gt;
&lt;p&gt;Details aside, SVG is an XML-based vector image format for defining two-dimensional graphics. In addition, SVG supports interactivity and animation&lt;sup id="note_1"&gt;&lt;a href="en_1"&gt;1&lt;/a&gt;&lt;/sup&gt;. It is widely used over the World-Wide-Web to create logos and illustrations, and has many applications for data visualization. This post is about thinking of SVG more generally as an artistic medium. To illustrate, I'll walk through the process I've arrived at to draw the figure.&lt;/p&gt;
&lt;p&gt;Since it is XML-based SVG is technically "human readable". This aspect is &lt;em&gt;very&lt;/em&gt; useful to programmers who are willing and able to work through the syntax and semantics of the format. But for the creation of artworks you don't want to be generating SVG by hand. The good news is there are a number of tools and applications available to create SVG. But my all-time favorite is &lt;em&gt;Inkscape&lt;/em&gt;. So that's what I'll be using to create figure for this post.&lt;/p&gt;
&lt;p&gt;One of the more interesting applications of SVG (to me) is the creation of assets for games and other simulations. So, for purposes of illustration, I'll walk through the creation of a legendary hero figure -- Perseus -- of Graeko-Roman mythological fame &lt;sup id="note_2"&gt;&lt;a href="en_2"&gt;2&lt;/a&gt;&lt;/sup&gt; . &lt;/p&gt;
&lt;p&gt;&amp;lt;&lt;INSERT REFERENCE STATUE IMAGE&gt;&amp;gt;&lt;/p&gt;
&lt;p&gt;Reference Image. Whenever I'm modelling a character I like to work off references. In classical mythology, Perseus is a heroic figure. So as a starting point I found some imagery from classical Greece in the form of this statue.&lt;/p&gt;
&lt;h2&gt;Shape&lt;/h2&gt;
&lt;p&gt;Shape in art is a two dimensional area that gives rise to perceptible forms in created works. Shapes give character and personality to objects and can be manipulated to convey layoring and depth. Shapes range from mathematically well defined geometric clases to organic shapes created by artists. &lt;/p&gt;
&lt;p&gt;Toward creating the figure, I like to start by defining a set of component shapes. From the reference I've induced I'll need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A number of shapes for the body parts,&lt;/li&gt;
&lt;li&gt;A shape for the hair,&lt;/li&gt;
&lt;li&gt;Shapes for the helmet, and&lt;/li&gt;
&lt;li&gt;Since the figure I'm creating is intended as a game asset I'll need shapes for a sword and shield (we'll need to enable our hero to do battle, right?)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;lt;&lt;INSERT SVG&gt;&amp;gt;&lt;/p&gt;
&lt;p&gt;Component shapes for the legendary hero.&lt;/p&gt;
&lt;p&gt;In this example I have a set of shapes that can be readily arranged to create a wide range of poses for the hero type and maintain consistency. So, working with these components we acheive a level of &lt;em&gt;modularity&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Tip: A note on modularity. One of the benefits of working with SVG is that it readily lends itself to the creation of modular and re-usable component libraries. Modular libraries define component parts that can be readily assembled in order to help maintain consitency and produce animations and sequential artworks. &lt;/p&gt;
&lt;p&gt;To illustrate, in the next step I've arranged the parts to create a "resting pose" for the legendary hero. &lt;/p&gt;
&lt;p&gt;&amp;lt;&lt;INSERT SVG&gt;&amp;gt;&lt;/p&gt;
&lt;h1&gt;Line&lt;/h1&gt;
&lt;p&gt;Up to now I've sort of glossed over the point that inherent in our definition of shape is the notion that shapes have perimeters which can be called out with lines and curves. Perceptually, lines are synthetic in the sense that they're a construct of the brain. As you gaze out into your world you perceive objects, and these objects can be described by lines and curves. And yet these perceived linear elements are constucts synthesized by the brain based on the detection of light gradients, and color contrasts. Conversely, artists can use line to 'suggest' shapes, relationships, negative space and ultimately give form to their ideas.&lt;/p&gt;
&lt;p&gt;To illustrate consider the classic Rubin's vase (which I've reproduced below in SVG). &lt;/p&gt;
&lt;p&gt;&amp;lt;&amp;lt; INSERT EXAMPLE &amp;gt;&amp;gt; &lt;/p&gt;
&lt;p&gt;Rubin's vase in SVG. &lt;/p&gt;
&lt;p&gt;Rubin's vase illustrate the importance of synthesis in perception. It's an &lt;em&gt;ambigous&lt;/em&gt; figure that can be perceived either as a vase object or two faces symmetrically arranged around a negative space. Above I've used line to call out the two percepts. &lt;/p&gt;
&lt;p&gt;Here I want to use the illustration to raise an important aspect of working with SVG as a medium. SVG defines a shape in terms of both &lt;em&gt;fill&lt;/em&gt; and &lt;em&gt;stroke&lt;/em&gt;. The &lt;em&gt;stroke&lt;/em&gt; is the line defining the perimeter of the &lt;em&gt;shape&lt;/em&gt;. The fill is the area encased by the line. When you create a shape in SVG the stroke is implied. Both the fill and stroke in SVG can be associated with color through attributes on the shape &lt;sup id="note_3"&gt;&lt;a href="en_3"&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Getting back to the creation of the legendary hero; I've drawn a set of shapes and arranged them in a resting pose. In doing so I refined the shapes to better define relationships among the constituent body parts and to suggest a particular heroic physique. Finally, I've hidden the stroke-lines for reasons which I'll make evident momentarily. &lt;/p&gt;
&lt;p&gt;&amp;lt;&amp;lt; INSERT SVG &amp;gt;&amp;gt;&lt;/p&gt;
&lt;p&gt;At this point in the process I'd say I've locked in the "broad strokes" that are starting to better define the figure I'm after. Now is where I like to come in and flesh out greater detail using linear elements. &lt;/p&gt;
&lt;p&gt;INFO:  I think what attracts me most to SVG is the capability to work with Bezier Curves to create shapes. Working with SVG shapes is a completely different approach to creating art than working with brushes and strokes that are availible in bitmap centric applications like Photoshop. To me its a process more akin to &lt;em&gt;sculpting&lt;/em&gt; than &lt;em&gt;drawing&lt;/em&gt;. Working with Inkscape beziers I feel like I'm sculpting &lt;em&gt;lines&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;And this brings me to an important aspect of working with SVG as a medium. SVG defines a shape in terms of both &lt;em&gt;fill&lt;/em&gt; and &lt;em&gt;stroke&lt;/em&gt;. The stroke is the line defining the perimeter of the shape. The fill is the area encased by the line. When you create a shape in SVG the stroke is implied. &lt;/p&gt;
&lt;p&gt;But for artistic purposes I like to hide the stroke and apply a differnt technique to create the line art. Working with SVG it's straightforward to duplicate shapes. So once I have the broad strokes locked down I can add line art with a bit of style by duplicating any shape, slightly enlarging it, setting an "outline color" and dropping it below the original. &lt;/p&gt;
&lt;p&gt;TIP: Inskcape makes the process I just described somewhat easier using the &lt;em&gt;linked outset tool&lt;/em&gt;. Select a shape and use &lt;code&gt;path -&amp;gt; linked outset&lt;/code&gt; from the dropdown menu. This gives you a &lt;em&gt;linked outset&lt;/em&gt; shape which you can resize using a convenient handle. TODO CONSIDER EMBEDDING A SCREENSHOT...&lt;/p&gt;
&lt;p&gt;With this approach, I can add detailed linework independent of the stroke lines associated with the shapes in the artwork. &lt;/p&gt;
&lt;h1&gt;Color&lt;/h1&gt;
&lt;p&gt;It's hard to do justice to the concept of color in art in a short blog post. I could write an entire book about it if I had enough time. But here my aim is to distill color theory down to some of the key concepts most highly relevent to SVG. Artists use color theory to create imagery that influences mood and perception in many ways in order to acheive their artistic expression. &lt;/p&gt;
&lt;h2&gt;Perception&lt;/h2&gt;
&lt;p&gt;Understanding color theory starts with perception. Variation in color arises due to variation in light frequencies across the visible spectrum. We perceive color thanks to the structure of the human eye.&lt;/p&gt;
&lt;p&gt;&amp;lt;&lt;Insert EYEBALL DIAGRAM&gt;&amp;gt;&lt;/p&gt;
&lt;p&gt;Structure of the Human Eye.&lt;/p&gt;
&lt;p&gt;Above I've created a diagram of the eyeball (using SVG of course!) to illustrate. Light passes through the lens, traverses the aqueous humor on and falls on the &lt;em&gt;retina&lt;/em&gt;. The retina is a thin sheet of nerve cells covering the interior of the eyeball. It is the retina that enables us to sense light. The retina is comprised of two main types of cell that respond to light; &lt;em&gt;rods&lt;/em&gt; and &lt;em&gt;cones&lt;/em&gt;. Rods are sensitive to light intensity. It's the cones that underly our perception of color. Different cone cells respond differently to different light frequencies. They respond differntially depending on whether light hitting them falls closer to the red green or blue frequency ranges in the visible spectrum. &lt;/p&gt;
&lt;p&gt;So that's the biological underpinning of the red, green and blue primary color scheme. All the colors we perceive in the world around us are the result of combinations of red, green and blue intensity levels reflecting off of objects and landing on the retina. &lt;/p&gt;
&lt;h2&gt;Elements of Color&lt;/h2&gt;
&lt;p&gt;In art, the qualitative perception different colors (i.e., red vs. green vs. blue etc.) isn't the only way to think about color. Artists describe color not only in terms of &lt;em&gt;hue&lt;/em&gt; (the colors as they appear on the color wheel) but also in terms of &lt;em&gt;value&lt;/em&gt; and &lt;em&gt;saturation&lt;/em&gt;. Value refers to the lightness or darkness of a color. Saturation refers to intensity or purity. Highly saturated colors are vibrant. Low saturation is dull. &lt;/p&gt;
&lt;h2&gt;The Color Wheel&lt;/h2&gt;
&lt;p&gt;RESUME HERE &lt;/p&gt;
&lt;p&gt;INCLUDE A SCREENSHOT OF THE COLOR WHEEL FROM INKSCAPE...&lt;/p&gt;
&lt;p&gt;## Palettes
  ## Shade
  ## Highlight
  ## Gradients&lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;If you've followed along then maybe you can appreciate what I mean about SVG as an artistic medium. &lt;/p&gt;
&lt;p&gt;As a format it can describe libraries of reusable works.&lt;/p&gt;
&lt;p&gt;As an artform it has many applications: icons, illustrations, animations, lazer cuts, simulations, datavisualization and beyond...&lt;/p&gt;
&lt;h1&gt;End Notes&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;span id="en_1"&gt;Scalable Vector Graphics (SVG) is a technological specification that enables the creation of digital images using a format defined over mathematical operations (as opposed to bitmap image representational formats). I'm not going into much greater detail in this post -- I'm afraid that will have to await another article. But, long story short, vector graphics is a digital art form well known to web developers and graphics designers and I've had a long-standing passion revolving around applying vector graphics to create art &lt;a href="#note_1"&gt;INSERT UP&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span id="en_2"&gt;I created this character as an asset for a game demo I'm writing to demonstrate AI applications in game development &lt;a href="#note_2"&gt;INSERT UP&lt;/a&gt;&lt;/span&gt;. Spoiler alert; he battles Medusa.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;INSERT MEDUSA&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Technically the fill and stroke colors (along with other properties of these features) can be specified and modified using CSS &lt;sup id="note_3"&gt;&lt;a href="en_2"&gt;INSERT UP&lt;/a&gt;&lt;/sup&gt; . &lt;/li&gt;
&lt;/ol&gt;</content><category term="blog"></category><category term="svg"></category><category term="scalable vector graphics"></category><category term="art"></category><category term="artwork"></category><category term="artworks"></category><category term="Inkscape"></category><category term="Illustrator"></category><category term="Adobe"></category></entry><entry><title>Working with SVG Transformations</title><link href="https://dr-nick-nagel.github.io/blog/trans-matrix.html" rel="alternate"></link><published>2025-02-17T00:00:00-05:00</published><updated>2025-02-17T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2025-02-17:/blog/trans-matrix.html</id><summary type="html">&lt;p&gt;How to work with SVG transformations ...&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Having recently re-engaged in a passion project of mine -- building an SVG/javascript &lt;em&gt;artworks framework&lt;/em&gt; -- I found myself needing to make use of &lt;em&gt;SVG transforms&lt;/em&gt;.  Transforms are a &lt;em&gt;very&lt;/em&gt; powerful feature of SVG that can be used to create and position shapes, move and deform elements, and modify paths to create amazing and beautiful effects and animations. But, in order to exploit this feature to its utmost potential, programmers and artists must make the effort to fully understand SVG coordinate systems and how transforms work!&lt;/p&gt;
&lt;h1&gt;Coordinate Systems in SVG&lt;/h1&gt;
&lt;p&gt;SVG was created to define images in terms of mathematical abstractions as opposed to bitmap representations. In so doing it opens up a whole new universe of possibilities for creative types to produce artworks -- art that can be beautifully rendered with full fidelity without concern for information loss due to resolution. Conceptually, as a creator you're still working with a canvas. But to produce your art you're manipulating lines and shapes in ways that, to me, feels more like sculpting than like painting. &lt;/p&gt;
&lt;p&gt;In any case, to best make use of the features offered by the SVG format (especially transformations), we need to understand the constructs used by the system. Let's start with the SVG &lt;em&gt;canvas&lt;/em&gt; and associated concepts; the &lt;em&gt;viewport&lt;/em&gt; and the &lt;code&gt;viewBox&lt;/code&gt; attribute. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The SVG &lt;strong&gt;canvas&lt;/strong&gt; is an infinite, abstract coordinate space where all SVG content theoretically exists. Think of it as a conceptual drawing surface that extends indefinitely in all directions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The SVG &lt;strong&gt;viewport&lt;/strong&gt; is the visible region where the SVG is rendered. The viewport determines how much of the canvas is displayed on the screen. The viewport's coordinate system has its origin at (0,0), with $x$ increasing to the right and $y$ increasing downward (consistent with most computer graphics systems).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;viewBox&lt;/code&gt; &lt;em&gt;attribute&lt;/em&gt; maps a specific region of the SVG viewport to a display area in an SVG &lt;em&gt;client&lt;/em&gt; (a device rendering the artwork).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Visual thinker that I am, I've drawn an SVG illustration to try to illustrate these important conceptual elements...&lt;/p&gt;
&lt;style&gt;
#img_cont_01 {
    width: 300px;
}
&lt;/style&gt;
&lt;div id="img_cont_01"&gt;
    &lt;img alt="INSERT SVG ILLUSTING CANVAS HERE..." 
        src="/svg/tx_matrix/coord_systems_plain.svg"
        width="350px"
    /&gt;
&lt;/div&gt;

&lt;p&gt;The renderable region of the canvas is defined by &lt;code&gt;width&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt; attributes on the &lt;code&gt;svg&lt;/code&gt; &lt;em&gt;document element&lt;/em&gt; as shown in the following snippit. &lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;svg id='example_1'
   width="300"
   height="400"
   viewBox="0 0 300 400"&amp;gt;
   ...
&amp;lt;/svg&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;viewBox&lt;/code&gt; attribute defines a coordinate system as a basis for transforming SVG canvas coordinates to fit a specific display area. The syntax is &lt;em&gt;minX minY width&lt;/em&gt; and &lt;em&gt;height&lt;/em&gt;. The SVG elements in the defined region will be mapped to the client display area. If the width and height values are the same for the canvas and viewBox the mapping is 1:1. If the values differ between the two the client will translate and scale the canvas elements to fit the viewBox coordinate system. For example, if the &lt;code&gt;viewBox&lt;/code&gt; values above were changed to 0, 0, 600, 800 the SVG elements would be scaled down by half. If they were changed to 0, 0, 150, 200 they'd be scaled up by a factor of 2.&lt;/p&gt;
&lt;h1&gt;Client and SVG Viewports&lt;/h1&gt;
&lt;p&gt;Understanding these concepts becomes even more important when you embed your SVG in HTML Web Pages and other applications. In such cases you have to worry about not just your SVG but also the &lt;strong&gt;client viewport&lt;/strong&gt;. Here I've created an illustration depicting an SVG viewport embedded in a client webpage. &lt;/p&gt;
&lt;style&gt;
#viewport-container {
    display: inline-block;
    width:  300px;
    height: 300px;
}
&lt;/style&gt;
&lt;div id="viewport-container"&gt;
    &lt;svg id="svg_2"
        xmlns="http://www.w3.org/2000/svg" 
        width="300"
        height="300" 
    &gt;
        &lt;!--INSERT SVG CONTENT--&gt;
    &lt;/svg&gt;
&lt;/div&gt;

&lt;p&gt;If you happen to be reading this on computer and move your mouse around the illustration you can get a feel for the two coordinate systems. The dynamic display should update both the client and SVG coordinates for the model.&lt;/p&gt;
&lt;h1&gt;Key Takeaways&lt;/h1&gt;
&lt;p&gt;So the key takeaways from all this is that in working with SVG it's important to understand the conceptual relationships between the &lt;em&gt;canvas&lt;/em&gt;, the &lt;em&gt;viewbox&lt;/em&gt; and the SVG and client &lt;em&gt;viewport coordinate systems&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;Here's another way to think about these things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;SVG canvas&lt;/em&gt; is like a &lt;em&gt;World Coordinate System&lt;/em&gt;. Imagine an abstract, infinite coordinate plane on which you get to draw. This is where the SVG exists conceptually.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;User Coordinate System&lt;/em&gt; is defined by your SVG document. The &lt;code&gt;&amp;lt;svg&amp;gt;&lt;/code&gt; element defines a view into your world. This is the &lt;em&gt;user coordinate system&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;Viewport Coordinate System&lt;/em&gt; creates a basis for a Transformed View. As we'll see shortly a big part of the magic of SVG is that transforms can be applied to elements to create new shapes effects and animations. But the &lt;code&gt;&amp;lt;svg&amp;gt;&lt;/code&gt; element &lt;em&gt;itself&lt;/em&gt; undergoes a transformation determined by it's viewBox. This transform modifies how the user coordinate system is mapped to the actual display area (the viewport).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Why doe all this matter? I've seen folks working with software like Illustrator and other vector arts creation tools get bogged down when trying to export their content for display in other clients (think web browsers, phones and even physical media). Without a deep understanding of the concepts covered here it's easy to get puzzled when you're working with with extensive tool chains. &lt;/p&gt;
&lt;div class="admonition important"&gt;
&lt;p class="admonition-title"&gt;Tip&lt;/p&gt;
&lt;p&gt;For illustration I primarily work with &lt;em&gt;Inkscape&lt;/em&gt; (a very powerful open source tool for creating vector graphics). In using Inkscape as part of a tool-chain (e.g., for Web development or working with the soon-to-be revealed SVG Artworks Framework) you'll generally want to insure a 1:1 mapping between your SVG canvas and viewbox settings. In other words, make sure your viewbox settings are 0, 0, width and height with width and height corresponding to your root SVG. &lt;/p&gt;
&lt;/div&gt;
&lt;h1&gt;SVG Transformations&lt;/h1&gt;
&lt;style&gt;
#needle_cont {
    display: inline-block;
    float: right;
    margin: 20px;
    width: 50px;
    height:100px;
}

.left_float {
    float: left;
    margin: 20px;
}

.right_float {
    float: right;
    margin: 20px;
}

&lt;/style&gt;

&lt;div id="needle_cont"&gt;
  &lt;img alt='INSERT NEEDLE' 
       src='/svg/tx_matrix/compass.svg' 
  /&gt;
&lt;/div&gt;

&lt;p&gt;Armed with our thorough and comprehensive understanding of SVG coordinate systems we're ready to apply our knowledge to &lt;em&gt;SVG transformations&lt;/em&gt;. Transformations can be applied to SVG elements and groups to compose objects, create effects and enable animations. Transformations are acheived using the &lt;code&gt;transform&lt;/code&gt; attribute in combination with built in SVG functions for &lt;em&gt;translation&lt;/em&gt;, &lt;em&gt;rotation&lt;/em&gt;, &lt;em&gt;scale&lt;/em&gt; and &lt;em&gt;skew&lt;/em&gt;.  I'll illustrate each of these functions in turn using this compass needle I created to the right.&lt;/p&gt;
&lt;h1&gt;Translation&lt;/h1&gt;
&lt;p&gt;Translation moves an elment or group &lt;em&gt;relative to it's origin&lt;/em&gt; in the SVG viewport coordinate space. You can translate elements or groups using the &lt;code&gt;translate( x, y )&lt;/code&gt; function as a value for the transorm property.   &lt;/p&gt;
&lt;div&gt;
    &lt;img 
        alt='INSERT NEEDLE' 
        src='/svg/tx_matrix/tx_1_trans.svg' 
    /&gt;
&lt;/div&gt;

&lt;p&gt;To demonstrate this I've translated the needle by 200 pixels in both the x and y directions using the following code:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt;g  id="needle"
        transform="translate(200, 200)"
    &amp;gt;
    &amp;lt;!-- SVG CODE DEFINING THE NEEDLE --&amp;gt;
    &amp;lt;/g&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Important!&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;It's important to note that &lt;strong&gt;when a transform is applied to an element in SVG technically it's applied to the element's &lt;em&gt;local coordinate system&lt;/em&gt;&lt;/strong&gt;. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To illustrate the point, I've drawn the needle's local coordinate axes in green. So in addition to the &lt;em&gt;viewport coordinate system&lt;/em&gt; all objects on the canvas have their own &lt;em&gt;local coordinate systems&lt;/em&gt; affecting their transformations. &lt;/p&gt;
&lt;h1&gt;Rotation&lt;/h1&gt;
&lt;p&gt;Next let's look at rotating objects. In the same way we used the SVG built-in function &lt;code&gt;translate&lt;/code&gt; to move our needle we can use the &lt;code&gt;rotate&lt;/code&gt; function to rotate it around a point. Here I'll rotate the needle 45&amp;#x00B0;.  &lt;code&gt;&amp;lt;g
        id="needle"
        transform="rotate(45)"
    &amp;gt;...&amp;lt;/g&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;div class="left_float"&gt;
    &lt;img 
        alt='INSERT ROTATION' 
        src='/svg/tx_matrix/tx_2_rotation.svg' 
    /&gt;
&lt;/div&gt;

&lt;p&gt;This works as expected because I defined the compass needle with its local coordinate system origin to be the same as the viewport origin. In other words, &lt;em&gt;the center of the needle is at $P = (0,0)$ by design&lt;/em&gt;.  But what if I have an object that is not centered at the canvas origin?&lt;/p&gt;
&lt;p&gt;In the next example I've created a windfarm sprite with a &lt;em&gt;propeller group&lt;/em&gt; centered at $(100, 100)$. Notice what happens when I try rotating the group by 15&amp;#x00B0;. The group is rotated 15&amp;#x00B0; about the &lt;em&gt;viewport origin&lt;/em&gt;. And this is not exactly what I might want here. The reason is that the propeller-group local coordinate system starts out the same as the viewport coordinate system both centered at $(0, 0)$ on the canvas. The propeller components (the blades and center) are defined by points that are offset from the origin and when I apply &lt;code&gt;rotate&lt;/code&gt; the group is rotated as a whole by 15&amp;#x00B0; about the origin (I've tried to show the rotation with a green arc).&lt;/p&gt;
&lt;table style="border-spacing: 20px"&gt;
  &lt;tr&gt;
    &lt;td &gt;
        &lt;div&gt;
            &lt;img 
                alt='INSERT ROTATION' 
                src='/svg/tx_matrix/tx_3_rotation_a.svg' 
            /&gt;
            &lt;div&gt;Propeller at &lt;br /&gt;(100, 100)&lt;/div&gt;
        &lt;/div&gt;
    &lt;/td&gt;
    &lt;td&gt;
        &lt;div&gt;
            &lt;img 
                alt='INSERT ROTATION' 
                src='/svg/tx_matrix/tx_3_rotation_b.svg' 
            /&gt;
            &lt;div&gt;Rotated 15&amp;#x00B0; about viewport&lt;br&gt; origin: &lt;span style='font-family:monospace;color:red;font-size:smaller'&gt;rotate(15)&lt;/span&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;div class="right_float"&gt;
    &lt;img 
        alt='INSERT ROTATION' 
        src='/svg/tx_matrix/tx_3_rotation_c.svg' 
    /&gt;
&lt;/div&gt;

&lt;p&gt;For this reason -- the &lt;code&gt;rotate&lt;/code&gt; function permits additional arguments to specify the $x$ and $y$ coordinates about which to rotate a target like so:&lt;/p&gt;
&lt;pre style="margin-top:20px"&gt;

&amp;lt;g id="propeller"
  &lt;span style="color:red" &gt;transform="rotate(45 100 100)"&lt;/span&gt;
&gt;
...
&amp;lt;/g&gt;
&lt;/pre&gt;

&lt;p&gt;The arguments to &lt;span style="color:red"&gt;&lt;code&gt;rotate&lt;/code&gt;&lt;/span&gt; are:
1. The &lt;em&gt;angle of rotation&lt;/em&gt;, followed by 
2. The $x$, and $y$ coordinates of the &lt;em&gt;pivot point&lt;/em&gt;.&lt;/p&gt;
&lt;div class="admonition tip"&gt;
&lt;p class="admonition-title"&gt;Tip&lt;/p&gt;
&lt;p&gt;If you are an artist making SVG sprites design them to be centered at the SVG canvas origin (0, 0). Otherwise users of your artworks may have to calculate corrections depending on their transformation needs. &lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;Bonus: Animation Preview&lt;/h2&gt;
&lt;p&gt;It's worth noting here that a similar effect can be achieved using the &lt;code&gt;transform-origin&lt;/code&gt; attribute. &lt;code&gt;transform-origin&lt;/code&gt; can be applied to the target of a transform or an animation involving a transform. In this example, I've applied the attribute to the propeller group on the windmill. The effect is the same as above -- it specifies a pivot point for the transformation at (100, 100).&lt;/p&gt;
&lt;style&gt;
.markup {
    border: solid black 1px;
    background-color: rgb( 50, 50, 50 );
    color: rgb( 200, 200, 180 );
    padding: 5px;
}
&lt;/style&gt;
&lt;pre class="markup"&gt;
&amp;lt;g  id="propeller"
    &lt;span style="color:red" &gt;transform-origin="100 100"&lt;/span&gt;
    ...
    &gt;
  ...
&amp;lt;/g&gt;
&lt;/pre&gt;

&lt;p&gt;This is particularly useful for animation (which I'll be covering extensively in forthcoming blog series revolving around my &lt;strong&gt;SVG Artworks Framework&lt;/strong&gt;). But -- to forshadow -- a quick and easy way to achieve basic animation is using SVG tags (technically &lt;a href="https://www.w3.org/TR/SMIL3/"&gt;SMIL&lt;/a&gt;). To animate this example is as easy as adding an &lt;code&gt;animateTransform&lt;/code&gt; tag to the SVG.&lt;/p&gt;
&lt;pre class="markup"&gt;
&amp;lt;g  id="propeller"
    &lt;span style="color:red" &gt;transform-origin="100 100"&lt;/span&gt;
    ...
&gt;
    &amp;lt;!-- Animate Rotation --&gt;
    &lt;span style="color:red" &gt;&amp;lt;animateTransform 
        attributeName="transform" 
        type="rotate"
        from="0" to="360" 
        dur="2s" 
        repeatCount="indefinite"/&gt;&lt;/span&gt;
  ...
&amp;lt;/g&gt;
&lt;/pre&gt;

&lt;p&gt;And voil&amp;#xE0;! Rotation at work for us. Since XML is human readible I'll leave it to you, the reader, to parse the attributes. In this case they're pretty obvious.&lt;/p&gt;
&lt;div&gt;
    &lt;img 
        alt='INSERT ROTATION' 
        src='/svg/tx_matrix/tx_3_rotation_d.svg' 
    /&gt;
&lt;/div&gt;

&lt;h1&gt;Scale&lt;/h1&gt;
&lt;div class="right_float"&gt;
   &lt;img alt='INSERT favicon' width='16' src='/svg/tx_matrix/favicon.svg' /&gt;
&lt;/div&gt;

&lt;p&gt;Next let's look at &lt;em&gt;scale&lt;/em&gt;. Scale puts the 'S' in SVG. Again, a large part of the beauty of Scalable Vector Graphics is that lines and shapes can be scaled to any size for presentation without loss of information. To illustrate this point consider the SVG graphic that I created as an icon for my blog (shown on the right). I use it as a "favicon" -- an image which browsers display in tabs holding the blog pages. Below I've scaled up the graphic to 10 times it's original dimensions. To the naked eye the scaling operation reveals greater detail associated with the image than can be observed in it's usual scale. Down and to the right I've applied the same scaling operation to the image in rastor format (png). The result presents with a classic case of pixellation typical of changing the resolution of bitmap images. &lt;/p&gt;
&lt;table style="border-spacing: 20px"&gt;
  &lt;tr&gt;
    &lt;td &gt;
        &lt;div&gt;
            &lt;img alt='INSERT favicon' 
                 width='90' 
                 src='/svg/tx_matrix/favicon.svg' /&gt;
            &lt;div&gt;SVG favicon scaled by 10x&lt;/div&gt;
        &lt;/div&gt;
    &lt;/td&gt;
    &lt;td&gt;
        &lt;div&gt;
            &lt;img alt='INSERT favicon bitmap scaled' 
                 width='90' 
                 src='/svg/tx_matrix/favicon_rastor.png' /&gt;
            &lt;div&gt;Same icon in PNG format when scaled.&lt;/div&gt;
        &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Historically, this aspect of the SVG format was extremely significant for web-development (and, indeed, was part of the impetus behind the effort). Before browser support for the SVG standard became available web designers had to spend considerable effort creating graphics icons used across their websites. The problem was compounded as more Internet capable devices emerged. For any given graphic multiple versions had to be created and managed to support varying resolutions. But with SVG support now ubiquitous across graphics rendering systems designers no longer have that problem.&lt;/p&gt;
&lt;p&gt;Using SVG, scaling objects is as simple as using the &lt;code&gt;scale&lt;/code&gt; function in  a transform. In the next example I've scaled the compass needle we used earlier to three times it's size. For comparison, I'm showing it side by side against the original size in the same SVG file.&lt;/p&gt;
&lt;div&gt;
    &lt;img 
        alt='INSERT NEEDLE SCALED' 
        src='/svg/tx_matrix/tx_4_scale.svg' 
    /&gt;
&lt;/div&gt;

&lt;p&gt;Here's the code (just a one liner) ... &lt;/p&gt;
&lt;pre class="markup"&gt;
&amp;lt;g  id="needle_scaled"
  transform="translate(300, 200) &lt;span style="color:red" &gt;scale(2)&lt;/span&gt;"
&gt;
  ...
&amp;lt;/g&gt;
&lt;/pre&gt;

&lt;p&gt;There are a couple of important things to notice here. First and formost, if you look closely you'll see I've applied not just one but two functions in the transform attribute; &lt;code&gt;scale&lt;/code&gt; of course but also &lt;code&gt;translate&lt;/code&gt;. I did that in order to show the original and scaled needles side-by-side. But I also wanted to make the point that the &lt;code&gt;transform&lt;/code&gt; attribute actually takes a &lt;em&gt;list&lt;/em&gt; of functions and will apply them in the order given.&lt;/p&gt;
&lt;p&gt;And that brings us to the second major point here; &lt;em&gt;order matters&lt;/em&gt;. Remember, that the transform functions are applied to the &lt;em&gt;coordinate systems&lt;/em&gt; of the targets. So in this case we (1) &lt;em&gt;translate&lt;/em&gt; the object and then (2) &lt;em&gt;scale&lt;/em&gt; it.That's different than applying &lt;code&gt;scale&lt;/code&gt; and &lt;em&gt;then&lt;/em&gt; translating the object. &lt;/p&gt;
&lt;pre class="markup"&gt;
&amp;lt;g  id="needle_scaled"
  transform="&lt;span style="color:red" &gt;scale(2)&lt;/span&gt; translate(300, 200)"
&gt;
  ...
&amp;lt;/g&gt;
&lt;/pre&gt;

&lt;p&gt;In the latter case we first scale the cooridinate system of the needle, and then translate it by the scaled coorinates effectively moveing it outside the original SVG viewport (which I can show by playing with the viewBox) ...&lt;/p&gt;
&lt;div&gt;
    &lt;img 
        alt='INSERT NEEDLE SCALED' 
        src='/svg/tx_matrix/tx_4_scale_b.svg' 
    /&gt;
&lt;/div&gt;

&lt;div class="admonition important"&gt;
&lt;p class="admonition-title"&gt;Important&lt;/p&gt;
&lt;p&gt;So try to keep in mind that all these transforms apply to the targets' coordinate systems. And the order of operations counts! Applying the functions in the wrong order can lead to unexpected results.&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;The Story so Far...&lt;/h2&gt;
&lt;p&gt;At this point it's probably worth summing up what we've covered so far. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We started with a discussion of SVG and client coordinate systems and saw how the canvas sets the basis for moving stuff around in SVG.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next we explored some of the main SVG transform functions; &lt;em&gt;translate&lt;/em&gt;, &lt;em&gt;rotate&lt;/em&gt; and &lt;em&gt;scale&lt;/em&gt; and saw how these functions can be applied to primitive shapes and groups using the &lt;code&gt;transfom&lt;/code&gt; attribute.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As we just saw, these functions can be applied in series. Remember, order counts!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And a more subtle point to keep in mind, all these functions are applied to the &lt;em&gt;local coordinate systems&lt;/em&gt; of the transform targets (and we briefly went into the implications of that fact).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And as an added bonus we got a bit of a preview to another topic I'll cover in greater depth elsewhere; &lt;em&gt;animating SVG&lt;/em&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All that being said and done, we still have a few more topics to cover to round out SVG transforms. &lt;/p&gt;
&lt;h1&gt;Skew&lt;/h1&gt;
&lt;p&gt;The final SVG transform attribute function we can look at is &lt;strong&gt;skew&lt;/strong&gt;.  A &lt;em&gt;skew&lt;/em&gt; transform in SVG distorts an object by slanting it along the x-axis, the y-axis, or both. In geometry that would be referred to as a &lt;em&gt;shear&lt;/em&gt; transformation. To apply shear use the &lt;code&gt;skew&lt;/code&gt; function in the &lt;code&gt;transform&lt;/code&gt; attribute as shown in the following card examples...&lt;/p&gt;
&lt;div&gt;
    &lt;img alt='INSERT skew x' 
        src='/svg/tx_matrix/club.svg' /&gt;
&lt;/div&gt;

&lt;p&gt;This first example skews the card 30&amp;#x00B0; along X using: &lt;/p&gt;
&lt;pre&gt;
&amp;lt;g id="card_sheared" 
    transform="skewX( 30 )" &gt;
    ...
&amp;lt;/g&gt;
&lt;/pre&gt;

&lt;p&gt;The next skews a card on the Y axis.&lt;/p&gt;
&lt;div&gt;
    &lt;img alt='INSERT skew y' 
        src='/svg/tx_matrix/diamond.svg' /&gt;
&lt;/div&gt;

&lt;pre&gt;
&amp;lt;g id="card_sheared" 
    transform="skewY( 30 )" &gt;
    ...
&amp;lt;/g&gt;
&lt;/pre&gt;

&lt;p&gt;And as we see next, you can apply skew on both the X and Y axes.&lt;/p&gt;
&lt;div&gt;
    &lt;img alt='INSERT skew y' 
        src='/svg/tx_matrix/spade.svg' /&gt;
&lt;/div&gt;

&lt;pre&gt;
&amp;lt;g id="card_sheared" 
    transform="skewX( 30 ) skewY( 15 )" &gt;
    ...
&amp;lt;/g&gt;
&lt;/pre&gt;

&lt;p&gt;And finally, as I've shown below, you can apply multiple transforms including skew in a single transform attribute. I'll leave it as an exercise for the reader to identify the transformations I applied to create the effect. &lt;/p&gt;
&lt;div&gt;
    &lt;img 
        alt='INSERT CARDS' 
        src='/svg/tx_matrix/card_suits.svg' 
    /&gt;
&lt;/div&gt;

&lt;p&gt;As we've seen applying skew in SVG is straightforward and can be used to create compelling effects. Beyond playing with cards it can be applied to slant text, create shadows, and create reflections. In games and simulations skewed shapes can be used to build orthographic projections adding a whole new dimension of information to an SVG scene. &lt;/p&gt;
&lt;p&gt;This just about rounds out our discussion of SVG transformations. We've looked at translation, rotation, scale and shear and seen how these transformations can be applied to create interesting and beautiful effects and objects suitable for a multitude of purposes. But a discussion of SVG transformations would not be complete without due consideration of the &lt;em&gt;transformation matrix&lt;/em&gt;. &lt;/p&gt;
&lt;h1&gt;SVG Transformation Matrices&lt;/h1&gt;
&lt;p&gt;So it turns out that all the transformations we've discussed up to this point can be represented and handled mathematically using &lt;em&gt;SVG transformation matrices&lt;/em&gt;. &lt;/p&gt;
&lt;div class="admonition important"&gt;
&lt;p class="admonition-title"&gt;LinAlg for Artists&lt;/p&gt;
&lt;p&gt;Now at this point I can imagine many reactions like; "Waitaminnit Nick! Why on earth would I ever want to work with a matrix? Doesn't that involve math?!" And you're right. It does. So, yeah this section on the SVG transformation matrix is a bit more advanced. And to be honest some folks might want to skip it. And you probabably could and still take away a &lt;em&gt;lot&lt;/em&gt; from this blog post. But I'd really like to encourage you not to. I know many folks have somewhat of an aversion to it, but, personally, I believe anyone with the desire can do the math -- and maybe even come to appreciate its beauty. In any case, for SVG creators it's important to at least know &lt;em&gt;about&lt;/em&gt; the math. And who knows -- exploring a bit of math may open up whole new worlds of creative possibilities for you. To that end, I've written a &lt;a href="TBD"&gt;primer&lt;/a&gt; which explains the math related to transformations covered in this blog post. So if you're interested in learning a bit about linear algebra for computer graphics I'd highly encourage you to give it a shot. &lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;Forms&lt;/h2&gt;
&lt;p&gt;SVG transformation matrices have the following form:&lt;/p&gt;
&lt;p&gt;$$
TM = 
\begin{bmatrix} 
a &amp;amp; c &amp;amp; e \\ 
b &amp;amp; d &amp;amp; f \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;Where a, b, c, d, e, and f are values that can be applied to transform coordinate systems in all the ways we discussed above with functions. The following list shows the transformation matrices for &lt;em&gt;translation&lt;/em&gt;, &lt;em&gt;scale&lt;/em&gt; and &lt;em&gt;rotation&lt;/em&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Translation:
$$
TM_{translation} = 
\begin{bmatrix} 
1 &amp;amp; 0 &amp;amp; t_x \\
0 &amp;amp; 1 &amp;amp; t_y \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scale:
$$
TM_{scale} = 
\begin{bmatrix} 
s_x &amp;amp; 0 &amp;amp; 0 \\
0 &amp;amp; s_y &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rotation:
$$
TM_{rotation} = 
\begin{bmatrix} 
cos(a) &amp;amp; -sin(a) &amp;amp; 0 \\
sin(a) &amp;amp; cos(a) &amp;amp; 0 \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Mappings&lt;/h2&gt;
&lt;p&gt;As we've been seeing all along, transformations provide a mapping from a prior (or &lt;em&gt;parent&lt;/em&gt;) coordinate sytem to a new coordinate system. Transformation matrices provide a formal means of describing the mapping operations. The general form of the mapping looks like this:&lt;/p&gt;
&lt;p&gt;$$
\begin{bmatrix} 
x_{prevCoordSystem} \\
y_{prevCoordSystem} \\
1
\end{bmatrix} = \begin{bmatrix} 
a &amp;amp; c &amp;amp; e \\ 
b &amp;amp; d &amp;amp; f \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix} \cdot \begin{bmatrix} 
x_{newCoordSystem} \\
y_{newCoordSystem} \\
1
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;Given the general form, let's look at a concrete example.&lt;/p&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Let's revisit our windmill. Since wind direction can change we might need to rotate our blades in order to efficiently generate power. So let's see how we'd apply a transformation matrix to achieve that end. &lt;/p&gt;
&lt;div class='right_float'&gt;
    &lt;img 
        alt='INSERT ROTATION' 
        src='/svg/tx_matrix/wind_turbine_matrix.svg' 
    /&gt;
&lt;/div&gt;

&lt;p&gt;Once again, consider the SVG transformation matrix.&lt;/p&gt;
&lt;p&gt;$$
\begin{bmatrix} 
a &amp;amp; c &amp;amp; e \\ 
b &amp;amp; d &amp;amp; f \\
0 &amp;amp; 0 &amp;amp; 1
\end{bmatrix}
$$&lt;/p&gt;
&lt;p&gt;Notice that only the first two rows represent used values. The bottom row is an identity provided to enable matrix multiplication (think of it as multiplying any number by $1$ -- you get the same number; identity). Since this is the case we can reduce the matrix to a vector of six values: $[a, b, c, d, e, f]$ where: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$a$ = scale factor on the X-axis, &lt;/li&gt;
&lt;li&gt;$b$ = skew on X,&lt;/li&gt;
&lt;li&gt;$c$ = skew on Y,&lt;/li&gt;
&lt;li&gt;$d$ = scale factor on the Y-axis, &lt;/li&gt;
&lt;li&gt;$e$ = X-axis translation, and&lt;/li&gt;
&lt;li&gt;$f$ = Y-axis translation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So for this example, I wanted to achieve the effect of rotating the blades about the mast of the wind turbine. To do so I had to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Translate the blades along the X-axis, and &lt;/li&gt;
&lt;li&gt;Scale them down on X (to get the right perspective).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here's the svg fragment with the relevant &lt;code&gt;matrix&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;
&amp;lt;g id="windmill"&gt;
    ...
    &amp;lt;g id="blades"
       ...
       transform-origin="100 100"
       transform="&lt;span style="color:red" &gt;matrix(0.6, 0, 0, 1, 10, 0)&lt;/span&gt;"
    &gt;
    ...
&amp;lt;/g&gt;
  ...
&lt;/pre&gt;

&lt;h2&gt;Really Nick! Why on Earth Would I &lt;em&gt;Ever&lt;/em&gt; want to Work with a transformation Matrix?!&lt;/h2&gt;
&lt;p&gt;If you've made it this far into this post you probably fall into one of two camps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Like me, maybe you appreciate the beauty of mathematics and have enjoyed the discussion around the application of matrix operations to achieve SVG transforms. &lt;/p&gt;
&lt;p&gt;Or...&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maybe you don't appreciate the mathematics underlying SVG's linear transformations and through eyes glazed over are wondering why on earth you should ever have to worry about it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fair enough. Working with SVG you can achieve quite a bit with the &lt;em&gt;function&lt;/em&gt; syntax -- which may feel more "user friendly". Either way though, the view into the matrix operations applicable to SVG is good to have and keep in mind. And there are a number of reasons to consider the matrix approach in the creation of your artworks. Especially if you use animations and need to make your work interactive. &lt;/p&gt;
&lt;p&gt;The SVG transformation matrix provides a powerful mechanism to for manipulating and animating SVG lines shapes and objects. The ability to apply matrices in SVG transforms offers numerous key advantages. Here's a short list.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;. As we've seen, a single matrix can represent a combination of multiple transformations (e.g., translation, rotation, scale, and skew). This allows you to apply multiple transformations in a single step, which is more efficient than applying them sequentially.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Control&lt;/strong&gt;. Matrices provide precise control over the order in which transformations are applied. Beyond that, knowledge of matrix applications enables the creation of custom effects over and above the list we've covered here. &lt;em&gt;Perspective&lt;/em&gt; is an important example. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimization&lt;/strong&gt;. The matrix allows individual transformations to be mathematically combined into a single, equivalent transformation. Consolidation reduces the calculations to be performed by the rendering engine leading to significant performance benefits. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DOM Manipulation&lt;/strong&gt;. Using matrix computations as the potential to reduce DOM manipulation which can have big impacts on performance. &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The bottom line is that using the transformation matrix to combine multiple transformations like translate, skew, and rotate can optimize your SVG by reducing processing overhead and leveraging GPU optimization. This is particularly beneficial for complex SVGs and animations where performance is a key factor.&lt;/p&gt;
&lt;style&gt;
#wind_farm {
    border: inset 4px #585;
    width:  300px;
    height: 170px;
    margin-left: auto;
    margin-right: auto;
}
&lt;/style&gt;
&lt;div id="wind_farm"&gt;
    &lt;svg id="svg_wf"
        xmlns="http://www.w3.org/2000/svg" 
        width="300"
        height="300" 
    &gt;
        &lt;!--INSERT SVG CONTENT--&gt;
    &lt;/svg&gt;
&lt;/div&gt;

&lt;style&gt;
#airstrip {
/*
    display: inline-block;
    width:  300px;
    height: 300px;
*/
margin-top: 200px;
}
&lt;/style&gt;

&lt;!--
&lt;div id="airstrip"&gt;
    &lt;svg id="svg_strip"
        xmlns="http://www.w3.org/2000/svg" 
        width="300"
        height="300" 
    &gt;
    &lt;/svg&gt;
&lt;/div&gt;
--&gt;

&lt;script src="/svg/loadsvg.js"&gt;&lt;/script&gt;

&lt;script&gt;
/**
 * One off script for this article. VERY fragile. Synchronization
 * going on to 1. load all the svg's. and ONLY THEN work off DOM...
 */
async function doLoads () {

    //await loadSvg( "/svg/tx_matrix/tx_matrix_1_trans.svg", "canvas_1" );

    await loadSvg( "/svg/tx_matrix/svg_viewport.svg", "svg_2" );
    await loadSvg( "/svg/tx_matrix/wind_farm_plain.svg", "svg_wf" );

    // await loadSvg( "/svg/tx_matrix/plane_plain.svg", "svg_strip" );

    let svg2 = document.getElementById( "svg_01_viewport" );
    let textElem = document.getElementById("mouse-coords"); 
    let textElem2 = document.getElementById("client-coords"); 

    function getSVGCoords(evt) {

        let pt = svg2.createSVGPoint();
        pt.x = evt.clientX;
        pt.y = evt.clientY;
        let transformed = pt.matrixTransform(svg2.getScreenCTM().inverse());

        let svg_x = transformed.x - 40 ;
        let svg_y = transformed.y - 120 ;
        let client_x = transformed.x - 20 ;
        let client_y = transformed.y - 50 ;

        // textElem.textContent = `x: ${transformed.x.toFixed(2)}, y: ${transformed.y.toFixed(2)}`;
        textElem.textContent = `SVG x: ${ Math.round(svg_x) }, SVG y: ${ Math.round(svg_y) }`;
        textElem2.textContent = `Client x: ${ Math.round(client_x) }, Client y: ${ Math.round(client_y) }`;

    }

    svg2.addEventListener("mousemove", getSVGCoords);
}
doLoads();
&lt;/script&gt;

&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This post ended up a bit longer than I'd originally anticipated. Originally I'd intended to write focusing mainly on just the SVG transformation &lt;em&gt;matrix&lt;/em&gt;. But soon into it I realized I'd have to provide some context -- namely the discussion around coordinate systems. And as I wrote, I went down the garden path of exploring the various SVG built-in transformation functions and thinking about the implications for creating SVG artworks. Nonetheless, I'm happy to've gone down that path. In doing so I hope to have shared some of the rich and vibrant features SVG offers to creative types of all kinds. I feel we've covered a lot in this post -- but keeping the main points we've covered in mind; the way viewport and client coordinate systems work, the SVG functions central to transformations, and the power of the transformation matrix itself will bring you a long way toward understanding what the world of Scalable Vector Grapics opens up! &lt;/p&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.w3.org/TR/SVGTiny12/coords.html#TransformAttribute"&gt;W3C: &lt;em&gt;Coordinate Systems, Transformations and Units&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.w3.org/TR/SVGTiny12/coords.html#TransformMatrixDefined"&gt;W3C: &lt;em&gt;Transform Matrix Defined&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Attribute/transform"&gt;mdn web docs: &lt;em&gt;transform&lt;/em&gt; &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/SVGTransform/matrix"&gt;mdn web docs: &lt;code&gt;SVGTransform&lt;/code&gt;&lt;em&gt;: matrix property&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://svgwg.org/svg2-draft/coords.html#TransformProperty"&gt;&lt;em&gt;SVG Coordinate Systems, Transformations and Units&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Draft"></category><category term="javascript"></category><category term="animation"></category><category term="svg"></category><category term="scalable vector graphics"></category><category term="art"></category><category term="artwork"></category><category term="motion"></category><category term="cartoon"></category><category term="physics"></category><category term="HTML5"></category><category term="artworks"></category><category term="framework"></category><category term="Inkscape"></category><category term="Illustrator"></category><category term="Adobe"></category></entry><entry><title>Time-Based Animation in Javascript using RequestAnimationFrame</title><link href="https://dr-nick-nagel.github.io/blog/raf-time.html" rel="alternate"></link><published>2025-02-10T00:00:00-05:00</published><updated>2025-02-10T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2025-02-10:/blog/raf-time.html</id><summary type="html">&lt;p&gt;How to use javascript RAF for time-based animation...&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this article I'll discuss key aspects of web-based animation with javascript. In particular, I'll be focusing on time-dependent animation with &lt;a href="TBD"&gt;RequestAnimationFrame&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;TODO: Expand a bit. Discuss the upcoming SVG Artworks Framework...&lt;/p&gt;
&lt;h1&gt;The Animation Loop&lt;/h1&gt;
&lt;p&gt;All animation occurs wihin a very basic loop. This was true from the inception of animated artworks from the early days of film with handrawn images to modern day computer-based systems. &lt;/p&gt;
&lt;p&gt;[[ INSERT IMAGE ANIMATION LOOP ]]
1. update
2. render
3. wait&lt;/p&gt;
&lt;p&gt;All animation involves rendering a scene, waiting for an interval to pass, updating the scene and rendering the updated imagery over and over again. &lt;/p&gt;
&lt;p&gt;TODO: ELABORATE A BIT&lt;/p&gt;
&lt;h1&gt;Request Animation Frame&lt;/h1&gt;
&lt;p&gt;Web-based animation has been around since the inception of the WWW. Today, web-based animation is achieved using HTML5, CSS and Javascript. There are a few ways in which animation can be acheived in Javascript. This article focuses on the use of &lt;code&gt;RequestAnimationFrame&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/requestAnimationFrame"&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For basic animation, using RAF may be simple enough. But for serious animations (think complex artworks) you need time-based animations. &lt;/p&gt;
&lt;h1&gt;Elementary Kinematics&lt;/h1&gt;
&lt;p&gt;I've always had a love of cartoon physics. Who can stop but to laugh out loud watching a &lt;em&gt;Tom 'n' Jerry cartoon&lt;/em&gt; or a &lt;em&gt;Road Runner&lt;/em&gt; short with Wile E. Coyote? But when I started working with computer graphics and simulations I quickly needed to ground myself in real-world physics. Before you can control and bend them to your will in your artworks you need to understand the basic laws of motion. So the purpose of this section is to quickly review basic kenetic principles in order to apply them to good effect in the context of the SVG framework. &lt;/p&gt;
&lt;h2&gt;What is Kinematics?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Kinematics&lt;/em&gt; is the study of motion without worrying about the forces that cause it. The focus is on &lt;em&gt;position&lt;/em&gt;, &lt;em&gt;velocity&lt;/em&gt; and &lt;em&gt;acceleration&lt;/em&gt; and how these aspects of motion change over time. Basic kinematic principles have been well-known since Newton. &lt;/p&gt;
&lt;h3&gt;Position, Velocity and Accelaration&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Position.&lt;/strong&gt; &lt;em&gt;Position&lt;/em&gt; represents the location of an object in space relative to a reference point. Since the SVG Artworks Framework is mainly 2D we'll consider 2D descriptions in Cartesian coordinates. Position is represented as a &lt;em&gt;vector&lt;/em&gt;, $\vec v$, with coordinates $x$ and $y$ (horizontal and vertical).&lt;/p&gt;
&lt;p&gt;&amp;lt;&amp;lt; INSERT IMAGE &amp;gt;&amp;gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In order to be consisent with 2D rendering in web browsers this discussion treats positive y as down in the 2D space. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Velocity.&lt;/strong&gt; &lt;em&gt;Velocity&lt;/em&gt; is the &lt;em&gt;rate of change&lt;/em&gt; of position with respect to time. In physics velocity represents both &lt;em&gt;speed&lt;/em&gt; and &lt;em&gt;direction&lt;/em&gt; (represented as a vector with &lt;em&gt;orientation&lt;/em&gt; and &lt;em&gt;magnitude&lt;/em&gt;). &lt;/p&gt;
&lt;p&gt;Mathematically, velocity is the first derivative of position. &lt;/p&gt;
&lt;p&gt;$$
v = \frac{dx}{dt}
$$&lt;/p&gt;
&lt;p&gt;If you integrate an object's velocity you get its position.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Acceleration.&lt;/strong&gt; &lt;em&gt;Acceleration&lt;/em&gt; if the rate of change in &lt;em&gt;velocity&lt;/em&gt; over time. &lt;/p&gt;
&lt;p&gt;Acceleration is the first derivative of velocity and the second derviative of position. &lt;/p&gt;
&lt;p&gt;$$
a = \frac{dv}{dt} = \frac{d^2x}{dt^2}
$$&lt;/p&gt;
&lt;p&gt;If you integrate an object's acceleration you get its velocity, and as I just observed if you integrate that you get position.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;Let's look at a simple example. Suppose we have an object in motion starting at position $x=0$ with an initial velocity $v_0$ of $5 m/s$ and a constant acceleration, $a$, of $2 m/s^2$ . Using the definitions above we can represent its motion as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Velocity over time:&lt;/strong&gt;
    $$
    v(t) = v_0 + at
    $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Position over time:&lt;/strong&gt;
    $$
    p(t) = x_0 + v_0t + \frac{1}{2}at^2
    $$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's look at the object's motion over the course of a 3 second interval. Where is the object located after 3 seconds? &lt;/p&gt;
&lt;div&gt;
$$
\begin{align}
p(3) &amp;= 0 + (5 \times 3) + \frac{1}{2}(2 \times 3^2) \\
     &amp;= (15) + \frac{1}{2}(2 \times 9) \\
     &amp;= 15 + 9 \\
     &amp;= 24m 
\end{align}
$$
&lt;/div&gt;
&lt;p&gt;So, after a 3 second interval we see the object has moved 24 meters. &lt;/p&gt;
&lt;div class="admonition hint"&gt;
&lt;p class="admonition-title"&gt;An Important Note on Units&lt;/p&gt;
&lt;p&gt;In going over these computations you'll notice I've been using &lt;em&gt;meters&lt;/em&gt;  as the unit of measure. And this brings up an important point. If you're working on efforts concerning simulations, games, etc., you have to keep your units in mind when working out computations necessary to simulate forces acting on your objects. For example, in creating a particle effect if I want to simulate forces like gravity and drag I can apply real-world formulas but will have to translate real-world units associated with those formulas to my artworks space. Since the SVG Artworks Framework is mainly concerned with rendering SVG graphics I'll typically consider pixels as units.  &lt;strong&gt;TODO: make this better. Look for units discussion in UNREAL?&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;RESUME HERE...&lt;/p&gt;
&lt;div style="display:none"&gt;
SEE: 

https://chatgpt.com/c/679bb3f6-8dc4-8010-9865-4cbac92ccaad
https://chatgpt.com/c/6793b6c3-e21c-8010-b70c-f7f3dcb972be
https://gemini.google.com/app/caa89c6776f23682
https://developer.mozilla.org/en-US/docs/Web/API/Window/requestAnimationFrame

&lt;/div&gt;

&lt;h1&gt;A Javascript Example&lt;/h1&gt;
&lt;p&gt;INSERT JAVASCRIPT IMPLEMENTATIONS&lt;/p&gt;
&lt;h1&gt;SVG Artworks in Action!&lt;/h1&gt;
&lt;p&gt;This demo of the NN SVG ARTWORKS FRAMEWORK illustrates the application of the kinematic expressions described in this artical to simulate gravitational forces and resistance acting on an object's velocity. Somewhere between a toy and a game... &lt;/p&gt;
&lt;div id="viewport-container"&gt;
    &lt;svg id="viewport" 
        xmlns="http://www.w3.org/2000/svg" 
        width="500" 
        height="250" 
        style="background-color: rgb(143, 202, 206);"&gt;
    &lt;/svg&gt;
&lt;/div&gt;
&lt;div class='Panel'&gt;
    &lt;button id='start_anim'&gt;Fire&lt;/button&gt;
    &lt;button id='stop_anim'&gt;Stop&lt;/button&gt;
    &lt;button id='reset_anim'&gt;Reload&lt;/button&gt;
    &lt;span id='frame_rate'
        style="display:inline-block;min-width: 180px; width: 180px"&gt;Frame Rate: 0 (Interval: 0)&lt;/span&gt;
    &lt;span id='mouse_coords'
        style="display:inline-block;min-width: 150px; width: 130px"&gt;mouse x=0, y=0&lt;/span&gt;
    &lt;input type='text' id='vx' value='193'
        style="display:inline-block;width: 30px" /&gt;
    &lt;input type='text' id='vy' value='-52'
        style="display:inline-block;width: 30px" /&gt;
&lt;/div&gt;

&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;Hint: 300, -20 might work...&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;h1&gt;Resources&lt;/h1&gt;</content><category term="draft"></category><category term="javascript"></category><category term="animation"></category><category term="svg"></category><category term="scalable vector graphics"></category><category term="art"></category><category term="artwork"></category><category term="motion"></category><category term="cartoon"></category><category term="physics"></category><category term="HTML5"></category><category term="artworks"></category><category term="framework"></category><category term="Inkscape"></category><category term="Illustrator"></category><category term="Adobe"></category></entry><entry><title>Our Encounter with the Gorgon!</title><link href="https://dr-nick-nagel.github.io/blog/paestum.html" rel="alternate"></link><published>2025-02-08T00:00:00-05:00</published><updated>2025-02-08T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2025-02-08:/blog/paestum.html</id><summary type="html">&lt;p&gt;Encounter the Gorgon!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Even though I do a lot of technical blogging I've also always liked to blog a bit about my travels. This past summer I had the good fortune to visit my long time friend in Italy. While there we visited Paestum -- a major ancient Greek city on the coast of the Tyrrhenian Sea. It was there where we encountered the fearsome Gorgon!...&lt;/p&gt;
&lt;style &gt;
.image-container {
    background: rgb(200, 200, 200);
    background-image: url('/images/gorgon/temple_alessandro_v2.0.png');
    background-size: contain; /* cover or 'contain' depending on your needs */
    position: relative;
    width: 400px; /* adjust to the size of your image */
    height: 533px; /* example height, adjust as necessary */
    border: inset 4px rgb(255 200 235);
    margin-left: auto;
    margin-right: auto;
}
&lt;/style&gt;

&lt;script&gt;
/**
 * ONE OFF CONFIGUATION FOR THIS BLOG POST.
 * TARGET FOR THIS CONFIG IS MY SVG ARTWORKS 
 * FRAMEWORK...
 */
const svgConfig = {
    SPRITES_FILE: "medusa_sprites_plain.svg",
    SVG_PATH: "/svg/gorgon/",
    SVG_CANVAS_NODE_ID: 'svg_canvas_blank'
}
&lt;/script&gt;

&lt;div class="image-container"&gt;
    &lt;svg id="svg_canvas_blank" 
        xmlns="http://www.w3.org/2000/svg"&gt;
    &lt;/svg&gt;
&lt;/div&gt;

&lt;!--
&lt;div class='Panel'&gt;
    &lt;span id='mouse_coords'
        style="display:inline-block;min-width: 150px; width: 130px"&gt;mouse x=0, y=0&lt;/span&gt;
&lt;/div&gt;
--&gt;

&lt;h1&gt;Paestum&lt;/h1&gt;
&lt;p&gt;Paestum was a major ancient Greek city on the coast of the Tyrrhenian Sea, in Magna Graecia.&lt;/p&gt;
&lt;style &gt;
.map-container {
    width: 400px; 
    height: 450px; 
    border: inset 4px rgb(200 200 200);
    margin-left:auto;
    margin-right:auto;
}
&lt;/style&gt;

&lt;div class="map-container"&gt;
    &lt;iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d564800.6600191377!2d14.456165690980129!3d40.55729097809587!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x133bfd0d8e6681c5%3A0xd20cf732fa251d53!2s84047%20Paestum%20SA%2C%20Italy!5e0!3m2!1sen!2sus!4v1739727769303!5m2!1sen!2sus" width="400" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The ruins of Paestum are famous for their three ancient Greek temples in the Doric order dating from about 550 to 450 BC that are in an excellent state of preservation. The temples were in honor of Athena, Poseidon and Hera. We encountered the Gorgon at the temple of Poseidon. &lt;/p&gt;
&lt;style&gt;
.fox-container {
    width:100px;
    /* height: 120px; */
    /* border: solid 1px rgb(200 200 200); */
    margin-left:auto;
    margin-right:auto;
    overflow:hidden;
    font-size: 8px;
    text-align:center;
}
&lt;/style&gt;

&lt;div class="fox-container"&gt;
    &lt;img src="/svg/foxy_plain.svg" width=100px&gt;&lt;/img&gt;
    &lt;div&gt;Character Design by Alessandro Gasparini&lt;/div&gt;
&lt;/div&gt;</content><category term="Blog"></category><category term="ancient"></category><category term="Paestum"></category><category term="Rome"></category><category term="Salerno"></category><category term="Greece"></category><category term="Athena"></category><category term="Hera"></category><category term="Poseidon"></category><category term="Neptune"></category><category term="Juno"></category></entry><entry><title>SVG with a dash of Kinematics</title><link href="https://dr-nick-nagel.github.io/blog/kinematics.html" rel="alternate"></link><published>2025-02-07T00:00:00-05:00</published><updated>2025-02-07T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2025-02-07:/blog/kinematics.html</id><summary type="html">&lt;p&gt;Applying kinematics to animation with SVG ...&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I've always had a love of cartoon physics. Who can stop but to laugh out loud watching a &lt;em&gt;Tom 'n' Jerry cartoon&lt;/em&gt; or a &lt;em&gt;Road Runner&lt;/em&gt; short with &lt;em&gt;Wile E. Coyote&lt;/em&gt;? But when I started working with computer graphics and simulations over the course of  developing a flight simulator subsystem for US Air force pilot training I quickly needed to ground myself in real-world physics. &lt;/p&gt;
&lt;p&gt;More recently I've had some time to re-engage in a long-standing passion project of mine and found myself revisiting the fundamentals of &lt;em&gt;kinematics&lt;/em&gt;. The project revolves around a Framework (watch for forthcoming announcements!) I'm developing to support SVG artworks. But, ahead of formally announcing the framework, I'm writing this post for artistic types who might have a general interest in kinematics. Before you can control and bend them to your will in your artworks you need to understand the basic laws of motion. So the purpose of this post is to quickly review basic kinetic principles in order to apply them to good effect in the creation of SVG art for simulations, games, sequential art any any other applications that may be inspired by the muse. &lt;/p&gt;
&lt;h1&gt;Elementary Kinematics&lt;/h1&gt;
&lt;h2&gt;What is Kinematics?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Kinematics&lt;/em&gt; is the study of motion without worrying about the forces that cause it. The focus is on &lt;em&gt;position&lt;/em&gt;, &lt;em&gt;velocity&lt;/em&gt; and &lt;em&gt;acceleration&lt;/em&gt; and how these aspects of motion change over time. Basic kinematic principles have been well-known since Newton. It is my view that understanding basic kinematics will help vector graphics artists be more creative in their artistic endeavors.&lt;/p&gt;
&lt;h2&gt;Position, Velocity and Acceleration&lt;/h2&gt;
&lt;p&gt;You don't have to be an expert in physics to produce good game art or illustrations. But I personally have found that a basic understanding of the relations between three aspects of motion can take you a looong way to creating a wide range of assets. Those aspects are &lt;em&gt;position&lt;/em&gt;, &lt;em&gt;velocity&lt;/em&gt; and acceleration. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Position.&lt;/strong&gt; &lt;em&gt;Position&lt;/em&gt; represents the location of an object in space relative to a reference point. Since SVG art is described in 2 dimensions I'll limit the scope of this post to a 2D coordinate system&lt;sup&gt;*&lt;/sup&gt;. But the principles we'll examine can be readily extended to 3D.&lt;/p&gt;
&lt;p&gt;&amp;lt;&amp;lt; INSERT IMAGE &amp;gt;&amp;gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;*&lt;/sup&gt; In order to be consisent with 2D rendering in web browsers I'll be treating positive $y$ as down in the 2D space. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Velocity.&lt;/strong&gt; &lt;em&gt;Velocity&lt;/em&gt; is the &lt;em&gt;rate of change&lt;/em&gt; of position with respect to time. In physics velocity represents both speed &lt;em&gt;and&lt;/em&gt; direction and so is conveniently represented as a &lt;em&gt;vector&lt;/em&gt; with &lt;em&gt;orientation&lt;/em&gt; and &lt;em&gt;magnitude&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;Mathematically, velocity is the first derivative of position. &lt;/p&gt;
&lt;p&gt;$$
v = \frac{dx}{dt}
$$&lt;/p&gt;
&lt;p&gt;If you &lt;em&gt;integrate&lt;/em&gt; an object's velocity you get its position.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Acceleration.&lt;/strong&gt; &lt;em&gt;Acceleration&lt;/em&gt; if the rate of change in velocity over time. &lt;/p&gt;
&lt;p&gt;Acceleration is the first derivative of velocity and the second derviative of position. &lt;/p&gt;
&lt;p&gt;$$
a = \frac{dv}{dt} = \frac{d^2x}{dt^2}
$$&lt;/p&gt;
&lt;p&gt;If you integrate an object's acceleration you get its velocity, and as I just observed if you integrate that you get position.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Representing Motion&lt;/h2&gt;
&lt;p&gt;To keep things simple I like to work by way of example. Suppose we have an object in motion starting at position $x=0$ with an initial velocity $v_0$ of $5 m/s$ and a constant acceleration, $a$, of $2 m/s^2$ . Using the definitions above we can represent its motion as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Velocity over time:&lt;/strong&gt;
    $$
    v(t) = v_0 + at
    $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Position over time:&lt;/strong&gt;
    $$
    p(t) = x_0 + v_0t + \frac{1}{2}at^2
    $$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's look at the object's motion over the course of a 3 second interval. Where is the object located after 3 seconds? &lt;/p&gt;
&lt;div&gt;
$$
\begin{align}
p(3) &amp;= 0 + (5 \times 3) + \frac{1}{2}(2 \times 3^2) \\
     &amp;= (15) + \frac{1}{2}(2 \times 9) \\
     &amp;= 15 + 9 \\
     &amp;= 24m 
\end{align}
$$
&lt;/div&gt;
&lt;p&gt;So, after a 3 second interval we see the object has moved 24 meters. &lt;/p&gt;
&lt;div class="admonition hint"&gt;
&lt;p class="admonition-title"&gt;An Important Note on Units&lt;/p&gt;
&lt;p&gt;In going over these computations you'll notice I've been using &lt;em&gt;meters&lt;/em&gt;  as the unit of measure. And this brings up an important point. If you're working on efforts concerning simulations, games, etc., you have to keep your units in mind when working out computations necessary to simulate forces acting on your objects. For example, in creating a particle effect if I want to simulate forces like gravity and drag I can apply real-world formulas but will have to translate real-world units associated with those formulas to my artworks space. &lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;A Javascript Implementation&lt;/h2&gt;
&lt;p&gt;With this in mind we can consider a programmatic implemenation of these formulas in Javascript. The forthcoming framework I'm developing represents movable objects (namely &lt;em&gt;sprites&lt;/em&gt;) using a base class that encapsulates the math.&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
/**
  * Base class for movable entities in my forthcoming SVG Artworks 
  * Framework. The Moveble base class encapsulates the math necessary
  * to move objects around the 2D SVG canvas applying the basic 
  * kinematic principles relating position to velocity and acceleration
  * over time. This class facilitates the creation of a wide variety of
  * animations and simulations.
  */
class Movable {
    constructor(svg_id, x, y, vx = 0, vy = 0, ax = 0, ay = 0) {
        this.id = svg_id;
        this.svg_group = document.getElementById( svg_id );
        this.px = x;
        this.py = y;
        this.vx = vx;
        this.vy = vy;
        this.ax = ax;
        this.ay = ay;
    }

    move(deltaTime) {
        // Update velocity based on acceleration
        this.vx += this.ax * deltaTime;
        this.vy += this.ay * deltaTime;

        // Update position based on velocity
        this.px += this.vx * deltaTime;
        this.py += this.vy * deltaTime;
    }

    render() {
        this.svg_group.setAttribute( 
            'transform',
            `translate( ${this.px}, ${this.py} )`
        );
     }
    ...
}
    &lt;/pre&gt;
&lt;/div&gt;

&lt;h1&gt;SVG Artworks in Action!&lt;/h1&gt;
&lt;p&gt;And here's an example of the application of the principles in action. Below you should see a demo in the form of something between a toy and a game illustrating the basic kinematics application with the classic example of projectile motion. The example illustrates a cannon ball shot and in playing with it you can observe the iconic parabolic trajectory of an object subjected to forces of gravity and drag represented in velocity and acceleration over time.&lt;/p&gt;
&lt;div id="viewport-container"&gt;
    &lt;svg id="viewport" 
        xmlns="http://www.w3.org/2000/svg" 
        width="500" 
        height="250" 
        style="background-color: rgb(143, 202, 206);"&gt;
    &lt;/svg&gt;
&lt;/div&gt;
&lt;div class='Panel'&gt;
    &lt;button id='start_anim'&gt;Fire&lt;/button&gt;
    &lt;button id='stop_anim'&gt;Stop&lt;/button&gt;
    &lt;button id='reset_anim'&gt;Reload&lt;/button&gt;
    &lt;span id='frame_rate'
        style="display:inline-block;min-width: 180px; width: 180px"&gt;Frame Rate: 0 (Interval: 0)&lt;/span&gt;
    &lt;span id='mouse_coords'
        style="display:inline-block;min-width: 150px; width: 130px"&gt;mouse x=0, y=0&lt;/span&gt;
    &lt;input type='text' id='vx' value='193'
        style="display:inline-block;width: 30px" /&gt;
    &lt;input type='text' id='vy' value='-52'
        style="display:inline-block;width: 30px" /&gt;
&lt;/div&gt;

&lt;p&gt;Note: you can modify the cannon ball's trajectory by changing the x and y values associated with its velocity. Back in the olden days, the only way to adjust your aim was to to change the cannon's orientation and/or add more gunpowder to increase its explosive force.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;So that's it for now. Inspired by work on my forthcoming SVG Artworks Framework the purpose of this post is merely to share observations on the fundamentals of kinematics as applied to illustration with 2D vector graphics. We looked at the basic math describing motion as a function of position, velocity and acceleration over time and played a bit with toy developed through applying these basic equations. Understanding kinematics can take you a long way to developing a wide range of applications in game art, simulations, educational materials, and much much more. Stay tuned for more SVG artwork support coming soon!&lt;/p&gt;</content><category term="Blog"></category><category term="javascript"></category><category term="animation"></category><category term="svg"></category><category term="scalable vector graphics"></category><category term="art"></category><category term="artwork"></category><category term="motion"></category><category term="cartoon"></category><category term="physics"></category><category term="HTML5"></category><category term="artworks"></category><category term="framework"></category><category term="Inkscape"></category><category term="Illustrator"></category><category term="Adobe"></category><category term="kinematics"></category><category term="kinetics"></category><category term="vectors"></category></entry><entry><title>Cultivate your Web Presence: Improve Visibility through Search Engine Optimization</title><link href="https://dr-nick-nagel.github.io/blog/seo.html" rel="alternate"></link><published>2025-01-23T00:00:00-05:00</published><updated>2025-01-23T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2025-01-23:/blog/seo.html</id><summary type="html">&lt;p&gt;Tips and technical practices to improve visibility on the WWW.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As an information systems architect and engineer with decades of experience in WWW development I'm often concerned with Internet visibility. Everybody wants their website to be highly visible, especially if they're using the Web to promote their business. Since the dawn of the WWW and the rise of search engines individuals and organizations have tried various ways to ensure their website rises to the top of the list in Internet searches. I personally have seen numerous techniques employed -- many legitimate but many not-so-much -- applied to improve search engine rankings. Some attempts to game the system have been drastic enough that search algorithms now have features to penalize bad actors. It should be kept in mind that there's no magical panacea that can guarantee website visibility. Simply standing up a site is no guarantee that world will beat a path to your door. Once it's up, promoting your website and your business is up to you and your customers. That said, three decades out from the inception of the World Wide Web, an Internet presence is a &lt;em&gt;must-have&lt;/em&gt; for nearly any successful business. And good visibility goes a long way toward building a thriving business. To help folks create and improve a Web presence, this article provides some technical recommendations to ensure visibility. I don't intend it as a comprehensive work on search engine optimization (SEO), or, SEO, but rather as a short-list of tips, techniques and practices with a focus on technical strategies to improve Website visibility.&lt;/p&gt;
&lt;h1&gt;Number One: Verify Indexing&lt;/h1&gt;
&lt;p&gt;Search engines work by indexing websites. Programs called &lt;em&gt;web crawlers&lt;/em&gt; scour the Internet daily discovering and indexing hyperlinks to web-sites to create a sort of catalog. Content is indexed by keywords which then can be used to retrieve information. "Googling" -- using key words to search for and retrieve information on the Internet has become a fundamental skill here at the dawn of the new millennium. So the first thing you want to do to make sure your site comes up in a search is verify that it has been indexed. The easiest way to do that is use the &lt;code&gt;site:&lt;/code&gt; operator the Google search bar (or pick the search engine of your choice). &lt;/p&gt;
&lt;p&gt;For example, enter: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;site:mywebsite.com&lt;/code&gt;
And you should see a list of pages that the search engine has indexed from your site.&lt;/p&gt;
&lt;h1&gt;Number Two: Submit Your Site to Google&lt;/h1&gt;
&lt;p&gt;If you're not sure whether your site or some specific set of "pages" on your site have been indexed you can submit them for indexing using &lt;a href="https://search.google.com/search-console/about"&gt;Google Search Console&lt;/a&gt;. In general, GSC us a great tool to know about since you can get a lot of information about your site indexes, statistics related to visits, inbound hyperlinks, etc..&lt;/p&gt;
&lt;h1&gt;Three: Get External Links to your Site&lt;/h1&gt;
&lt;p&gt;Indeed, inbound links is another factor in the search ranking algorithm that can help your site percolate its way up to the top of the list. You'll want to promote your website with hyperlinks which you can get in a number of ways (through social media like &lt;em&gt;Facebook&lt;/em&gt; and &lt;em&gt;Linked In&lt;/em&gt;, for example).&lt;/p&gt;
&lt;div class="admonition hint"&gt;
&lt;p class="admonition-title"&gt;Use the Yellow Pages&lt;/p&gt;
&lt;p&gt;If your business operates locally, you should look into getting listed in the modern equivalent of a phone directory. Get listed on the Yellow Pages where you can provide links to your website, and get on other local resource directories (e.g., yelp) as well.&lt;/p&gt;
&lt;/div&gt;
&lt;h1&gt;Tip Four: Optimize your HTML for Search&lt;/h1&gt;
&lt;p&gt;While using techniques like "keyword stuffing" (i.e., repeating certain keywords over and over to boost your ranking) is bad practice (indeed, this tactic will actually &lt;em&gt;lower&lt;/em&gt; your ranking) there are a number of legitimate techniques to boost your ranking using HTML. Here are a few to get you started:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Add Metadata: Ensure each page and post has a unique title, meta description, and relevant keywords.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Structure your content: Use Headings (H1, H2, etc.). Use clear headings and subheadings.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;em&gt;Alt Text&lt;/em&gt; for Images: Include descriptive alt attributes for all images.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Improve Content Quality&lt;/h1&gt;
&lt;p&gt;Write good descriptive content that clearly sets out what you have to offer. Use keywords naturally throughout your text. But don't try to stuff your text -- that will just sound clumsy and make your attempts to manipulate the system obvious.&lt;/p&gt;
&lt;h1&gt;Technical Improvements&lt;/h1&gt;
&lt;p&gt;There are also legitimate documents you can create that are actually &lt;em&gt;intended&lt;/em&gt; to help search engines index your site. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;a href="https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview"&gt;sitemap.xml file and submit it to Google Search Console&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensure your &lt;a href="https://developers.google.com/search/docs/crawling-indexing/robots/intro"&gt;robots.txt&lt;/a&gt; file doesn't block search engines.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;a href="https://developers.google.com/search/docs/crawling-indexing/canonicalization"&gt;canonical URLs&lt;/a&gt; to indicate the preferred URL for each page within your site, avoiding duplicate content issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure you’re not using &lt;meta name="robots" content="noindex"&gt; in your HTML.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;So, there you have it. As promised, this was not a comprehensive work on SEO for web development. Instead, it's a shortlist of tips and techniques to improve your web presence and Internet visibility. I suggested ways to verify your site is indexed, checking some aspects of your HTML to help with ranking and some technical files you ensure are present to help with indexing. Also, I recommended one or two bad practices to avoid. But at the end of the day what matters most is the value of the content you provide. It's really up to you to promote your work and provide top quality products and services to build your reputation. &lt;/p&gt;</content><category term="Blog"></category><category term="search engine optimization"></category><category term="web site"></category><category term="visibility"></category><category term="business"></category><category term="improve"></category><category term="optimize"></category><category term="search"></category><category term="indexing"></category><category term="web crawlers"></category><category term="search engines"></category><category term="SEO"></category></entry><entry><title>Conceptualizing Bayes's Theorem</title><link href="https://dr-nick-nagel.github.io/blog/baysean-statistics.html" rel="alternate"></link><published>2025-01-08T00:00:00-05:00</published><updated>2025-01-08T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2025-01-08:/blog/baysean-statistics.html</id><summary type="html">&lt;p&gt;If you have an interest in machine learning and you want to get into it at some point you'll want to deepen your understanding of Bayes's theorem.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;People who know me well enough at one time or another have probably heard me say: "I love math. Why? Because I believe math is truth! Well ... unless it's statistics ... ". Of course I'm being facetious here -- I say that jokingly. In fact, quite the opposite, statistics is really about getting at truth through our inherent biases, misattributions, and misconceptions. &lt;/p&gt;
&lt;p&gt;Back when I was in grad school I spent a lot of time studying -- and later teaching -- statistics (mainly as applied to psychological research). But recently, as I've undergone a rekindled interest in consciousness and Artificial Intelligence I've been revisiting statistical approaches and methodologies. And over the course of doing so I got to thinking about Bayes' Theorem. &lt;/p&gt;
&lt;p&gt;Bayes' Theorem (and, more generally, Bayesian statistics) is a statistical approach that delves into the nature of &lt;em&gt;belief&lt;/em&gt; as much explicating mathematical principles for probabilistic inference. Consequently, over and above adding more tools to the belt for statistical analysis, conceptualizing Bayes' Theorem has profound implications relating to our &lt;em&gt;general understanding&lt;/em&gt; of probability. &lt;/p&gt;
&lt;h1&gt;Bayes' Theorem&lt;/h1&gt;
&lt;p&gt;Bayes theorem is all about &lt;em&gt;likelihood&lt;/em&gt;. Mathematically, it can be expressed in terms of probabilities. Given that it's all about belief I like to express Bayes' theorem terms of &lt;em&gt;events&lt;/em&gt; and &lt;em&gt;hypotheses&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;$$
P ( Hyp | Event ) = \frac {P ( Event | Hyp ) \times P ( Hyp ) } { P ( Event ) }
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;P(Hyp | Event)&lt;/strong&gt; represents the probability of a hypothesis being true &lt;em&gt;given&lt;/em&gt; that an event has occurred. This is the &lt;strong&gt;posterior probability&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;P(Event | Hyp)&lt;/strong&gt; represents the probability of the event occurring &lt;em&gt;given that the hypothesis is true&lt;/em&gt; (which may be referred to as the &lt;strong&gt;likelihood&lt;/strong&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;P(Hyp)&lt;/strong&gt; represents the &lt;em&gt;prior&lt;/em&gt; probability of the hypothesis &lt;em&gt;irrespective of the event&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;P(Event)&lt;/strong&gt; represents the probability of the event irrespective of the hypothesis. It may be considered as the &lt;em&gt;marginal&lt;/em&gt; likelihood of the event. &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;h3&gt;Forecasting the Weather&lt;/h3&gt;
&lt;p&gt;With anything math I always like examples. Being a New Englander and an avid outdoorsperson (also given that my father was a meteorologist) I often worry about the weather. Since, as I write this, it's January and quite cloudy outside my window, let's consider, in Bayesian terms, whether I should be concerned about snow. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let's assume the probability that it will snow in my location on the given day in January ( $P(Hyp)$ ) to be 25% (based on historical data). &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The probability that it will snow is the &lt;em&gt;prior&lt;/em&gt; probability. But I've also observed an event that should impact the prior: It's cloudy. The probability of an overcast day occurring on a January day in New England is $P(E) = 50\%$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;But in Bayesian terms that's not the whole story. Whether I should be concerned with snow given that it's cloudy is &lt;em&gt;also&lt;/em&gt; impacted by the likelihood it will be overcast if it's snowing. Given snow, it may be cloudy 99% of the time but on rare occaisions I've seen snow when it's not completely overcast. So let $P( Event | Hyp  ) = 99\%$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let's do the math. &lt;em&gt;Calculate the probability of snow in Boston on January 9th&lt;/em&gt; (the hypothesis) &lt;em&gt;given that it's overcast in the morning&lt;/em&gt; (the event)...&lt;/p&gt;
&lt;div class='latex_align'&gt;
&lt;!--NOTE: Pelican causes &amp;amp; substitution so need to wrap in div to preempt...--&gt;
$$
\begin{align}
P ( Hyp | Event ) &amp;= \frac {P ( Event | Hyp ) \times P ( Hyp ) } { P ( Event ) } \\
                  &amp;= \frac {0.99 \times 0.25 } { 0.50 } \\
                  &amp;= 0.495
\end{align}
$$
&lt;/div&gt;

&lt;p&gt;So, Bayesian analysis enables us to add information to the determination of a probability, often greatly enhancing the estimate. In the weather example, adding observational information to the equation raised the probability of snow from the baseline (prior) probability significantly.&lt;/p&gt;
&lt;h3&gt;Medical Diagnostics&lt;/h3&gt;
&lt;p&gt;Just for fun let's work through a slightly more complex example. Imagine a scenario where you want to determine the probability of a patient presenting with a particular rare condition. The condition can be detected by a test which has 99% accuracy. In other words, if the condition is present the probability that the test is positive is 99% (there is a 1% chance of a &lt;em&gt;false negative&lt;/em&gt;). If the condition is &lt;em&gt;not&lt;/em&gt; present the test will be positive 1% of the time (a false &lt;em&gt;positive&lt;/em&gt;). &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let's say the condition occurs in 1% of the population ($P(Cond) = 0.01$) . &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We know the test is 99% accurate. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the condition is present the test is positive 99% of the time ( $P(Pos|Cond) = 0.99$ ).&lt;/li&gt;
&lt;li&gt;If the condition is &lt;em&gt;not&lt;/em&gt; present the test is negative 99% of the time ( $P( Pos | \neg Cond) = 0.01$ ).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's say we have a case where a patient tests positive for the condition. The question is; what is the probability that the patient &lt;em&gt;actually has the condition&lt;/em&gt;. In Bayesian terms we ask; "What is the probability that the patient has the condition given the &lt;em&gt;evidence&lt;/em&gt; of a positive test result?" &lt;/p&gt;
&lt;p&gt;At this point we have all the information we need to apply Bayes' Theorem, but not quite in the form given above. That is, we don't have a number for $P( Pos )$ (the probability of getting a positive test result &lt;em&gt;irrespective of the condition&lt;/em&gt;). But we can determine that probability and expand the theorem to address our question. &lt;/p&gt;
&lt;p&gt;We can obtain $P(Pos)$ by collapsing conditions across the entire population:&lt;/p&gt;
&lt;p&gt;$$
P( Pos ) = P( Pos | Cond ) \times P( Cond ) + P( Pos | \neg Cond ) \times P( \neg Cond ) 
$$&lt;/p&gt;
&lt;p&gt;Given that we can expand our original formulation of Bayes' Theorem. If the hypothesis (Hyp) put to the test is that the patient has the condition, and the event (Event) is testing positive then:&lt;/p&gt;
&lt;p&gt;$$
P ( Hyp | Event ) = \frac {P ( Event | Hyp ) \times P ( Hyp ) } { P( Event | Hyp ) \times P( Hyp ) + P( Event | \neg Hyp ) \times P( \neg Hyp ) }
$$&lt;/p&gt;
&lt;p&gt;Now the calculation boils down to simple arithmetic:&lt;/p&gt;
&lt;div class='latex_align'&gt;
$$
\begin{align}
P ( Cond | Pos ) &amp;= \frac { 0.99 \times 0.01 } {(0.99)(0.01) + (0.01)(0.99)}  \\
                 &amp;= 0.50
\end{align}
$$
&lt;/div&gt;

&lt;p&gt;So, with this example, we see that it's not enough to just consider the test accuracy in gauging the probability of a correct diagnosis. Over and above that we need to consider the &lt;em&gt;frequency of the condition&lt;/em&gt; with respect to the &lt;em&gt;population writ large&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;This example demonstrates a crucial point about Bayesian probability; the &lt;em&gt;prior&lt;/em&gt; probability (the prevalence of the condition in the population) greatly influences the &lt;em&gt;posterior&lt;/em&gt; probability (the probability of having the condition &lt;em&gt;given&lt;/em&gt; a positive test result). The failure to bring the prior probability to bear on the assessment (referred to as &lt;em&gt;base-rate neglect&lt;/em&gt;) is an example of a well-known fallacy in statistical reasoning -- a form of cognitive bias that can lead to errors in judgment that depend on estimating probabilities associated with uncertain events.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In summary, Bayes' theorem defines probability in terms of evidence for a hypothesis. Key concepts include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prior Probability:&lt;/strong&gt; Our initial belief in the hypothesis before observing any evidence.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Likelihood:&lt;/strong&gt; How &lt;em&gt;likely&lt;/em&gt; observed evidence would be &lt;em&gt;if&lt;/em&gt; the hypothesis is true, and &lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;Posterior Probability:&lt;/strong&gt; Our updated belief in the hypothesis after considering the evidence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words the Bayesian interpretation of probability is one where probability expresses &lt;em&gt;a degree of belief in an event&lt;/em&gt;. A level of certainty.&lt;/p&gt;
&lt;h1&gt;Discussion: Understanding Statistics&lt;/h1&gt;
&lt;p&gt;All this bares thinking about as much as for deepening our understanding of statistics as for computational applications. Bayesian analysis actually predates in large part the formalization of what some statisticians might term "classical" hypothesis testing. Bayes theorem was originally formulated by Thomas Bayes in the 18th century. Back then many philosophers were concerned with the nature of &lt;em&gt;belief&lt;/em&gt; (and those concerns are as relevant today as ever)! Subsequent statistical approaches shifted toward methods around "proving" the "truth" of hypotheses through sampling from larger populations. Back when I was in grad school, Bayesian analysis was &lt;em&gt;re&lt;/em&gt;emerging as an analytic method often posed in contrast to formal hypothesis testing based on sampling distributions. &lt;/p&gt;
&lt;p&gt;I feel in large part the confusion people often feel around statistics stems from the tendency to conflate &lt;em&gt;belief&lt;/em&gt; with &lt;em&gt;fact&lt;/em&gt;. To fully understand the concept of probability it's critical to understand that nothing is fully certain until after the fact. &lt;/p&gt;
&lt;p&gt;In other words, to me statistics is all about &lt;em&gt;belief&lt;/em&gt;. At the heart of statistical analysis lies the notion that nothing is ever 100% certain. Frequentists have historically been concerned with drawing conclusions about &lt;em&gt;populations&lt;/em&gt; based on evidence present in &lt;em&gt;samples&lt;/em&gt;. But conclusions based on aggregate data can't be applied to individuals. Going back to the diagnosis example it's tempting to say something like, "Oh, based on your symptoms you have a 95% chance of having the disease". The fact is you either have the disease or you don't. That's a constant. Probabilistic assessments rely on variability over populations. And you can't confuse statements based on aggregate sample statistics with individual assessments. "95% of the people in this group have the disease" does not mean the same thing as saying "You have a 95% chance of having the disease". The distinction is subtle but it's not just arguing semantics. There are very real consequences of statistical fallacies!&lt;/p&gt;
&lt;p&gt;So in conclusion I'd have to say that going back and revisiting the Bayesian approach has been worth the effort. I feel I've gained some new insights into the nature of statistical reasoning and understanding probability. Bayesian inference is a key methodical approach to many machine learning applications. But key to them all is embracing uncertainty and understanding &lt;em&gt;levels of certainty&lt;/em&gt; in any sort of classification problem!&lt;/p&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://greenteapress.com/wp/think-bayes/"&gt; Think Bayes &lt;/a&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://allendowney.github.io/ThinkBayes2/"&gt; Think Bays on Github &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"&gt;Bayes' Theorem&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=HZGCoVF3YvM"&gt;3Blue1Brown&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog"></category><category term="math"></category><category term="Bayes Theorem"></category><category term="probability"></category><category term="statistics"></category><category term="neural networks"></category><category term="machine learning"></category></entry><entry><title>Implementing WebRTC Applications in Python Part I: Session Description Protocol</title><link href="https://dr-nick-nagel.github.io/blog/python_web_rtc_1.html" rel="alternate"></link><published>2024-12-20T00:00:00-05:00</published><updated>2024-12-20T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-12-20:/blog/python_web_rtc_1.html</id><summary type="html">&lt;p&gt;First is a series of blog posts exploring WebRTC application development in python.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Objective&lt;/h1&gt;
&lt;p&gt;The purpose of this article is to introduce a series blog-posts I'm writing on Web Real-Time Communications (known as WebRTC). WebRTC embodies a set of free and open  standards that enable devices connected over the Internet to communicate in real time. Taken together, WebRTC standards and protocols enable voice and video calls, live streaming, file sharing and much, much more.&lt;/p&gt;
&lt;p&gt;A detailed discussion of all the standards used in WebRTC would be quite an undertaking -- definitely too much to cover in a single blog post. So I'm carving out the coverage of this large topic into a series in which I'll explore implementations of these standards in the context of developing a streaming-media reference-application using python. &lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;If there's one thing I've learned over the years, it's the importance of standards. This became strikingly clear to me in the early '90s as the Internet and World Wide Web exploded into mainstream awareness. Back then, web pages were still in their infancy, and few people outside of universities or research institutions even knew about email. Then, almost overnight, the World Wide Web became "the next big thing," unleashing world-shaking disruptions and economic transformations.&lt;/p&gt;
&lt;p&gt;If I were to ask, "Who invented the Internet?" I'd likely get a range of answers, with few people today remembering a specific name. Back when I used to teach my course, &lt;em&gt;Exploring Internet Development&lt;/em&gt;, at Boston College, I would conduct an informal experiment. At the start of each semester I'd ask my students: "Who here has heard of O.J. Simpson?" Nearly every hand would go up. Then I'd follow with: "Who's ever heard of Tim Berners-Lee?" The room would fall silent.&lt;/p&gt;
&lt;p&gt;And yet, Berners-Lee's contribution to humanity is arguably on par with Gutenberg's invention of the printing press. By introducing HTML (a groundbreaking standard for text-based markup), and creating the first web browser, Berners-Lee revolutionized how the world accesses and consumes information. Unlike Gutenberg's printing press though, Berners-Lee's invention wasn't a patentable mechanical device but rather a set of &lt;em&gt;standards&lt;/em&gt;. &lt;em&gt;Open&lt;/em&gt; standards to be precise. These standards became the foundation of an interconnected world and helped propel the Information Age to unprecedented heights.&lt;/p&gt;
&lt;p&gt;Through the development of these standards Berners Lee and many others paved the way for the emergence of the World Wide Web and the world-wide adoption and further development of the Internet. The world simply would not be where it is today without the wide-spread adoption of standards. &lt;/p&gt;
&lt;p&gt;Much more recently, the importance of standards was brought back home to me when I undertook the architecture and implementation of a system enabling peer-to-peer (P2P) communcations for a company building security devices Interconnected over the Internet. While I'm not at liberty to get into the specifics, suffice it to say that an effort that might have taken months to complete took merely a handful of days thanks to the recent development and application of a relatively new set of WWW standards -- those revolving around Web Real-Time Communcations, or, &lt;em&gt;WebRTC&lt;/em&gt;. &lt;/p&gt;
&lt;h1&gt;WebRTC: Emerging Standards for Peer-to-Peer Communications&lt;/h1&gt;
&lt;p&gt;&lt;span id='note_1'&gt;So, what exactly is WebRTC?&lt;sup&gt;&lt;a href="#endnote_1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; &lt;strong&gt;WebRTC (Web Real-Time Communications)&lt;/strong&gt; is not just one but rather a &lt;em&gt;set&lt;/em&gt; of open standards, protocols and APIs that transcends any one particular product or platform. Its purpose is to enable Internet connected users to communicate in real-time by defining protocols for steaming data among interconnected &lt;em&gt;peers&lt;/em&gt; (P2P communications). &lt;/p&gt;
&lt;div style='text-align:center;width:350px;margin:auto'&gt;

    &lt;img alt='P2P Illustrated'
         src='/diagrams/P2P_illustrated.png' 
         width='300px'/&gt;

    &lt;div style="font-size:smaller"&gt;&lt;span style="font-weight:bold"&gt;Figure 1.&lt;/span&gt; The P2P network architecture enabled by WebRTC.&lt;/div&gt;  

&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Figure 1&lt;/em&gt; illustrates the P2P network communications architecture enabled by WebRTC. Network connected devices can communicate as &lt;em&gt;peers&lt;/em&gt; over the WWW with protocols designed to enable Network Address Translation (NAT) and firewall traversal using specialized relay servers where necessary. WebRTC supports a vast range of use-cases from simple messaging across the &lt;em&gt;Internet of Things&lt;/em&gt; to streaming video, screen-share, real-time collaboration tools, multi-player gaming ~~ the possibilities are endless.&lt;/p&gt;
&lt;p&gt;Officially standardized in 2021 through efforts conducted under the auspices of the &lt;em&gt;World Wide Web Consortium&lt;/em&gt; (W3C) and the &lt;em&gt;Internet Engineering Task Force&lt;/em&gt; (IETF), WebRTC is now supported by most major browsers and communications platforms vastly facilitating development efforts revolving around network communications. &lt;/p&gt;
&lt;h2&gt;WebRTC Protocols and Standards&lt;/h2&gt;
&lt;p&gt;Among others, core WebRTC protocols include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SDP (Session Description Protocol):&lt;/strong&gt; used to exchange information about the media streams (audio, video, data) being transmitted, including codecs, bitrates, and encryption parameters,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ICE (Interactive Connectivity Establishment):&lt;/strong&gt; a framework enabling WebRTC peers to discover and connect with each other, even when they are behind different types of NAT devices or firewalls,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;STUN (Session Traversal Utilities for NAT):&lt;/strong&gt; helps peers determine public IP addresses and ports enabling the establishment of direct channels for communication, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TURN (Traversal Using Relays around NAT):&lt;/strong&gt; which provides a standardized means to create relay servers that can be used as a fallback option when direct connections are not possible due to strict NAT configurations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional standards include: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RTP (Real-time Transport Protocol):&lt;/strong&gt; the underlying protocol for transmitting media over the Internet, along with &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VP8, VP9 and Opus:&lt;/strong&gt; audio and video codecs used for encoding and decoding a/v streams.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's a lot of protocols! But, taken together, they support and facilitate the development of a vast range of application use-cases!&lt;/p&gt;
&lt;h2&gt;Session Description Protocol&lt;/h2&gt;
&lt;p&gt;Again, a comprehensive discussion of all the standards associated with WebRTC is too much for a single blog post and instead I'll cover a number of standards over the course of this series. &lt;span id="note_2"&gt;But to kick things off, I'll open up the discussion with an exploration of SDP&lt;sup&gt;&lt;a href="#endnote_2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt; which is fundamental to &lt;em&gt;all&lt;/em&gt; WebRTC applications.&lt;/p&gt;
&lt;div class="admonition hint"&gt;
&lt;p class="admonition-title"&gt;Session Description Protocol&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Session Description Protocol (SDP)&lt;/strong&gt; is a well-defined format for conveying sufficient information to discover and participate in a multimedia session.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;SDP is fundamental inasmuch as that it paves the way for any sort of P2P data transmission over the Internet. SDP defines a &lt;em&gt;handshake&lt;/em&gt;; the procedure enabling devices to negotiate the parameters of the transmission. I'm a visual thinker and so to better understand the protocol I've created a diagram to illustrate the way the SDP handshake works.&lt;/p&gt;
&lt;div style='text-align:center;font-size:smaller;width:400px;margin:auto'&gt;
    &lt;img alt='INSERT THREADED UI DIAGRAM'
         src='/diagrams/webrtc_signalling.drawio.svg' 
         width='400px'/&gt;
    &lt;div&gt;&lt;span style='font-weight:bold'&gt;Figure 2.&lt;/span&gt; Illustration of the SDP &lt;em&gt;signalling&lt;/em&gt; protocol.&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Let's call the peers participating in the exchange &lt;em&gt;senders&lt;/em&gt; and &lt;em&gt;receivers&lt;/em&gt;. The WebRTC session begins with the exchange of information between the peers (referred to as &lt;em&gt;signaling&lt;/em&gt;) in the form of SDP &lt;em&gt;offers&lt;/em&gt; and &lt;em&gt;answers&lt;/em&gt;. These structured text documents contain all the meta-information about a multimedia session, such as the media types, codecs, transport protocols, and other relevant parameters. If you're curious about the format, have a look at a couple of sample documents which I've captured using a python test GUI I whipped up for this effort. &lt;span id="append_2"&gt;I've added the samples as an &lt;a href="#appendix_2"&gt;appendix&lt;/a&gt;&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2&lt;/em&gt; illustrates the protocol.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;sender&lt;/strong&gt; creates an &lt;em&gt;offer&lt;/em&gt; which includes information the receiver will need to receive the transmission. The sender sets the offer as its &lt;em&gt;local description&lt;/em&gt; and then sends it to the &lt;em&gt;receiver&lt;/em&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On receiving the offer, the &lt;strong&gt;receiver&lt;/strong&gt; sets it as its &lt;em&gt;remote description&lt;/em&gt; and generates an &lt;em&gt;answer&lt;/em&gt; which it sets as its &lt;em&gt;local description&lt;/em&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Then The receiver sends the answer to the sender which sets it as its remote description.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With this in mind we can start looking at a series of python code examples embodying these concepts.&lt;/p&gt;
&lt;h1&gt;Coding WebRTC Applications in Python Example 1: Signaling&lt;/h1&gt;
&lt;p&gt;WebRTC signaling objects and methods are defined on the &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection"&gt;RTCPeerConnection interface&lt;/a&gt;. The interface defines &lt;em&gt;local&lt;/em&gt; and &lt;em&gt;remote&lt;/em&gt; &lt;em&gt;session description&lt;/em&gt; attributes (encapsulated in the offer and answer documents). &lt;/p&gt;
&lt;p&gt;A python implementation of the &lt;em&gt;RTCPeerConnection&lt;/em&gt; interface is available in the &lt;em&gt;aiortc&lt;/em&gt; package. &lt;a href="https://github.com/aiortc/aiortc"&gt;aiortc&lt;/a&gt; is a python library that provides WebRTC functionality using the &lt;em&gt;asyncio&lt;/em&gt; framework. It allows for asynchronous management of WebRTC connections, handling signaling, and managing media streams. I'll be relying on this package over the course of this blog series. &lt;/p&gt;
&lt;p&gt;The first set of abstractions I find useful in using &lt;code&gt;aiortc&lt;/code&gt; are python classes encapsulating &lt;em&gt;sender&lt;/em&gt; and &lt;em&gt;receiver&lt;/em&gt; attributes and methods. Both sender and receiver objects will hold &lt;code&gt;RTCPeerConnection&lt;/code&gt; references. To start things off, I defined two classes; &lt;code&gt;DataSender&lt;/code&gt; and &lt;code&gt;DataReceiver&lt;/code&gt; for simple data transmission over a &lt;em&gt;data channel&lt;/em&gt;. Later in this series I'll do a deep dive into transmission involving streaming media.&lt;/p&gt;
&lt;h2&gt;The DataSender Class&lt;/h2&gt;
&lt;p&gt;The following sample code includes fragments from the &lt;code&gt;DataSender&lt;/code&gt; class relevant to the present discussion. &lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
class DataSender : 
    '''
    This class encapsulates signalling associated with a 
    WebRTC 'sender' (i.e., the object associated with 
    transmitting data from *producer* to *consumer*). 
    '''

    def __init__ ( self ) :
        &lt;font color='#f88'&gt;self.pc = RTCPeerConnection()&lt;/font&gt;
        &lt;font color='#8f8'&gt;# continue initialization ...&lt;/font&gt;

    async def handle_offer_request ( self ) : 
        self.data_channel = self.pc.createDataChannel( "nn channel 1" )
        &lt;font color='#f88'&gt;offer = await self.pc.createOffer()
        await self.pc.setLocalDescription( offer )
        offer_desc = self.pc.localDescription&lt;/font&gt;
        return offer_desc

    async def handle_answer( self, answer_description ) :
        &lt;font color='#f88'&gt;await self.pc.setRemoteDescription( answer_description )&lt;/font&gt;

    &lt;font color='#8f8'&gt;# Continue DataSender custom methods...&lt;/font&gt;

    &lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As illustrated in Figure 2 WebRTC signaling starts with an 'offer' from the &lt;em&gt;sender&lt;/em&gt;. In this implementation, the &lt;code&gt;DataSender&lt;/code&gt; class is responsible for handling "sender side" signaling events. The relevant methods are &lt;code&gt;handle_offer_request&lt;/code&gt; and &lt;code&gt;handle_answer&lt;/code&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;span id="note_4"&gt;&lt;code&gt;handle_offer_request&lt;/code&gt; assumes it will receive a request from some entity for an offer describing the nature of the transmission it is prepared to send &lt;sup&gt;&lt;a href="#endnote_4"&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;/span&gt;. &lt;code&gt;DataSender&lt;/code&gt; handles the request by: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;generating an &lt;em&gt;offer&lt;/em&gt; using its instance of &lt;code&gt;RTCPeerConnection&lt;/code&gt;, and &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;setting the returned offer description object as the &lt;code&gt;RTCPeerConnection&lt;/code&gt;'s  &lt;code&gt;localDescription&lt;/code&gt; attribute.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="admonition important"&gt;
&lt;p class="admonition-title"&gt;Important&lt;/p&gt;
&lt;p&gt;Notice how &lt;code&gt;handle_offer_request&lt;/code&gt; returns the offer encapsulated in an aiortc &lt;code&gt;RTCSessionDescription&lt;/code&gt; object. The offer will be used later by the receiver class to generate an SDP answer.&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;handle_answer&lt;/code&gt;. Once the offer is sent off to the receiver object it will generate an answer which will be sent back to &lt;code&gt;DataSender&lt;/code&gt;. &lt;code&gt;handle_answer&lt;/code&gt; completes the SDP exchange by setting the answer description as its &lt;em&gt;remote description&lt;/em&gt; for the session.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above fragments encapsulate a basic signaling example from the sender side of the exchange. Next let's look at the receiver side.&lt;/p&gt;
&lt;div class="admonition tip"&gt;
&lt;p class="admonition-title"&gt;Tip&lt;/p&gt;
&lt;p&gt;The reader may have noticed that this simplified example has only one RTCPeerConnection instance to participate in an exchange. The example can be readily extended to handle multiple recipients by adding additional &lt;code&gt;RtcPeerConnection&lt;/code&gt; instances. In other words, the cardinality on &lt;code&gt;RtcPeerConnection&lt;/code&gt;'s for sender to recipients is &lt;strong&gt;1:N&lt;/strong&gt; .&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;The DataReceiver Class&lt;/h2&gt;
&lt;p&gt;Next we have the &lt;code&gt;DataReceiver&lt;/code&gt; class.&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
class DataReceiver : 
    '''
    This class encapsulates signalling associated with a 
    WebRTC 'receiver' (i.e., the object associated with 
    consuming data from a producer). 
    '''
    def __init__ ( self ) :
        &lt;font color='#f88'&gt;self.pc = RTCPeerConnection()&lt;/font&gt;
        &lt;font color='#8f8'&gt;# continue initialization ...&lt;/font&gt;

    async def handle_offer ( self, offer_description ) : 
        &lt;font color='#f88'&gt;await self.pc.setRemoteDescription( offer_description )
        answer = await self.pc.createAnswer()
        await self.pc.setLocalDescription( answer )
        answer_desc = self.pc.localDescription&lt;/font&gt;
        return answer_desc

    &lt;font color='#8f8'&gt;# Continue DataReceiver custom methods...&lt;/font&gt;

    &lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Again we use an &lt;code&gt;RTCPeerConnection&lt;/code&gt; to handle the SDP. Here, we handle the sender's  &lt;em&gt;offer&lt;/em&gt; by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Setting it as the receiver &lt;code&gt;RTCPeerConnection&lt;/code&gt;'s &lt;code&gt;localDescription&lt;/code&gt;, &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generating an &lt;em&gt;answer&lt;/em&gt;, and &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Returning the &lt;em&gt;answer&lt;/em&gt; encapsulated in an &lt;code&gt;RTCSessionDescription&lt;/code&gt; object so that it can be sent back to the sender.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So the receiver class encapsulates handling an SDP exchange on the receiver side of the transmission.&lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;In the above examples, we see a very basic implementation of an SDP exchange defined using aiortc &lt;code&gt;RTCPeerConnection&lt;/code&gt;'s on python &lt;em&gt;sender&lt;/em&gt; and &lt;em&gt;receiver&lt;/em&gt; classes. If you're new to WebRTC and especially if you're new to networking concepts in general that may seem like a lot to take in! But as the examples show aiortc provides nice implementations to handle a lot of the low-level details required to generate the offer/answer session descriptions. You just have to know how to use them and what to expect when you do so!&lt;/p&gt;
&lt;p&gt;Since it may be a lot to digest I'll leave off for now and pick up from here in subsequent posts where I'll get into STUN, NAT traversal using TURN, and ultimately streaming media. But before leaving off, it's well worth saying a few words about &lt;em&gt;RTCPeerConnection state&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;RTCPeerConnection State Changes&lt;/h2&gt;
&lt;p&gt;WebRTC peers transition through many states over the life cycle of a connection. It's very important to understand these states and associated state-transition triggers. The &lt;em&gt;RTCPeerConnection&lt;/em&gt; interface defines a high-level read-only property, &lt;code&gt;connectionState&lt;/code&gt;, which can be used to inspect the state of a peer connection over the course of its life cycle for purposes of development, error-handling and trouble shooting. The following diagram illustrates the possible states reflected by this property. &lt;/p&gt;
&lt;div style='font-size:smaller;width:300px;margin:auto'&gt;
    &lt;img alt='RtcPeerConnection States'
         src='/diagrams/rtc_peer_conn_states.svg' 
         width='300px' 
         style='width:300px; margin:auto;margin-bottom: 1em'
         /&gt;
     &lt;div&gt;&lt;span style='font-weight:bold'&gt;Figure i:&lt;/span&gt; RTCPeerConnection object states.&lt;/div&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;new&lt;/strong&gt;: The connection object has been created but there is not yet any network activity associated with it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;connecting&lt;/strong&gt;: Participating WebRTC peers are negotiating transmission parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;connected&lt;/strong&gt;: A connection between peers has been successfully negotiated and is operational. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;failed&lt;/strong&gt;: The connection could not be established.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;disconnected&lt;/strong&gt;: The connection is temporarily disconnected due to network issues.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;closed&lt;/strong&gt;: The connection is closed.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When a change in connection state is triggered a &lt;code&gt;connectionstatechange&lt;/code&gt; event is dispatched to the &lt;code&gt;RTCPeerConnection&lt;/code&gt; object owning the connection. The following code fragment shows how to handle the event using an &lt;code&gt;aiortc.RTCPeerConnection&lt;/code&gt; instance.&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
&lt;font color='#afa'&gt;# Given an RTCPeerConnection instance, pc, define a callback 
# to handle connection-state transitions ...&lt;/font&gt;

@pc.on("connectionstatechange")
async def on_connection_state_change():
    print(f"Connection state changed: {pc.connectionState}")
    if pc.connectionState == "connected":
        &lt;font color='#afa'&gt;# handle transition to connected state...&lt;/font&gt;
    elif pc.connectionState == "failed":
        &lt;font color='#afa'&gt;# handle transition to failed state...&lt;/font&gt;
        await pc.close()
    elif pc.connectionState == "disconnected":
        &lt;font color='#afa'&gt;# handle transition to disconnected state...&lt;/font&gt;
    elif pc.connectionState == "closed":
        &lt;font color='#afa'&gt;# handle transition to closed state (may required 
        # release of allocated resources...)&lt;/font&gt;
    &lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice how "cleanup" should be performed on transition of the connection to 'closed' state ensuring robust handling of transitions while preventing resource leaks.&lt;/p&gt;
&lt;h1&gt;Summary and Next Steps&lt;/h1&gt;
&lt;p&gt;In summary, this blog-post is the first in a series exploring WebRTC application development in python. The scope of this post was limited to covering SDP -- arguably the most fundamental protocol in WebRTC since it sets the stage for many types of data transmission between peers. In this post we saw:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A high-level description of the SDP protocol,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sample python code implementing SDP signaling using aiortc, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A discussion of RTCPeerConnection states over the life cycle of a WebRTC session.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Subsequent articles in this series will explore network traversal (using STUN and TURN), media streaming, and real-time integration of machine learning in python WebRTC applications.&lt;/p&gt;
&lt;h1&gt;End notes&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;p id="endnote_1"&gt;Over the course of researching my needs for a WebRTC project (in which I'm engaged at the time of this writing) I came across a &lt;a href="https://www.digitalsamba.com/blog/webrtc-market-trends-predictions-for-2023"&gt;"blog post"&lt;/a&gt; purporting to explain &lt;em&gt;WebRTC&lt;/em&gt;. The problem is the post contains a lot of misinformation -- information which has the potential to mislead decision makers and impact progress on the development and adoption of standards and supporting technologies. Consequently, I feel the need here to "set the record straight" regarding some key points.&lt;p&gt;&lt;/p&gt;
&lt;p&gt;The blog post in question asserts that WebRTC is "...not a protocol" instead it is a "project". Actually, half the point of the present post is that WebRTC is a &lt;em&gt;set&lt;/em&gt; of protocols and standards, &lt;em&gt;open&lt;/em&gt; standards, designed to support the development of &lt;em&gt;de&lt;/em&gt;centralized (P2P) web-based communications. The article goes on to assert that the &lt;em&gt;WebRTC&lt;/em&gt; "project" is owned by Google. The fact that WebRTC standards are &lt;em&gt;open&lt;/em&gt; means that they are not "owned" by any one individual or company. WebRTC standards and protocols may be adopted by anyone anywhere who is willing and able to provide standards-compliant implementations. Implying otherwise insults the invaluable effort and work of individuals and organizations who contribute to open standards and the development of open software systems from which all companies across the board benefit. &lt;a href="#note_1"&gt;&amp;uarr;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;p id="endnote_2"&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc4566"&gt;SDP: Session Description Protocol&lt;/a&gt; . &lt;a href="#note_2"&gt;&amp;uarr;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;p id="endnote_3"&gt;&lt;a href="https://www.electronjs.org/"&gt;Electron&lt;/a&gt; is a framework for building beautiful cross-platform desktop applications using HTML, JavaScript and CSS. &lt;a href="#note_3"&gt;&amp;uarr;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;p id="endnote_4"&gt;SDP in-and-of-itself does not specify a mechanism for requesting an offer. A framework for doing so is defined in &lt;a href="https://www.ietf.org/rfc/rfc3264.txt"&gt;RFC3264&lt;/a&gt; (commonly referred to as the &lt;em&gt;offer/answer model&lt;/em&gt;). For present purposes assume an external entity issues an offer request. Later in the series I'll be covering cases where the external entity is a user device (e.g., a desktop computer). &lt;a href="#note_4"&gt;&amp;uarr;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;&lt;span id="appendix_2"&gt;Appendix 1: Sample SDP Documents &lt;a href="#append_2"&gt;&amp;uarr;&lt;/a&gt;&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;This appendix provides a sample SDP document in order to help gain better understanding of the SDP format. The document comprises an offer defining parameters for information exchange using a WebRTC &lt;em&gt;datachannel&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Sample SDP Offer&lt;/h3&gt;
&lt;pre style='font-size: smaller'&gt;
v=0
o=- 3939468060 3939468060 IN IP4 0.0.0.0
s=-
t=0 0
a=group:BUNDLE 0
a=msid-semantic:WMS *
m=application 47419 DTLS/SCTP 5000
c=IN IP4 192.168.1.158
a=mid:0
a=sctpmap:5000 webrtc-datachannel 65535
a=max-message-size:65536
a=candidate:92f48e2b25b0cf96833e74c0e6d4b612 1 udp 2130706431 192.168.1.158 47419 typ host
a=candidate:0d0b6cc018d79c453c3f8b96c8c6a899 1 udp 1694498815 71.184.100.169 47419 typ srflx raddr 192.168.1.158 rport 47419
a=end-of-candidates
a=ice-ufrag:WnTS
a=ice-pwd:TNMCpvqGZ4terU2c1tbn6w
a=fingerprint:sha-256 AB:FA:49:9D:BD:DA:63:82:E6:C4:CA:D2:06:8C:15:6D:A5:2D:B6:3D:32:6A:F7:B0:EE:FF:82:5D:3A:9B:B0:88
a=setup:actpass
&lt;/pre&gt;

&lt;h3&gt;Analysis&lt;/h3&gt;
&lt;p&gt;SDP documents are a text-based machine-readable format defined so as to provide sufficient information for network data transmission of a range of media types. Without getting too deep into the details, cursory examination of the specimen reveals some &lt;em&gt;key properties&lt;/em&gt; of the format:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Media Type:&lt;/strong&gt; The media type offered in the transmission (in this sample &lt;em&gt;webrtc-datachannel&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;IP Address and Port:&lt;/strong&gt; Key for transmission over the internet, participating peers must provide IP and port addressing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Transport Protocols:&lt;/strong&gt; Required to set up media transport mechanisms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ICE (Interactive Connectivity Establishment) Candidates:&lt;/strong&gt; Used for network traversal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DTLS Fingerprints:&lt;/strong&gt; SHA-256 fingerprints provided for DTLS authentication.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, this sample SDP offer describes a WebRTC data channel. It specifies the network parameters, security mechanisms, and data channel capabilities required for the transfer. The offeror is proposing to establish a secure data channel using DTLS and SCTP protocols. The ICE parameters facilitate network traversal to establish the connection.&lt;/p&gt;
&lt;p&gt;This type of SDP offer is commonly used in WebRTC applications for various purposes, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;file transfer&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;real-time messaging:&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;custom protocol implementation&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://w3c.github.io/webrtc-pc/"&gt;W3C WebRTC Specification&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://datatracker.ietf.org/doc/html/rfc8866"&gt;Session Description Protocol&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.ietf.org/rfc/rfc3264.txt"&gt;RFC3264: An Offer/Answer Model with the Session Description Protocol (SDP)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/aiortc/aiortc"&gt;aiortc&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://w3c.github.io/webrtc-pc/#dom-rtcpeerconnection"&gt;RTCPeerConnection interface&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/connectionState"&gt;RTCPeerConnection: connectionState property&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog"></category><category term="python"></category><category term="computer science"></category><category term="asynchronous"></category><category term="code"></category><category term="inter process communication"></category><category term="I/O"></category><category term="WebRTC"></category><category term="networking"></category><category term="streaming"></category><category term="media"></category><category term="standards"></category><category term="SDP"></category><category term="session description protocol"></category></entry><entry><title>Thin Veil Project</title><link href="https://dr-nick-nagel.github.io/blog/thin-veil.html" rel="alternate"></link><published>2024-12-08T00:00:00-05:00</published><updated>2024-12-08T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-12-08:/blog/thin-veil.html</id><summary type="html">&lt;p&gt;Sketchwork and prototypes for the thin veil project...&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Notes on the Thin Veil&lt;/h1&gt;
&lt;p&gt;Sketchwork and prototypes for the thin veil.&lt;/p&gt;
&lt;h1&gt;Dunkirk&lt;/h1&gt;
&lt;p&gt;This is a &lt;em&gt;very&lt;/em&gt; preliminary sketch for Dunkirk.&lt;/p&gt;
&lt;div style='text-align:center'&gt;
    &lt;img alt='INSERT Dunkirk'
         src='/svg/dunkirk_wip_plain.svg'
         width='600px'/&gt;
    &lt;div&gt;Dunkirk...&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Early experiments with SVG effects like flame and billowing smoke...&lt;/p&gt;
&lt;h1&gt;Princess of Cups&lt;/h1&gt;
&lt;div style='text-align:center'&gt;
    &lt;img alt='INSERT Princess of Cups'
         src='/svg/wave_effect_0.2.svg'
         width='200px'/&gt;
    &lt;div&gt;Princess of Cups ...&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Towards a billowing effect ...&lt;/p&gt;
&lt;h1&gt;Crowley Char Type&lt;/h1&gt;
&lt;div style='text-align:center'&gt;
    &lt;img alt='INSERT Crowley type character desgin'
         src='/images/crowley/crowley.svg'
         width='400px'/&gt;
    &lt;div&gt;Aleister Crowley type character.&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;This is a prototype for an "Aleister Crowley like" character as it might appear in a web-based graphic novel -- think "web comic". &lt;/p&gt;
&lt;p&gt;Keep in mind -- this is just a very rough early stage prototype for brain-storming. The purpose is just to break the ice and get some character sheets going...&lt;/p&gt;
&lt;p&gt;As a reference point, someone has published a literal &lt;a href="https://www.amazon.com/Aleister-Adolf-Douglas-Rushkoff/dp/1506701043"&gt;Aleister Crowley graphic novel&lt;/a&gt; ...&lt;/p&gt;</content><category term="Blog"></category><category term="svg"></category><category term="illustration"></category><category term="graphic novel"></category><category term="web-based"></category><category term="sequential"></category><category term="art"></category></entry><entry><title>Process Managment in Python Part 2: Multi-threaded GUIs</title><link href="https://dr-nick-nagel.github.io/blog/python_process_management_2.html" rel="alternate"></link><published>2024-11-16T00:00:00-05:00</published><updated>2024-11-16T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-11-16:/blog/python_process_management_2.html</id><summary type="html">&lt;p&gt;Patterns for process management and threading in python with graphical user interfaces.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In a &lt;a href="https://dr-nick-nagel.github.io/blog/python_process_management_1.html"&gt;previous post&lt;/a&gt; I blogged about a &lt;em&gt;mediator&lt;/em&gt; pattern, for python development to direct the instantiation, execution and management of subprocesses in an application. In that post I concluded with a promise that I'd have more to say about using processes and threads in the context of &lt;em&gt;Graphical User Interface Development&lt;/em&gt;, and, well, here we are. My aim in this present piece is to tie together the concepts and patterns from those previous articles -- those revolving around using asynchronous python code, sub-processes and &lt;em&gt;threads&lt;/em&gt; -- with a focus on applying them to GUI development.&lt;/p&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Back in olden days, before GUIs became ubiquitous, programmers used to create applications using &lt;em&gt;command-line interfaces&lt;/em&gt; (CLIs). Life seems to have been so much simpler back then compared to the exponentially increasing complexity that surrounds us today. But we are where we are only by virtue of standing (and building!) on the shoulders of giants, and so, perhaps ironically, in order to move forward one must look backward every now and then.&lt;/p&gt;
&lt;h2&gt;My Favorite Programming Tool: The Command Line&lt;/h2&gt;
&lt;p&gt;Of course I say all that "tongue-in-cheek". As anyone with any interest in programming must surely know, the CLI remains a very important tool to this day and many programs are produced requiring interaction with the command line. That said, the CLI can, at times, feel mysterious, somewhat arcane, and in need of demystification. A big part of the beauty of the command-line lies in the simplicity of the interface. Interacting with a well-designed command-line tool or application can feel like having a conversation. You provide a command, the system might respond with some requests for information, and then it generates your results. &lt;/p&gt;
&lt;h2&gt;Enter the GUI&lt;/h2&gt;
&lt;p&gt;All that being said, as applications grow and become increasingly complex the need for a GUI to facilitate user-interaction becomes manifest. To make the discussion somewhat concrete, lately I've been working on implementing a networking protocol and needed a prototype to establish the basis for a distributed system. In order to conduct the analyses I needed to build the prototype, I reached a point where I needed to stand up a GUI as a sort of &lt;em&gt;harness&lt;/em&gt; for development. In part, this series of posts documents some of the issues I encountered and some of the pain-points along the way.&lt;/p&gt;
&lt;h1&gt;Reading Subprocess Output with Threads&lt;/h1&gt;
&lt;p&gt;One of the problems I encountered revolved around the need to execute and coordinate subprocesses which had been previously defined as python console-applications. As noted and discussed in detail elsewhere, the problem with executing subprocesses -- and indeed any &lt;em&gt;asynchronous&lt;/em&gt; code -- in a GUI app is that GUIs are event driven. They execute in a continuous loop that will &lt;em&gt;block&lt;/em&gt; if it gets caught up in a lengthy routine or if a subprocess spawned by the GUI blocks for some reason. Again, &lt;a href="https://dr-nick-nagel.github.io/blog/gui-asynchronous.html"&gt;I've discussed GUI event loops and asynchronous development elsewhere&lt;/a&gt;, so won't get into the details of the event-loop here. Instead I'd like to extend the previous discussions with a focus on reading and writing to subprocesses using threads.&lt;/p&gt;
&lt;div class="admonition warning"&gt;
&lt;p class="admonition-title"&gt;The Lost art of Multi-threaded Application Development&lt;/p&gt;
&lt;p&gt;Over the course of researching solutions to best meet my needs I came across many knee-jerk reactions against using threads in python development. Many developers are cautious about delving into multi-threaded applications due to the challenges posed by race conditions, dead-lock and the complexity of synchronizing on data structures. Instead, we are admonished to favor asynchronous approaches and non-blocking I/O with select API's. And all that is generally true. But, that being said, there are some use-cases where multi-threaded solutions work best and the capability to use threads becomes mission critical. In the example below I've simulated a use-case that blocks (awaiting user input) and an approach using &lt;code&gt;async&lt;/code&gt; simply won't work.&lt;/p&gt;
&lt;/div&gt;
&lt;h1&gt;Visualizing the Solution&lt;/h1&gt;
&lt;p&gt;I am a visual thinker -- I like visualizing solutions and I'm big fan of a good diagram. So before jumping into the code I've created a visualization of the solution we're about to dig into. Please indulge me as I walk through it.&lt;/p&gt;
&lt;div style='text-align:center'&gt;
&lt;img alt='INSERT THREADED UI DIAGRAM'
     src='/diagrams/processes_and_threads.drawio.svg' 
     width='400px'/&gt;
&lt;/div&gt;

&lt;p&gt;Figure 1 illustrates reading subprocess output with threads in a GUI driven application. Here's the analysis.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, notice the illustration of the &lt;strong&gt;process pipe&lt;/strong&gt; representing the sub-process &lt;em&gt;standard output&lt;/em&gt;. I kind of like the pipe analogy, so much so that I've gone ahead and drawn a little valve on it. I did so to highlight the point that &lt;em&gt;this is what will block the GUI in an application that reads output from a sub-process&lt;/em&gt;. If the subprocess reaches a point where it awaits input (obtained through another pipe -- namely &lt;code&gt;stdin&lt;/code&gt;) Then it's &lt;code&gt;stdout&lt;/code&gt; which will be blocked. The valve is a reminder that the pipe can be shut off at times. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;So if you want to interact with a long-running process from a GUI, you'll want to &lt;em&gt;spin off the reading of that process's &lt;code&gt;stdout&lt;/code&gt; pipe to a&lt;/em&gt; &lt;strong&gt;&lt;em&gt;dedicated thread&lt;/em&gt;&lt;/strong&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Under such conditions, the safest way to communicate information back to the main thread (here, the one executing the &lt;em&gt;tkinter loop&lt;/em&gt;) is to use a &lt;strong&gt;queue&lt;/strong&gt;. Recall that a queue is a data structure that encapsulates a "First-In-First-Out" (FIFO) data processing routine. The queue defines methods for retrieving data in the order it's entered. The good news is that python provides pre-defined queue-type data structures that are said to be "thread safe". Probably.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The thread reading the output (the &lt;em&gt;output reader&lt;/em&gt;) executes a &lt;em&gt;read loop&lt;/em&gt; wherein it simply; (1) reads a line at a time from the pipe, and (2) puts each line read into a shared queue. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Meanwhile, the main thread sits there continuously executing the tkinter event loop. This is where we can tie in to periodically update tkinter GUI views using &lt;code&gt;await&lt;/code&gt; to read new line items should they become available on the queue. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In essence, what we have here is a classic &lt;em&gt;producer/consumer&lt;/em&gt; use-case. The &lt;em&gt;output reader&lt;/em&gt; is a producer who's role is to populate the process output queue with data as it becomes available over the course of the application life cycle. The main thread is the consumer, which periodically dequeues information and updates a view in the GUI. &lt;/p&gt;
&lt;p&gt;With this visualization in mind, let's look at a concrete example...&lt;/p&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;h2&gt;Listing 1&lt;/h2&gt;
&lt;p&gt;The first listing is a just some scaffolding I put up to test the process. It's just a simple python script that prints some output, asks for end-user input, and echos that input back to the end-user. All in all, pretty straightforward. The thing to notice though, is the part that gets user input using the python &lt;code&gt;input()&lt;/code&gt; function (which I've highlighted in red). &lt;code&gt;input&lt;/code&gt; causes the program to block and wait for the end-user to enter data. Normally, this is what we'd want since it enables interaction via the console through &lt;code&gt;stdin&lt;/code&gt; (by default the keyboard in a console app). The problem is that if we run this script from a tkinter GUI app the GUI will freeze up when the script hits this part of the application.&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
import sys
import argparse
import asyncio

def send_output ( text ) :
    print( text )
    sys.stdout.flush()

&lt;font color='#F89'&gt;def get_input () : 
    print( "Enter Text Input ... " )
    test_in = input()
    sys.stdout.flush(  )
    return test_in&lt;/font&gt;

def echo_input( obtained_in ) : 
    send_output( obtained_in )

async def send_async_output (text) :
    print( text ) 

def test_send_async ( text ) :
    loop.run_until_complete( send_async_output( text ) )

if __name__ == "__main__" :
    print( "----    START TEST    ----" )
    parser = argparse.ArgumentParser (
        prog = 'nn_testscript',
        description = 'Test scaffolding for process mediator pattern',
        epilog = '\u266B Always look on the bright side of life... '
    )
    parser.add_argument( 'instance_label' )
    args = parser.parse_args()
    print( sys.argv[0] )
    instance_label = args.instance_label
    print(f"Instance: {instance_label}")
    loop = asyncio.new_event_loop()
    send_output ( "Testing 1, 2, 3" )
    &lt;font color='#F89'&gt;test_in = get_input()&lt;/font&gt;
    test_send_async( f"ECHO: {test_in}" )
    print( "----    END TEST    ----" )
    &lt;/pre&gt;
&lt;/div&gt;

&lt;h2&gt;Listing 2&lt;/h2&gt;
&lt;p&gt;Listing 2 (below) comprises a simple test-GUI (using &lt;em&gt;tkinter&lt;/em&gt;) which I whipped up to demonstrate the process. The listing shows how the GUI code:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Launches the test script above using a &lt;em&gt;process mediator&lt;/em&gt;, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enables interaction through GUI controls.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case there are only a few widgets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A &lt;em&gt;text area&lt;/em&gt; to display the process output,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An &lt;em&gt;entry field&lt;/em&gt; to enable an end-user to enter data,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;em&gt;launch&lt;/em&gt; button to launch the subprocess (executing the test script), and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;em&gt;send&lt;/em&gt; button to send data to the process via its &lt;code&gt;stdin&lt;/code&gt; pipe.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
import tkinter as tk
from process_mediator import ProcessDirector

# Main application class with tkinter GUI
class App:
    def __init__( self, root ):
        self.root = root
        self.root.title("Read/Write Process Thread Example")
        # Instantiate a 'process mediator'
        self.process_mediator = ProcessDirector()
        # Create GUI elements
        self.text_output = tk.Text(root, wrap='word', height=20, width=40)
        self.text_output.pack(pady=5)
        self.entry_input = tk.Entry(root)
        self.entry_input.pack(pady=5)

        self.start_button1 = tk.Button( 
            root, 
            text="Launch Subprocess", 
            command=lambda:self.start_process( TEST_SCRIPT )
        )
        self.start_button1.pack( side=tk.LEFT, padx=5 )

        self.send_button1 = tk.Button(
            root, 
            text="Send Input", 
            command=self.send_input
        )
        self.send_button1.pack(side=tk.LEFT, padx=5)

        # Start updating output display
        self.update_output()

    def start_process(self, script_name):
        """Start the specified process."""
        self.subProcId = &lt;font color='#F67'&gt;self.process_mediator.launch_process( 
            script_name, 
            ["TEST_PROCESS"] 
        )&lt;/font&gt;

    def send_input( self ):
        """Send input to the selected process."""
        input_text = self.entry_input.get()
        &lt;font color='#F67'&gt;self.process_mediator.send_input( 
            self.subProcId, input_text 
        )&lt;/font&gt; 
        self.entry_input.delete(0, tk.END)

    def update_output(self):
        """Update output view(s)."""
        if  hasattr( self, 'subProcId' ) :
            output = &lt;font color='#F67'&gt;&lt;strong&gt;self.process_mediator.process_q()&lt;/strong&gt;&lt;/font&gt;
            self.text_output.insert( tk.END, output )
        self.root.after(100, self.update_output)  # Update every 100ms

if __name__ == "__main__":
    TEST_SCRIPT = "nn_testscript.py"
    root = tk.Tk()
    print( "====    START GUI TEST    ====" )
    app = App(root)
    root.mainloop()
    print( "====    END GUI TEST    ====" )
    &lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice that the GUI is &lt;em&gt;very thin&lt;/em&gt;. It doesn't 'know' or 'care' how to orchestrate communication with the sub-process. All that is the responsibility of the &lt;em&gt;process mediator&lt;/em&gt; (&lt;code&gt;ProcessDirector&lt;/code&gt;). To get script output to update the relevant view all the client code has to worry about is calling: &lt;font color='#F67'&gt;&lt;strong&gt;&lt;code&gt;self.process_mediator.process_q()&lt;/code&gt;&lt;/strong&gt;&lt;/font&gt;&lt;/p&gt;
&lt;h2&gt;Listing 3: The Process Mediator&lt;/h2&gt;
&lt;p&gt;The next listing contains selected portions of the class &lt;code&gt;ProcessDirector&lt;/code&gt;. I've provided a full listing of the class as an appendix but here I've in-lined those parts most relevent to the present discussion for convenience. &lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
import queue
import subprocess
import time
...
from threading import Thread

class ProcessDirector :

    def __init__ ( self ) : 
        self.processes = [] 
        self.t_queue = queue.Queue()

    def launch_process( self, python_script, cmd_ln_args ) : 
        launch_sequence = [ sys.executable, python_script ] + cmd_ln_args
        &lt;font color='#F56'&gt;proc = subprocess.Popen(
            launch_sequence,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )&lt;/font&gt;
        self.processes.append( proc )
        &lt;font color='#F56'&gt;t = Thread(
            target=self.read_proc_output,
            args=[proc.pid]
        )
        t.daemon = True
        t.start()&lt;/font&gt;
        return proc.pid

    ...

    def read_proc_output( self, pid ) : 
        for process in self.processes :
            if process.pid == pid :
                proc = process
        &lt;font color='#F56'&gt;while True: 
            # sleep. otherwise you work too hard and heat up the box...
            time.sleep( 0.02 )

            output = proc.stdout.readline()
            if not output :
                continue
            self.t_queue.put( output )&lt;/font&gt;

    ...

    &lt;font color='#F56'&gt;def process_q ( self ) : 
        output = ""
        while not self.t_queue.empty () :
            output += self.t_queue.get()
        return output&lt;/font&gt;

    ...

    &lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Notice: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;ProcessDirector&lt;/code&gt; is responsible for launching subprocesses and orchestrating process interactions. It launches subprocesses in the &lt;code&gt;launch_process&lt;/code&gt; method using &lt;code&gt;subprocess.Popen&lt;/code&gt;. Notice further that the process director obtains handles to the standard process pipes; &lt;code&gt;stdout&lt;/code&gt;, &lt;code&gt;stdin&lt;/code&gt;, and &lt;code&gt;stderror&lt;/code&gt;. The &lt;code&gt;buffsize=1&lt;/code&gt; argument sets the buffer to a line.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, it spins off a thread targeting the &lt;code&gt;read_proc_output&lt;/code&gt; method. &lt;code&gt;read_proc_output&lt;/code&gt; defines the read-loop that reads from the subprocess stdout pipe and populates the &lt;em&gt;process output queue&lt;/em&gt; (owned by the process director) a line at a time. It's this read operation that has the potential to block (and freeze up) the GUI so this is the part that gets spun off to its own thread.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally -- notice the &lt;code&gt;process_q&lt;/code&gt; method. This is the method of concern for &lt;code&gt;ProcessDirector&lt;/code&gt; client code that wants to update views associated with the process output. Client code would call this method to dequeue output for display.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And that's it. Taken as a whole the code we've walked through here is a basic implementation of a pattern that enables subprocess management and the integration of asynchronous code in Python GUI driven applications. The following screenshot shows the GUI displaying the output of the test script following an interactive session. &lt;/p&gt;
&lt;div style='text-align:center'&gt;
&lt;img alt='INSERT GUI SCREEN'
     src='/images/thread_gui/Thread_GUI_Example.png' 
     width='250px'/&gt;
&lt;/div&gt;

&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;The code that I've provided for this blog post is pretty bare-bones. It's intended to serve as reference material and also to provide a basis for further development. The &lt;code&gt;ProcessDirector&lt;/code&gt; class is a first-pass implementation of what I've described as a &lt;em&gt;mediator&lt;/em&gt; pattern for subprocess management. The GUI code is simple enough that it should be readily adaptable to other use-cases or to play with the &lt;em&gt;process mediator&lt;/em&gt;. &lt;/p&gt;
&lt;h2&gt;The &lt;em&gt;Process Mediator&lt;/em&gt; Pattern&lt;/h2&gt;
&lt;p&gt;The main considerations in adapting the process mediator are that it owns relevant data structures and manages the life-cycle of subprocesses tailored to execute specific sub-routines. The example we walked through here uses a &lt;em&gt;queue&lt;/em&gt; to manage process output and defines a public method intended to enable client code to process the queue from a "consumer" thread (in the present case the main tkinter loop). &lt;/p&gt;
&lt;div class="admonition hint"&gt;
&lt;p class="admonition-title"&gt;Is Python Queue 'Thread Safe'?&lt;/p&gt;
&lt;p&gt;It's well worth noting that the python &lt;a href="https://docs.python.org/3/library/queue.html"&gt;Queue&lt;/a&gt; used here is "thread safe". What this means is that the &lt;em&gt;queue&lt;/em&gt; module internally handles synchronization. It manages the locks required to prevent conflicts when threads are adding or removing items from the queue thus ensuring that operations are atomic and that data integrity is maintained.&lt;/p&gt;
&lt;p&gt;That said, if you are adding items to your queue in a manner that requires atomicity outside the scope of the python queue &lt;code&gt;put&lt;/code&gt; and &lt;code&gt;get&lt;/code&gt; methods you'll need to use additional synchronization mechanisms.&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;Additional Considerations&lt;/h2&gt;
&lt;p&gt;Another issue that came up for me concerns the subprocess output. Handling output from subprocesses can sometimes be a bit tricky. Python buffers standard output which can lead to issues with subprocesses that use &lt;code&gt;print&lt;/code&gt; statements to write to standard output. So if you're working with subprocesses and encounter unexpected issues like missing output, process blocking, or race conditions consider the following. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Python &lt;code&gt;print&lt;/code&gt; statements are usually line buffered. But when redirected to a pipe the system may switch to &lt;em&gt;block buffering&lt;/em&gt;. So it is important to ensure that the buffer gets flushed in in the subprocess. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can do this on print statements with: &lt;code&gt;print( "ipsum lorum ... ", flush=True )&lt;/code&gt; .&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can also flush the buffer from the subprocess with &lt;code&gt;sys.stdout.flush()&lt;/code&gt; .&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you cannot modify your subprocess script you ca try the workaround of running the process with the environment variable:  &lt;code&gt;PYTHONUNBUFFERED=1&lt;/code&gt; .&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And as a final consideration, here are some useful linux commands if you find yourself working with subprocess and need to troubleshoot issues.&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
$ ps aux | grep [[PATTERN]]

$ kill [[ pid ]]
    &lt;/pre&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The first command will get you a list of processes on your system. You can grep on the script name if you want to see instances of script processes launched from your application. Use this if you need to get process IDs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second command can be used to kill a process given the &lt;em&gt;pid&lt;/em&gt;. Use &lt;code&gt;kill -9&lt;/code&gt; to force termination if its gone unresponsive. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;This post concludes a series of blog entries revolving around python GUI development with subprocesses, asynchronous routines, and multi-threaded process I/O. The present article provided a rationale and discussion for using threads to prevent UI blocking in tkinter application development. A number of code listings are provide for reference and potentially to bootstrap development.&lt;/p&gt;
&lt;h1&gt;Appendix 1: The Process Mediator&lt;/h1&gt;
&lt;p&gt;Below is a complete listing of the class &lt;em&gt;ProcessDirector&lt;/em&gt; along with a bit of associated test scaffolding. &lt;em&gt;ProcessDirector&lt;/em&gt; is an implementation of a &lt;em&gt;mediator&lt;/em&gt; pattern which I've described in an &lt;a href="https://dr-nick-nagel.github.io/blog/python_process_management_1.html"&gt;earlier entry to this blog series&lt;/a&gt;. I've also made the &lt;code&gt;ProcessDirector&lt;/code&gt; source file and the test GUI described in this and prior articles &lt;a href="https://github.com/dr-nick-nagel/process-mediator/"&gt;available on github&lt;/a&gt;.&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
import queue
import subprocess
import time
import sys
import os
from threading import Thread

class ProcessDirector :
    '''
    Responsible for spawning and directing processes to 
    execute python scripts. 
    '''
    def __init__ ( self ) : 
        self.processes = [] 
        self.t_queue = queue.Queue()

    def launch_process( self, python_script, cmd_ln_args ) : 
        '''
        Launch a process and add it to your list given...

        arguments: 
            python_script: name of script to launch
            cmd_lin_args: a sequence of command line 
            arguments...

        returns: the new process id. The client should hold onto it.
        '''
        launch_sequence = [ sys.executable, python_script ] + cmd_ln_args

        proc = subprocess.Popen(
            launch_sequence,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        self.processes.append( proc )

        t = Thread(
            target=self.read_proc_output,
            args=[proc.pid]
        )
        t.daemon = True
        t.start()

        return proc.pid

    def terminate_children( self ) : 
        '''
        Dispose of all spawned child processes
        '''
        for i in range( len( self.processes ) - 1, -1, -1 ) : 
            target = self.processes[i]
            target.terminate()
            target.wait()
            if target.returncode is not None:
                self.processes.pop( i )

    def read_proc_output( self, pid ) : 
        '''
        Read-loop target for read thread. Reads stdout of child 
        process given the process ID. Note: The pid enables the
        director to determine which pipe to read...

        Arguments: 
            pid: The pid (process id) of the process 
            (should be obtained on launch...) 
        '''
        for process in self.processes :
            if process.pid == pid :
                proc = process
        while True: 
            # sleep. otherwise you work too hard and heat up the box...
            time.sleep( 0.02 )

            output = proc.stdout.readline()
            if not output :
                continue
            self.t_queue.put( output )

    def send_input(self, pid, input_text):
        '''
        Send input to the specified process.

        Arguments:
          pid: Process id to send to
          input_text: text to send
        '''
        for process in self.processes :
            if process.pid == pid :
                proc = process
        proc.stdin.write(input_text + '\n')
        # proc.stdin.write(input_text)
        proc.stdin.flush()

    def process_q ( self ) : 
        '''
        This is the function clients should call to get data
        from the `process mediator`. The queue is expected to
        hold data accumulated since the last dequeue operation...
        '''
        output = ""
        while not self.t_queue.empty () :
            output += self.t_queue.get()
        return output

    def terminate( self, pid ) :
        '''
        Kill the specified process given ...

        Arguments: 
            pid : process id (obtained at launch)
        '''
        for i, process in enumerate( self.processes ) :
            if process.pid == pid :
                target = process
                target_idx = i
                break
        if not target :
            return
        target.terminate()
        target.wait()
        if target.returncode is not None:
            self.processes.pop( target_idx )


if __name__ == "__main__" : 
    print( "----    START TEST     ----" )
    CWD = os.getcwd()
    TEST_SCRIPT_NAME = CWD + &lt;font color='#F66'&gt;"/path/to/your/python_script.py"&lt;/font&gt;
    pd = ProcessDirector()
    test_proc_1_pid = pd.launch_process( TEST_SCRIPT_NAME, [] )
    test_proc_2_pid = pd.launch_process( TEST_SCRIPT_NAME, [] )

    print("TESTING. WAITA MINNIT...")
    time.sleep( 5 )
    print( f"Q SIZE PRE: {pd.t_queue.qsize()}" )
    test_output = pd.process_q()
    print( test_output )
    print( f"Q SIZE POST: {pd.t_queue.qsize()}" )

    pd.terminate_children()
    print( "----    END   TEST     ----" )
    &lt;/pre&gt;
&lt;/div&gt;

&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/tk.html"&gt;Graphical User Interfaces with Tk&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/tkinter.html#threading-model"&gt;The TkInter Threading Model&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/threading.html"&gt;The Python Threading API&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/threading.html#thread-objects"&gt;The Python Thread Object&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/queue.html"&gt;queue -- A synchronized queue class&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/subprocess.html"&gt;Python Subprocess Management&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/dr-nick-nagel/process-mediator/"&gt;&lt;code&gt;process-mediator&lt;/code&gt; on github&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog"></category><category term="graphical user interface"></category><category term="python"></category><category term="tkinter"></category><category term="development"></category><category term="threading"></category><category term="computer science"></category><category term="asynchronous"></category><category term="code"></category><category term="event loops"></category><category term="queues"></category><category term="inter process communication"></category><category term="I/O"></category><category term="STDOUT"></category><category term="processes"></category><category term="subprocess"></category><category term="pipes"></category><category term="STDIN"></category><category term="blocking"></category><category term="threads"></category><category term="thread safety"></category><category term="GUI"></category></entry><entry><title>Process Managment in Python Part 1: The Process Mediator</title><link href="https://dr-nick-nagel.github.io/blog/python_process_management_1.html" rel="alternate"></link><published>2024-11-08T00:00:00-05:00</published><updated>2024-11-08T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-11-08:/blog/python_process_management_1.html</id><summary type="html">&lt;p&gt;Patterns for process management and threading in python with graphical user interfaces.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post is part 1 of a multi-part arc in which I intend to explore process management and multi-threaded application development in Python. The purpose in doing so is to share my experience and provide guidance and recommendations to python enthusiasts interested in inter-process communication and multi-threaded application development.&lt;/p&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Recently I've been working on application prototypes using python and have had the need to quickly create graphical user interfaces to facilitate research, development and testing. While some might expect such efforts to be pretty much straight-forward I found working through the docs to be a somewhat of an obtuse experience -- particularly with regard to process management. So I thought I'd try to clarify some things here and offer up some patterns for process management in GUI development for anyone who might be interested.&lt;/p&gt;
&lt;h2&gt;First, What Exactly are Processes?&lt;/h2&gt;
&lt;p&gt;In modern operating systems a &lt;strong&gt;process&lt;/strong&gt; is a program in execution. A process is an &lt;em&gt;instance&lt;/em&gt; of a program. As such, it comprises a &lt;em&gt;context&lt;/em&gt; which includes a program stack and memory which holds data. A key aspect of a computer process (that seems easy to forget in the age of the graphical user interface) is that execution is always &lt;em&gt;sequential&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;Over the course of execution, a process can enter into any of a number of states as illustrated below. &lt;/p&gt;
&lt;div style='text-align:center'&gt;
    &lt;img alt='INSERT INSTANCE DIAGRAM'
         src='/diagrams/process_states.drawio.svg' 
         width='350px'/&gt;
&lt;/div&gt;

&lt;p&gt;From process launch to completion, the states may be described as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;new&lt;/strong&gt; : The process is just being created.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;running&lt;/strong&gt; : The process is sequentially executing instructions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;waiting&lt;/strong&gt; : The process is waiting for an event (e.g., I/O or receipt of a signal).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ready&lt;/strong&gt; : The process is ready for some CPU time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;terminated&lt;/strong&gt; : The process is done (with or without error).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The important thing to understand here is that &lt;em&gt;only one process can be running on a processor at any particular instant in time&lt;/em&gt;. And depending on the requirements of the program associated processes may &lt;em&gt;block&lt;/em&gt; or enter into a waiting state from which it may be released on the occurrence of an event or receipt of some signal.&lt;/p&gt;
&lt;h2&gt;Process Management in Python Applications&lt;/h2&gt;
&lt;p&gt;Python provides a number of modules intended to facilitate process management (and relatedly, multi-threaded application development) -- one of which is &lt;a href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing"&gt;multiprocessing&lt;/a&gt;. &lt;em&gt;Multiprocessing&lt;/em&gt; is very useful for optimizing code intended for execution in multi-processor environments but is overkill for my current needs so it's out-of-scope for the present discussion. Instead, in this post I'm focusing on the &lt;em&gt;&lt;a href="https://docs.python.org/3/library/subprocess.html#module-subprocess"&gt;subprocess&lt;/a&gt;&lt;/em&gt; module -- which provides a lower level API allowing you to spawn new processes and connect to their input/output/error pipes.&lt;/p&gt;
&lt;p&gt;Inter process communication is enabled through OS layer &lt;em&gt;pipes&lt;/em&gt; &lt;img src='/diagrams/pipe.svg' height='10px' /&gt; . A pipe is essentially a queue of bytes shared between two processes. One process writes into the pipe and another reads from it. Unix and derivative operating systems (and possibly Windows) define three standard pipes; &lt;em&gt;standard output&lt;/em&gt;, &lt;em&gt;standard input&lt;/em&gt;, and &lt;em&gt;standard error&lt;/em&gt; (termed &lt;code&gt;stdout&lt;/code&gt;, &lt;code&gt;stdin&lt;/code&gt; and &lt;code&gt;stderror&lt;/code&gt;) respectively. In advanced python development often there is a need to create child processes and redirect these pipes to enable communication with the parent. &lt;/p&gt;
&lt;h1&gt;Mediating Inter process Communication&lt;/h1&gt;
&lt;p&gt;The problem with working with sub processes is that -- while they facilitate re-use -- creating multiple interconnections to enable communication adds complexity that quickly becomes hard to manage and difficult to maintain. Multiprocessing introduces &lt;em&gt;process dependencies&lt;/em&gt; that often require coordination among participants. For example, consider the implementation of a network protocol which requires a number of inter process interactions to occur in a specific sequence. &lt;/p&gt;
&lt;p&gt;Many issues arising in these sorts of scenarios can be prevented and/or addressed through the use of a &lt;em&gt;mediator-type object&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;The mediator should encapsulate the control and coordination of interactions among groups of processes vastly simplifying (among other things): &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The management of shared resources, &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The tracking and enforcement of sequential dependencies, &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The proper disposition of allocated system resources over the life-cycle of the application. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The mediator facilitates inter process communication by &lt;em&gt;reducing the overall number of connections&lt;/em&gt; among objects. Objects can communicate through the mediator obviating the need to maintain state in a distributed fashion across multiple process instances and define direct communication protocols. This, in turn, enables the development of more atomic functions lowering the potential for unwanted side-effects and eliminating whole classes of issues. &lt;/p&gt;
&lt;p&gt;The following instance diagram illustrates the relationships among objects with a sample mediator; &lt;em&gt;ProcessDirector&lt;/em&gt;. &lt;/p&gt;
&lt;div style='text-align:center'&gt;
    &lt;img alt='INSERT INSTANCE DIAGRAM'
         src='/diagrams/mediator_instance_diagram.drawio.svg' 
         width='250px'/&gt;
&lt;/div&gt;

&lt;p&gt;Notice how the director object mediates communication between sub processes and client code. Processes communicate between each other and with clients only &lt;em&gt;indirectly&lt;/em&gt; via the mediator. The client doesn't need to "know" any of the internal implementation details of the sub processes, nor do the processes need to maintain any state related to each other. This is all the responsibility of the director. With the responsibility of directing the behavior of its aggregates scoped to one specific class (and potentially sub classes) logic can be readily changed and/or replaced by extending or swapping out that singular implementation. &lt;/p&gt;
&lt;p&gt;The following class diagram highlights key collaborations and functionality in such a sub-system. &lt;/p&gt;
&lt;div style='text-align:center'&gt;
    &lt;img alt='INSERT CLASS DIAGRAM'
         src='/diagrams/ProcessDirector_class_diagram.drawio.svg' 
         width='400px'/&gt;
&lt;/div&gt;

&lt;p&gt;The diagram illustrates the aggregate relationship between the &lt;code&gt;ProcessDirector&lt;/code&gt; class and it's &lt;code&gt;SubProcess&lt;/code&gt; instances. The director class can define logic to properly launch and dispose of child processes executing python scripts and business logic to orchestrate sequential behavior. In addition, it serves as a composite whole maintaining any shared resources (e.g., pipes, queues, streams, etc.) to enable communication.&lt;/p&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;
&lt;p&gt;The following code example highlights some of the implementation details. &lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
&lt;pre&gt;
class ProcessDirector :
    '''
    Responsible for spawning and directing processes to execute python scripts. 
    '''
    def __init__ ( self ) : 
        self.processes = [] 
        self.t_queue = queue.Queue()

    def launch_process( self, python_script, cmd_ln_args ) : 
        &lt;font color='#f88'&gt;launch_sequence = [ sys.executable, python_script ] + cmd_ln_args
        proc = subprocess.Popen(
            launch_sequence,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )
        self.processes.append( proc )&lt;/font&gt;

        t = Thread(
            target=self.read_proc_output,
            args=[proc.pid]
        )
        t.daemon = True
        t.start()

        return proc.pid

    &lt;font color='#f88'&gt;def terminate_children( self ) : 
        for i in range( len( self.processes ) - 1, -1, -1 ) : 
            target = self.processes[i]
            target.terminate()
            target.wait()
            if target.returncode is not None:
                self.processes.pop( i )&lt;/font&gt;

    def read_proc_output( self, pid ) : 
        '''
        Read-loop target for read thread. Reads stdout of child process given

        Arguments: 
            pid: The pid (process id) of the process (should be obtained on launch...) 
        '''
        for process in self.processes :
            if process.pid == pid :
                proc = process
        while True: 
            # sleep. otherwise you work too hard and heat up the box...
            time.sleep( 0.02 )

            output = proc.stdout.readline()
            if not output :
                continue
            self.t_queue.put( output )

    def send_input(self, input_text):
        '''
        Send input to the process.
        '''
        proc = self.processes[0]
        proc.stdin.write(input_text + '\n')
        proc.stdin.flush()

    def process_q ( self ) : 
        output = ""
        while not self.t_queue.empty () :
            output += self.t_queue.get()
        return output

&lt;/pre&gt;
&lt;/div&gt;

&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;The key points for the present analysis revolve around the creation and disposition of sub processes and disposition of resources. Notice the &lt;em&gt;ProcessDirector&lt;/em&gt; ...&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Defines logic for &lt;strong&gt;launching and maintaining a list of sub processes&lt;/strong&gt; given; (a) a &lt;em&gt;script name&lt;/em&gt;, and (b) &lt;em&gt;command-line arguments&lt;/em&gt; (which it marshals to create a 'launch sequence'),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provides a method to &lt;strong&gt;properly dispose of the sub processes&lt;/strong&gt; it creates -- releasing any system resources and insuring that no orphan child processes should be left behind, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implements &lt;strong&gt;business logic&lt;/strong&gt; aimed at orchestrating communication between sub processes and exposing resultant information to client objects (for example GUI components).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Those details are the main focus of the present post. I'll provide more details around &lt;em&gt;inter process communication using threads&lt;/em&gt; in the context of GUI development in subsequent additions to this arc.&lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;In this post I've explored the application of a &lt;em&gt;mediator&lt;/em&gt; pattern to facilitate inter process communication in python. The approach involves the use of an object responsible for tracking process creation, disposition, and state over the life-cycle of an application. &lt;/p&gt;
&lt;p&gt;This approach centralizes the control of sub processes and facilitates the management of shared resources. It stands in contrast to federated approaches  requiring &lt;em&gt;dependency injection&lt;/em&gt;. A full-blown comparison between the two approaches is out-of-scope for this post. But suffice it to say that based on my experience, systems that rely too heavily on dependency injection are very difficult to iteratively and incrementally develop and maintain. &lt;/p&gt;
&lt;p&gt;The &lt;em&gt;benefits&lt;/em&gt; the centralized approach include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;simplification of &lt;em&gt;synchronization logic&lt;/em&gt;,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;facilitation of the enforcement of &lt;em&gt;sequential dependencies&lt;/em&gt;, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;ease of maintenance and code re-use&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The use-cases for the pattern are ubiquitous and include any application requiring task parallelization such as real-time data processing with end-user interaction, simulations, and any application requiring concurrent I/O operations. Similar patterns are used in python's &lt;em&gt;multiprocessing&lt;/em&gt; module with its &lt;em&gt;manager&lt;/em&gt; objects. Working through this "homegrown" example helps better understand the need for and application of these patterns.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;This is the first part of a multi-post arc exploring multiprocess communication in python. In this post the focus was on the creation and management of sub-processes and communication between them using python's &lt;em&gt;subprocess&lt;/em&gt; module. In a subsequent post I'll dig a bit deeper into GUI application development using sub processes and &lt;em&gt;threads&lt;/em&gt;. &lt;/p&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing"&gt;multiprocessing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://euccas.github.io/blog/20161231/python-multiprocessing.html"&gt;Python Multiprocessing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/subprocess.html#module-subprocess"&gt;subprocess&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog"></category><category term="graphical user interface"></category><category term="python"></category><category term="tkinter"></category><category term="development"></category><category term="threading"></category><category term="computer science"></category><category term="asynchronous"></category><category term="code"></category><category term="event loops"></category><category term="queues"></category><category term="inter process communication"></category><category term="I/O"></category><category term="STDOUT"></category><category term="processes"></category><category term="subprocess"></category><category term="pipes"></category><category term="STDIN"></category><category term="blocking"></category><category term="threads"></category><category term="thread safety"></category><category term="GUI"></category><category term="software architecture"></category><category term="design patterns"></category></entry><entry><title>Graphical User Interfaces with Asynchronous Code in Python</title><link href="https://dr-nick-nagel.github.io/blog/gui-asynchronous.html" rel="alternate"></link><published>2024-10-23T00:00:00-04:00</published><updated>2024-10-23T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-10-23:/blog/gui-asynchronous.html</id><summary type="html">&lt;p&gt;A pattern for rapid prototyping asynchronous routines in python with tkinter...&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;As a machine-learning scientist and engineer I often have a need for rapid prototyping. And often over the course of hammering out a concept proof the need for a graphical user interface arises. Now, I've had extensive experience in UI development in many languages including java, HTML5, and electron application development. But -- though similar in many ways to other systems -- UI development in python poses some unique issues. &lt;/p&gt;
&lt;h1&gt;The Problem&lt;/h1&gt;
&lt;p&gt;One of the issues I encountered recently stems from working with python asynchronous IO (&lt;code&gt;asyncio&lt;/code&gt;). Asynchronous programming is a means of developing routines that can execute independently without blocking the main thread of execution. It is provided as a more basic alternative to spinning off routines in multiple threads. One of the most common tasks suitable for asynchronous programming is IO. Python enables handling IO operations -- and other tasks -- asynchronously through various modules using the &lt;code&gt;await&lt;/code&gt; keyword. The problem I encountered was when I needed to spin up a quick prototype using &lt;code&gt;asyncio&lt;/code&gt;. The issue revolves around the &lt;em&gt;event-driven architecture&lt;/em&gt;. Event-driven architectures are frameworks provided for, among other things, asynchronous programming -- but also for development with graphical user interfaces.&lt;/p&gt;
&lt;p&gt;When I need to spin up a GUI real quick I usually rely on the lightest weight option for whatever framework I'm working in. Sure, if I'm doing professional development I'll set up a fully functional interface supporting all the features demanded of modern real-world applications using a heavy-weight framework like &lt;em&gt;electron&lt;/em&gt; or &lt;em&gt;QT&lt;/em&gt;. But for a quick demo or rapid prototype I prefer something light-weight and fast. And for python that's &lt;code&gt;tkinter&lt;/code&gt;. The problem with using &lt;code&gt;tkinter&lt;/code&gt; (or any other python GUI-development- framework for that matter) alongside &lt;code&gt;asyncio&lt;/code&gt; is that both use independent &lt;em&gt;event loops&lt;/em&gt;. Surveying the 'net for solutions to my issues I noticed some confusion around the concept, so I figure it's worth delving into here. &lt;/p&gt;
&lt;h2&gt;Event Loops&lt;/h2&gt;
&lt;p&gt;Whenever you develop a GUI for a windowing system you typically kick off an event loop, which essentially &lt;em&gt;blocks the main thread&lt;/em&gt; and sits there waiting for various user-events to trigger callbacks. The other major responsibility of the GUI system is (re)painting itself periodically.&lt;/p&gt;
&lt;div style='text-align:center'&gt;
&lt;img alt='INSERT EVENT LOOP DIAGRAM'
     src='/diagrams/event_loop_tk.drawio.svg' 
     width='250px'/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: The &lt;em&gt;TkInter&lt;/em&gt; event loop.&lt;/p&gt;
&lt;p&gt;Figure 1 illustrates this concept for &lt;em&gt;tkinter&lt;/em&gt;. Running &lt;code&gt;root.mainloop()&lt;/code&gt; in tkinter kicks off an event loop, which then executes continuously -- waiting for user events which are accumulated on an event queue. On each loop cycle, &lt;code&gt;tkinter&lt;/code&gt; pops all the events and updates the GUI (repainting all the areas that may have changed over the course of the cycle). &lt;/p&gt;
&lt;p&gt;That's all well-and-good for many use-cases but poses a problem for asynchronous programming in python. The problem is that asynchronous modules (e.g., &lt;code&gt;asyncio&lt;/code&gt;) require an independent event-loop of their own. Simply declaring a routine with the &lt;code&gt;async&lt;/code&gt; keyword and trying to bind it to a &lt;code&gt;tkinter&lt;/code&gt; widget isn't enough -- python just won't let you get away with that.&lt;/p&gt;
&lt;h1&gt;Solutions&lt;/h1&gt;
&lt;p&gt;So the purpose of this post is, first and foremost, to provide some solutions to the problem. Again, I saw a some confusion when surveying the 'net and reading the docs so I figure it's worth documenting a couple of patterns here.&lt;/p&gt;
&lt;h2&gt;The Simplest Approach&lt;/h2&gt;
&lt;p&gt;The simplest approach to the problem of using your async code in a tkinter application is to &lt;em&gt;run the async loop within the tkinter loop&lt;/em&gt;. Python's &lt;a href="https://docs.python.org/3/library/asyncio.html"&gt;&lt;code&gt;asyncio&lt;/code&gt; API&lt;/a&gt; allows you to run an &lt;code&gt;asyncio&lt;/code&gt; event loop within an existing loop as shown in the following bare-bones example.&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
```
    import asyncio
    import tkinter as tk
    import aiortc

    async def do_async () :
        print( "START ASYNC TASK" )
        await asyncio.sleep( 3 )
        print( "END ASYNC TASK" )

    def do_async_task ( task ) : 
        # LAUNCH THE ASYNC TASK...
        async_loop.run_until_complete( task() )


    def handle_click () : 
        print( "'Tis but a scratch!" )

    root = tk.Tk()
    root.title( "Test Harness" )
    root.geometry( "400x300" )

    button_tk = tk.Button(
        root,
        text="Click Me",
        command=handle_click
    )
    button_tk.pack( pady=20 )

    button_async = tk.Button(
        root,
        text="Do Async",
        command=lambda : do_async_task( do_async )  
    )
    button_async.pack( pady=20 )

    #  asyncio.run( do_async_task )
    async_loop = asyncio.new_event_loop()

    root.mainloop()
```
&lt;/div&gt;

&lt;p&gt;In this example I've defined two tasks; (1) &lt;code&gt;async def do_async ()&lt;/code&gt;, which simulates an asynchronous routine, and (2) &lt;code&gt;def handle_click ()&lt;/code&gt; which simulates a standard task that can execute within the tkinter loop. Notice that &lt;code&gt;do_async&lt;/code&gt; is marked with the &lt;code&gt;async&lt;/code&gt; keyword. This requires that when it is called it must be called with the keyword &lt;code&gt;await&lt;/code&gt;. The problem is that tkinter doesn't "know" how to do that and so it's not so easy to bind the function to a tkinter widget.&lt;/p&gt;
&lt;p&gt;The simple solution here overcomes that problem by kicking off the async task in a new async loop. This done in 3 steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First, I obtain a new, module-scoped &lt;code&gt;asyncio&lt;/code&gt; event loop: &lt;code&gt;async_loop = asyncio.new_event_loop()&lt;/code&gt; &lt;sup style="color:red"&gt;*&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, I define a function to launch any async task using the new loop -- effectively "joining" the &lt;em&gt;async&lt;/em&gt; loop to the &lt;em&gt;tkinter&lt;/em&gt; loop: 
    &lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
        &lt;pre&gt;
    def do_async_task ( task ) : 
        # LAUNCH THE ASYNC TASK...
        &lt;font color='#F55'&gt;async_loop.run_until_complete( task() )&lt;/font&gt;
        &lt;/pre&gt;
    &lt;/div&gt;
The function delegates the asynchronous execution of the task to any named async function using &lt;code&gt;run_until_complete&lt;/code&gt;. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, since &lt;code&gt;do_async_task&lt;/code&gt; is not, itself, marked async it can be used in a &lt;em&gt;lambda&lt;/em&gt; to bind asynchronous functions to tkinter widgets. 
    &lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
        &lt;pre&gt;
        button_async = tk.Button(
            root,
            text="Do Async",
            &lt;font color='#F55'&gt;command=lambda : do_async_task( do_async )&lt;/font&gt;
        )
        &lt;/pre&gt;
    &lt;/div&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;sup  style="color:red"&gt;*&lt;/sup&gt; Note that I've used &lt;code&gt;new_event_loop&lt;/code&gt; here as opposed to &lt;code&gt;get_event_loop&lt;/code&gt; which was deprecated in Python 3.7. &lt;/p&gt;
&lt;p&gt;The pattern embodied in this solution enables you to essentially launch any given async task from a tkinter GUI. However, by joining the async loop to the tkinter loop we defeat the purpose of the async module in the first place. The async loop will block the tkinter loop and the tkinter GUI will cease to be responsive until the asynchronous operation completes. That behavior may be OK for some use-cases but in order to take full-advantage of asynchronous functionality with tkinter you'll have to use threads. &lt;/p&gt;
&lt;h2&gt;Threaded Solution&lt;/h2&gt;
&lt;p&gt;To that end, I've extended the pattern developed so far to spin off the asynchronous tasks in new threads. In the next example, I keep the tkinter loop in the main thread of execution, and spin off a new thread to execute the async loop. Using this pattern, &lt;code&gt;asyncio&lt;/code&gt; routines can be controlled from tkinter GUIs using the python &lt;a href="https://docs.python.org/3/library/threading.html"&gt;threading API&lt;/a&gt; and/or methods from the asyncio API such as &lt;code&gt;call_soon_threadsafe&lt;/code&gt;. &lt;/p&gt;
&lt;div style='text-align:center'&gt;
&lt;img alt='INSERT EVENT LOOP DIAGRAM'
     src='/diagrams/event_loops_threads.drawio.svg' 
     width='250px'/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: The python main thread running the &lt;code&gt;tkinter&lt;/code&gt; loop and a child thread running the async-loop.&lt;/p&gt;
&lt;p&gt;The following barebones example embodies the extended pattern.&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
```
import asyncio
import tkinter as tk
import aiortc
from threading import Thread

async def do_async () :
    print( "START ASYNC TASK" )
    await asyncio.sleep( 3 )
    print( "END ASYNC TASK" )

def do_async_task ( task ) : 
    # LAUNCH TASK IN NEW THREAD...
    task_thread = Thread( 
        target=lambda :  async_loop.run_until_complete( task() )
    )
    task_thread.start()

def handle_click () : 
    print( "'Tis but a scratch!" )

root = tk.Tk()
root.title( "Test Harness" )
root.geometry( "400x300" )

button_tk = tk.Button(
    root,
    text="Click Me",
    command=handle_click
)
button_tk.pack( pady=20 )

button_async = tk.Button(
    root,
    text="Do Async",
    command=lambda : do_async_task( do_async )  
)
button_async.pack( pady=20 )

#  asyncio.run( do_async_task )
async_loop = asyncio.new_event_loop()

root.mainloop()

```
&lt;/div&gt;
&lt;p&gt;If you copy and execute this this example in your favorite python environment you should find that the GUI remains responsive even while the asynchronous operation is executing. &lt;em&gt;The pattern to achieve this is to extend the previous example by kicking off the async loop in a new child thread&lt;/em&gt;. You can see this in the updated function:&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
    &lt;pre&gt;
    def do_async_task ( task ) : 
        # LAUNCH TASK IN NEW THREAD...
        &lt;font color='#F88'&gt;task_thread = Thread( 
            target=lambda :  async_loop.run_until_complete( task() )
        )
        task_thread.start()&lt;/font&gt;
    &lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here's how it works:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A new thread is created with the constructor call targeting the asynchronous loop.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In this example, the 'run_loop_until_complete' function is invoked on the asynchronous task with the expectation that the task will run through its completion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Meanwhile, control is returned to the main thread which can continue execution without blocking. In this case the &lt;code&gt;tkinter&lt;/code&gt; event loop returns to monitoring for more events.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;The solution I've presented here solves the problem of using Python asynchronous modules with graphical user interface frameworks like tkinter. I've presented the solution in the form of patterns that can be applied toward the development of rapid prototypes and demos, and, yes, also to production code. The good news is that python provides a very powerful API for developing multi-threaded applications. However, as a wise man once said; "With great power comes great responsibility". &lt;/p&gt;
&lt;p&gt;Working with threads opens up a Pandora's box of possible issues (well beyond the scope of this post to cover). But for simple asynchronous tasks (e.g., local I/O operations, implementing WebRTC protocols -- things of that nature) the pattern I've presented here should prove useful. &lt;/p&gt;
&lt;p&gt;For more complex scenarios, the pattern could be elaborated with proper objects defined to handle responsibilities associated with thread-management within an application. Look for more posts on that topic in the not-too-distant future!&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;This article presents python development patterns that may be employed to enable utilization of asynchronous modules with python graphical-user-interface development frameworks. In order to facilitate the use of these patterns, the nature of &lt;em&gt;event-driven architectures&lt;/em&gt; is discussed with focus on the operation of &lt;em&gt;event loops&lt;/em&gt;. Having explored the "big picture' considerations, I proceed with "bare-bones examples" showing how to apply the patterns to &lt;code&gt;tkinter&lt;/code&gt; with &lt;code&gt;asyncio&lt;/code&gt; applications. Finally, a multi-threaded approach to handling asynchronous routines is presented, with the caveat that appropriate measures will always have to be taken to insure thread safety.&lt;/p&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/asyncio-eventloop.html"&gt;AsyncIO Event Loops&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/tk.html"&gt;Graphical User Interfaces with Tk&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/tkinter.html#threading-model"&gt;The TkInter Threading Model&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/threading.html"&gt;The Python Threading API&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/threading.html#thread-objects"&gt;The Python Thread Object&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog"></category><category term="graphical user interface"></category><category term="python"></category><category term="tkinter"></category><category term="development"></category><category term="rapid prototype"></category><category term="threading"></category><category term="computer science"></category><category term="asynchronous"></category><category term="code"></category><category term="event loops"></category></entry><entry><title>Streaming over Discord on an Ubuntu System</title><link href="https://dr-nick-nagel.github.io/blog/discord-streaming.html" rel="alternate"></link><published>2024-10-10T00:00:00-04:00</published><updated>2024-10-10T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-10-10:/blog/discord-streaming.html</id><summary type="html">&lt;p&gt;How to stream video using discord on ubuntu...&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;As a linux aficionado I've recently committed to a an Ubuntu system to meet the bulk of my laptop / workstation needs which include -- among many other things -- supporting distributed (i.e., remote) community engagement. I've had a long-standing interest in providing immersive on-line experiences using a wide range of media -- an interest which has often gone hand-in-hand with video gaming. So it should come as no surprise that, in order to satisfy numerous use-cases I decided to explore the possibility of hosting community meetings and events using &lt;em&gt;Discord&lt;/em&gt; on my Ubuntu system. &lt;/p&gt;
&lt;p&gt;While it proved &lt;em&gt;very good&lt;/em&gt; at meeting many of my needs as a video-voice-chat server, Discord proved difficult in one mission-critical aspect; streaming video content using screen share. The problem is that &lt;em&gt;Discord on Linux cannot stream application audio during screen shares&lt;/em&gt;. &lt;/p&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;A little research showed it wasn't just me. At the time of this writing Discord does not -- and cannot in-and-of-itself -- capture audio when screen sharing on linux. The problem revolves around streaming audio and the architecture surrounding audio capture on linux. &lt;/p&gt;
&lt;h2&gt;Purpose&lt;/h2&gt;
&lt;p&gt;Finding a solution to the problem -- a workaround really -- turned out to require some effort. So the purpose of this post is to share my results with the community and, hopefully, spare some linux/discord enthusiasts out there some of the effort I had to work through by way of trial-and-error. &lt;/p&gt;
&lt;h1&gt;Solution&lt;/h1&gt;
&lt;p&gt;On researching the solution I found most existing workarounds kind of vague and it took a while for me to land on something workable. Proposed solutions ranged from installing a utility called &lt;em&gt;pulsemixer&lt;/em&gt;, to attempting to set up a virtual camera using &lt;em&gt;OBS Studio&lt;/em&gt; . None of these really worked for me although OBS seems to have promise for other linux broadcast use-cases. What worked best for me in the end was a solution using &lt;em&gt;pavucontrol&lt;/em&gt; . &lt;/p&gt;
&lt;p&gt;So, in a nutshell, I'll summarize the workaround, and then dive a bit deeper into the details for those who may be interested. &lt;/p&gt;
&lt;h2&gt;The Recipe&lt;/h2&gt;
&lt;p&gt;First, the short answer...&lt;/p&gt;
&lt;p&gt;In order to stream video over discord what worked best for me was to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install &lt;em&gt;pavucontrol&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set up a voice channel on Discord.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use Discord's &lt;em&gt;screen share&lt;/em&gt; to select a screen or window to share.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;em&gt;pavucontrol&lt;/em&gt; to capture the audio from the selected screen or application window.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Just be aware, if you fall back on this approach, you won't be heard if you try using your mic while sharing your application A/V.&lt;/strong&gt; To get your voice back you'll have to switch back to your voice source in &lt;em&gt;pavucontrol&lt;/em&gt; . &lt;/p&gt;
&lt;h2&gt;Understanding your linux Audio System&lt;/h2&gt;
&lt;p&gt;To get this workaround to actually work, you'll want to understand a bit about your linux audio system. For Ubuntu at least, the framework for working with audio streams is &lt;em&gt;PulseAudio&lt;/em&gt; . Among other things, PulseAudio enables routing and mixing audio streams for recording and production. &lt;/p&gt;
&lt;p&gt;&lt;code&gt;pavucontrol&lt;/code&gt; is a lightweight GUI that sits on top of PulseAudio enabling you to control important aspects of audio streaming like source selection and volume controls.&lt;/p&gt;
&lt;p&gt;In order to use &lt;em&gt;pavucontrol&lt;/em&gt; with Discord to stream video sound you can use the following procedure. &lt;/p&gt;
&lt;h2&gt;Procedure&lt;/h2&gt;
&lt;h3&gt;One-time Setup&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Verify that PulseAudio is available on your system (it will usually be installed and operating on linux distros).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;$ pulseaudio --version&lt;/code&gt;
2. PulseAudio should be running as a process which can be verified with:
&lt;code&gt;$ ps aux | grep pulseaudio&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install &lt;em&gt;pavucontrol&lt;/em&gt;
&lt;code&gt;sudo apt update
sudo apt install pavucontrol&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And, of course, it's assumed you have Discord installed on your system (the easiest way as of this writing is with snap since it doesn't seem to be registered as a APT repository)...
&lt;code&gt;sudo snap install discord&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Routine Use&lt;/h3&gt;
&lt;p&gt;Once you've completed the initial setup, you can use the following procedure to stream audio and video through discord as desired. My use-case that prompted this post concerns streaming video in a browser but I'm guessing it will generalize to other application windows.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Launch Discord&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Launch your browser&lt;/strong&gt;. Find whatever video it may be that you want to share.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use the screen-share control in Discord&lt;/strong&gt; to select and share the screen/window with your video.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once you have the screen-share running you'll have an additional audio stream which you should be able to control through &lt;em&gt;pavucontrol&lt;/em&gt;. So ...&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Launch pavucontrol.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to the &lt;strong&gt;Recording Tab&lt;/strong&gt;. Find the drop down box and change the sound source to: "Monitor of Built-in Audio Analog Stereo". This is the WebRTC stream created by Discord when you start the share. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="text-align: center; width: 100%; "&gt;
  &lt;img src="/images/discord_streaming/pavu_recording_tab.png" 
       width="300px"
       alt="Voice channel settings in discord"/&gt;
&lt;/div&gt;

&lt;p&gt;And that's basically it. That's the workaround to fix the problem of audio streaming using screen share on discord on linux (at least for Ubuntu). &lt;/p&gt;
&lt;h2&gt;Additional Tweaks&lt;/h2&gt;
&lt;p&gt;A few more things worth mentioning. &lt;/p&gt;
&lt;h3&gt;More pavucontrol settings&lt;/h3&gt;
&lt;p&gt;Another problem I had to work out concerned the the perceived quality of my voice streaming over Discord. Friends complained that the audio quality of my voice was poor when speaking through my mic. The problem may have been due to audio settings on my system. Through trial-and-error we found that &lt;strong&gt;lowering the input volume on the &lt;em&gt;Input Devices&lt;/em&gt; tab&lt;/strong&gt; using pavucontrol significantly improved the quality. I found setting the dB level to about 25% worked well on my system.&lt;/p&gt;
&lt;div style="text-align: center; width: 100%; "&gt;
  &lt;img src="/images/discord_streaming/pavu_in_devices.png" 
       width="300px"
       alt="Voice channel settings in discord"/&gt;
&lt;/div&gt;

&lt;h3&gt;Disable Krisp for Music&lt;/h3&gt;
&lt;p&gt;Another issue we encountered almost immediately revolves around use-cases involving &lt;em&gt;music&lt;/em&gt;. On attempting to share music audio with friends we quickly found the workaround became unusable. By and large, songs we tried to stream (e.g., anime theme songs) were not captured. That is, only the segments of the song with vocals would stream out to users over the discord channel. &lt;/p&gt;
&lt;p&gt;Through trial-and-error we identified &lt;a href="https://support.discord.com/hc/en-us/articles/360040843952-Krisp-FAQ#:~:text=We've%20integrated%20Krisp%20to,while%20still%20transmitting%20your%20voice"&gt;discord's use of Krisp&lt;/a&gt; as the probable culprit. In all fairness to discord, their main use-case is to provide excellent voice streaming -- and Krisp helps with this by applying a machine learning solution to noise suppression. But when trying to stream music, the unhappy side effect for us is that music gets filtered out. Users may want to keep this in mind when attempting to stream music using this workaround. &lt;/p&gt;
&lt;p&gt;To disable Krisp:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open the settings view for your discord voice channel...&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="text-align: center; width: 100%; "&gt;
  &lt;img src="/images/discord_streaming/discord_voice_channel_settings.png" 
       width="300px"
       alt="Voice channel settings in discord"/&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Go to "Video and Voice", and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Find the section enabling you to disable the noise suppression ...&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style="text-align: center; width: 100%; "&gt;
  &lt;img src="/images/discord_streaming/discord_crisp.png" 
       width="300px"
       alt="noise suppression area"/&gt;
&lt;/div&gt;

&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;So that's just about it. To sum up, this post is intended to help with a workaround to the problems encountered by linux users wanting to stream A/V content using Discord. The problem is that discord does not have an internal solution enabling screen capture and streaming that includes application audio. The workaround provided here shows how to use &lt;em&gt;pavucontrol&lt;/em&gt; to redirect the sound source for WebRTC streaming (utilized by Discord) to the desired share application (albeit at the expense of losing the host mic during the share). &lt;/p&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;So, it's a bit of a disappointment to me, that, as usual, linux adopters are treated as second class citizens when it comes to feature support for gaming systems. By that I mean that video game applications and applications supporting the industry seem to be developed for Windows first. This despite the overwhelmingly vast contributions made by the linux community of users to software engineering and computer science in general. The screen share feature on Discord is reported to support application A/V streams internally on Windows. But on linux, as you see here, not-so-much. So hopefully the workaround I've described here will help mitigate the problem -- at least for some users for some use-cases. In any event, it's prompted me to head down the path of a prototype implementation for peer-to-peer WebRTC applications on linux. Tune back in for future posts on that project.&lt;/p&gt;
&lt;p&gt;One more point before leaving off. The elephant-in-the-room that I haven't mentioned in this post concerns copyright and copyright infringement. As more and more advancements are made facilitating the sharing and distribution of digital content it becomes increasingly easier and more tempting to violate copyright restrictions. A full discussion is beyond the scope of this post, but, for the moment suffice it to say that it is my strong opinion that copyright restriction on all content should be respected. As a content provider, I know as well as anyone that mechanisms must be in place, and respected, that allow content producers to make a living off their hard work. So support your local artists and compensate those whose content you use accordingly!&lt;/p&gt;
&lt;h1&gt;Acknowledgments&lt;/h1&gt;
&lt;p&gt;Special thanks to &lt;a href="https://www.facebook.com/lyraproductionsTN/"&gt;Luke Nagel&lt;/a&gt; for contributing time and effort to develop the workaround proposed in this post.&lt;/p&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://support.discord.com/hc/en-us/articles/360040843952-Krisp-FAQ#:~:text=We've%20integrated%20Krisp%20to,while%20still%20transmitting%20your%20voice"&gt;Krisp in Discord&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://freedesktop.org/software/pulseaudio/pavucontrol/"&gt;pavucontrol&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.freedesktop.org/wiki/Software/PulseAudio/"&gt;PulseAudio&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog"></category><category term="linux"></category><category term="ubuntu"></category><category term="peer-to-peer"></category><category term="streaming"></category><category term="audio"></category><category term="video"></category><category term="screen share"></category></entry><entry><title>What Exactly is a Convolution Anyway?</title><link href="https://dr-nick-nagel.github.io/blog/convolution-operator.html" rel="alternate"></link><published>2024-09-30T00:00:00-04:00</published><updated>2024-09-30T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-09-30:/blog/convolution-operator.html</id><summary type="html">&lt;p&gt;If you have an interest in machine learning and you want to get into it at some point you'll want to deepen your understanding of the mathematics of the convolutional operator.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I have a confession to make. I've never been very good at "black box programming". The reason for this is my insatiable curiosity. In learning to code I've rarely been satisfied with lessons and instructions that hand me some code and say here, since you're trying to do &lt;em&gt;X&lt;/em&gt; use this. Not only do I need to know the solution but I need to understand why and how the solution works. And this mindset has saved me a lot of trouble many times. Although complete understanding of every line of code you use absolutely requires more time and effort up front, it saves much more time and effort downstream when you're trying to debug and troubleshoot issues. Many times I've seen naive programmers, when faced with a bug or issue, start arbitrarily changing lines of code without complete understanding hoping their changes will fix the issue. As often as not such an approach will add more complexity and potential for error to the solution even if it may initially seem to address the problem. &lt;/p&gt;
&lt;p&gt;So I continue to maintain, when something you're trying to code doesn't behave as you expect, it pays to have a thorough and complete understanding of every line of code in your system. That's why, when I re-engaged neural network application development after many years of working on other things I wanted to revisit all the basics -- including the concept of &lt;em&gt;convolution&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;Convolution lies at the heart of &lt;em&gt;much&lt;/em&gt; of what we see today in applied artificial intelligence. But what exactly is the &lt;em&gt;convolution&lt;/em&gt; in &lt;em&gt;convolutional neural networks&lt;/em&gt; and how does it work. For me, the best way to understand concepts -- especially mathematical concepts -- is to roll up my sleeves, get my hands dirty and interactively achieve understanding. I used to tell my students that in order to achieve  deepest understanding you have to connect your &lt;em&gt;input&lt;/em&gt; neurons to your &lt;em&gt;output&lt;/em&gt; neurons. For me, especially when it comes to math, this means I have to do the exercises. I have to work with the formulas and equations rather than just read and memorize them. So this post is a result of such an exercise toward understanding. Yes, of course the convolution operation is already written into your machine learning libraries and frameworks. So, no, you don't have to worry about the math if you're really good at black-box programming. But if, like me, you crave a deep understanding of exactly what you're doing with your code, then it behooves you to do what it takes to deepen your understanding.&lt;/p&gt;
&lt;h1&gt;What is a Convolution?&lt;/h1&gt;
&lt;p&gt;Technically speaking, a convolution is a mathematical operation that can be applied to functions. The convolution operation is fundamental in many fields including signal processing, probability theory, and, by extension, machine learning. &lt;/p&gt;
&lt;p&gt;Mathematically, the operation can be defined as the &lt;em&gt;convolution integral&lt;/em&gt;; the product of two functions [denoted (f * g)(t)] where one function is reversed and shifted. &lt;/p&gt;
&lt;p&gt;$$
(f \ast g)( t ) := \int_{-∞}^{\infty} f(x) g( t-x ) dx
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;f&lt;/em&gt; and &lt;em&gt;g&lt;/em&gt; are the two functions undergoing convolution,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;t&lt;/em&gt; is the independent variable of the convolution,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;x&lt;/em&gt; is the integration variable, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;g(t-x)&lt;/em&gt; is the function, &lt;em&gt;g&lt;/em&gt;, reversed and shifted by t units. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Understanding the Operation&lt;/h1&gt;
&lt;p&gt;Intuitively I find it useful to conceptualize convolution as a "sliding window". One function is reversed and shifted across the other with corresponding values multiplied and -- for discrete cases -- summed to generate the convolution at a given point. &lt;/p&gt;
&lt;p&gt;Consider the following equation which expresses convolution as a discrete function:&lt;/p&gt;
&lt;p&gt;$$
(a \ast b)[n] = \sum_{k=0}^{N-1} a[k] \cdot b[n-k]
$$&lt;/p&gt;
&lt;p&gt;For the discrete operation (which is what's actually applied in machine learning) we can achieve deeper understanding by working through through a simple example. Suppose we have two lists:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;[1, 2, 3], and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[2, 3, 4]&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Essentially, as defined above, convolving the lists simply means applying the operation to generate a new list given the two input lists. In other words we flip one operand and slide, or, shift it along the second to generate the output...&lt;/p&gt;
&lt;div  &gt;
    &lt;div style="background-color: #888; color: #ffe; padding:0.5em;" &gt;
```
    1 2 3
4 3 2           1*2                2
```
    &lt;/div&gt;
    &lt;div style="background-color: #555; color: #ffe; padding:0.5em;" &gt;
```
    1 2 3
  4 3 2         1*3 + 2*2          7
```
    &lt;/div&gt;
    &lt;div style="background-color: #888; color: #ffe; padding:0.5em;" &gt;
```
    1 2 3
    4 3 2       1*4 + 2*3 + 3*2    16
```
    &lt;/div&gt;
    &lt;div style="background-color: #555; color: #ffe; padding:0.5em;" &gt;
```
    1 2 3
      4 3 2     2*4 + 3*3          17
```
    &lt;/div&gt;
    &lt;div style="background-color: #888; color: #ffe; padding:0.5em;" &gt;
```
    1 2 3
        4 3 2   3*4                12
```
    &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;So the result of the convolution for this simple example is [ 2, 7, 16, 17, 12 ]&lt;/p&gt;
&lt;p&gt;To gain further insight into the operation (and practice with algorithms) you might consider implementing the algorithm in your favorite programming language. I've included my own naive python implementation as an appendix to this post. For further study you might even consider looking at the python numpy implementation, but you'll find that bit more complicated.&lt;/p&gt;
&lt;h1&gt;Applications&lt;/h1&gt;
&lt;p&gt;There are innumerable applications that rely on convolution. It is widely used in signal processing, probability theory, and image processing -- just to name a few broad fields -- and, of course, machine learning. &lt;/p&gt;
&lt;p&gt;In machine learning, for purposes of image processing, the inputs to convolution (i.e., the source matrix and &lt;em&gt;kernel&lt;/em&gt;) are 2D matrices. Again, toward deeper understanding of the mathematics, it's worth working through a few examples by hand. &lt;/p&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;Let's consider the following matrix and associated kernel ( also referred to as &lt;em&gt;filter&lt;/em&gt; ). &lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td style="padding: 10px;"&gt; 
      &lt;font color='#F00'&gt;
$$
\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; 3 \\
4 &amp; 1 &amp; 0 &amp; 2 \\
3 &amp; 2 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 2 &amp; 4
\end{bmatrix}
$$
      &lt;/font&gt;
    &lt;/td&gt;
    &lt;td style="padding: 10px;"&gt;
      &lt;font color='#0AF'&gt;
$$
\begin{bmatrix}
0 &amp; 1 &amp; 2 \\
2 &amp; 2 &amp; 0 \\
1 &amp; 0 &amp; 1
\end{bmatrix}
$$ 
      &lt;/font&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style="padding-left: 18px;"&gt;
Input Matrix: $M$
    &lt;/td&gt;
    &lt;td style="padding-left: 18px;"&gt;
Kernel: $K$
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;As we did for the simple one dimensional example above, we can obtain the convolution of the matrix and its kernel by sliding the kernel -- this time over the two dimensions. To keep things simple for this example, I'll consider just the positions where there is complete overlap between the kernel and its input (i.e., no padding). This case is technically referred to as a &lt;em&gt;valid convolution&lt;/em&gt;. A valid convolution will yield a smaller matrix (fewer rows and columns) than the input. If we wanted an output matrix with the same dimensions (shape) as the input we'd have to "pad" the edges.&lt;/p&gt;
&lt;p&gt;So, given $M$ and $K$ as defined above we want a valid convolution, $O$, of the two: $ O = M \ast K $ . What would that look like? Below I've illustrated the convolution steps highlighting the elements in $M$ contributing to the output at each step. The result of the convolution will be a 2 X 2 matrix. &lt;/p&gt;
&lt;hr /&gt;
&lt;table&gt;

  &lt;tr&gt;
    &lt;td style="text-align:center"&gt;STEP 1&lt;/td&gt;
    &lt;td style="text-align:center"&gt;STEP 2&lt;/td&gt;
    &lt;td style="text-align:center"&gt;STEP 3&lt;/td&gt;
    &lt;td style="text-align:center"&gt;STEP 4&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td style="padding: 10px;"&gt; 
      &lt;font color='#F00'&gt;
$$
\begin{bmatrix}
\color{#08F} 1 &amp; \color{#08F} 2 &amp; \color{#08F} 0 &amp; 3 \\
\color{#08F} 4 &amp; \color{#08F} 1 &amp; \color{#08F} 0 &amp; 2 \\
\color{#08F} 3 &amp; \color{#08F} 2 &amp; \color{#08F} 1 &amp; 0 \\
0 &amp; 1 &amp; 2 &amp; 4
\end{bmatrix}
$$
      &lt;/font&gt;
    &lt;/td&gt;
    &lt;td style="padding: 10px;"&gt;
      &lt;font color='#F00'&gt;
$$
\begin{bmatrix}
1 &amp; \color{#08F} 2 &amp; \color{#08F} 0 &amp; \color{#08F} 3 \\
4 &amp; \color{#08F} 1 &amp; \color{#08F} 0 &amp; \color{#08F} 2 \\
3 &amp; \color{#08F} 2 &amp; \color{#08F} 1 &amp; \color{#08F} 0 \\
0 &amp; 1 &amp; 2 &amp; 4
\end{bmatrix}
$$
      &lt;/font&gt;
    &lt;/td&gt;
    &lt;td style="padding: 10px;"&gt;
      &lt;font color='#F00'&gt;
$$
\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; 3 \\
\color{#08F} 4 &amp; \color{#08F} 1 &amp; \color{#08F} 0 &amp; 2 \\
\color{#08F} 3 &amp; \color{#08F} 2 &amp; \color{#08F} 1 &amp; 0 \\
\color{#08F} 0 &amp; \color{#08F} 1 &amp; \color{#08F} 2 &amp; 4
\end{bmatrix}
$$
      &lt;/font&gt;
    &lt;/td&gt;
    &lt;td style="padding: 10px;"&gt;
      &lt;font color='#F00'&gt;
$$
\begin{bmatrix}
1 &amp; 2 &amp; 0 &amp; 3 \\
4 &amp; \color{#08F} 1 &amp; \color{#08F} 0 &amp; \color{#08F} 2 \\
3 &amp; \color{#08F} 2 &amp; \color{#08F} 1 &amp; \color{#08F} 0 \\
0 &amp; \color{#08F} 1 &amp; \color{#08F} 2 &amp; \color{#08F} 4
\end{bmatrix}
$$
      &lt;/font&gt;
    &lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td style="text-align:center"&gt;$\odot$&lt;/td&gt;
    &lt;td style="text-align:center"&gt;$\odot$&lt;/td&gt;
    &lt;td style="text-align:center"&gt;$\odot$&lt;/td&gt;
    &lt;td style="text-align:center"&gt;$\odot$&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td&gt;
      &lt;font color='#0AF'&gt;
$$
\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 2\\
2 &amp; 1 &amp; 0
\end{bmatrix}
$$ 
      &lt;/font&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;font color='#0AF'&gt;
$$
\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 2\\
2 &amp; 1 &amp; 0
\end{bmatrix}
$$ 
      &lt;/font&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;font color='#0AF'&gt;
$$
\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 2\\
2 &amp; 1 &amp; 0
\end{bmatrix}
$$ 
      &lt;/font&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;font color='#0AF'&gt;
$$
\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 2\\
2 &amp; 1 &amp; 0
\end{bmatrix}
$$ 
      &lt;/font&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Illustrates the convolution of the matrix, $M$, with the kernel, $K$. At each step, the elements of the kernel are multiplied against the elements of the input matrix (highlighted in blue). &lt;/p&gt;
&lt;p&gt;In general, the equation for a 2D convolution can be expressed as follows:&lt;/p&gt;
&lt;p&gt;$$
O(i, j) = \sum_{m=0}^{h-1} \sum_{n=0}^{w-1} I(i+m, j+n) \cdot K(h-1-m, w-1-n)
$$&lt;/p&gt;
&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$I(i+m, j+n)$ is the element of the input matrix at position $(i+m, j+n)$&lt;/li&gt;
&lt;li&gt;$K(h-1-m, w-1-n)$ is the corresponding element of the &lt;em&gt;flipped&lt;/em&gt; kernel, and &lt;/li&gt;
&lt;li&gt;O(i, j) is the element of the output matrix at the position $(i, j)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All this means is simply that the output matrix $O$ is obtained by flipping the kernel and sliding it over the input, performing element-wise multiplication at each step along the way. The convolution at each position $(i,j)$ of the output matrix is simply the sum of the element-wise products at each step.&lt;/p&gt;
&lt;p&gt;So for this example ...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Flip the kernel by 180 degrees:&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;&lt;td&gt;
$$
K_{180} =
\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 2 &amp; 2\\
2 &amp; 1 &amp; 0
\end{bmatrix}
$$
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Step 2.&lt;/strong&gt;  Compute the matrix element value for each step in the convolution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$O_{0,0} = (1\times1)+(0\times2)+(1\times0)+(0\times4)+(2\times1)+(2\times0)+(2\times3)+(1\times2)+(0\times1) = 11$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$O_{0,1} = (1\times2)+(0\times0)+(1\times3)+(0\times1)+(2\times0)+(2\times2)+(2\times2)+(1\times1)+(0\times0) = 14$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$O_{1,0} = (1\times4)+(0\times1)+(1\times0)+(0\times3)+(2\times2)+(2\times1)+(2\times0)+(1\times1)+(0\times2) = 11$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$O_{1,1} = (1\times1)+(0\times0)+(1\times2)+(0\times2)+(2\times1)+(2\times0)+(2\times1)+(1\times2)+(0\times4) = 9$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And so therefore the result of the convolution is the output matrix:&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;&lt;td&gt;
$$
O =
\begin{bmatrix}
11 &amp; 14 \\
11 &amp;  9 
\end{bmatrix}
$$
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;And just to be sure, we can check the answer we obtained using python ...&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
```
import numpy as np
from scipy.signal import convolve2d
input = [
    [1, 2, 0, 3],
    [4, 1, 0, 2],
    [3, 2, 1, 0],
    [0, 1, 2, 4]
]

kernel = [
    [0, 1, 2],
    [2, 2, 0],
    [1, 0, 1]
]
output = convolve2d( input, kernel, mode='valid')
print ( output )

```
&lt;/div&gt;

&lt;p&gt;&lt;code&gt;[[11 14]
 [11  9]]&lt;/code&gt;
Notice how we set the mode to &lt;em&gt;valid&lt;/em&gt;. scipy uses padding by default for &lt;code&gt;convolve2d&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, the above example walks through the convolution of a 2d matrix and a kernel -- an operation commonly applied in image processing. Next let's consider application of convolutions to machine learning -- let's draw the connection to CNN's, or, &lt;em&gt;convolutional neural networks&lt;/em&gt;.&lt;/p&gt;
&lt;h1&gt;Convolutional Neural Networks&lt;/h1&gt;
&lt;p&gt;So having done a bit of a deep dive into the mathematics of the convolution operation it's worth considering its application to machine learning. Again, convolution is ubiquitous in machine learning, but to launch into discussion here let's look at a very basic example from image processing. &lt;/p&gt;
&lt;p&gt;Suppose we want a classification system that can learn to categorize images. Image classification systems are widely used -- consider for example medical imaging, object identification in satellite images, traffic control systems -- the possibilities are endless. But, again, at the heart of a wide range systems in use today lies the &lt;em&gt;convolutional neural network&lt;/em&gt;, or, CNN. &lt;/p&gt;
&lt;h2&gt;The Models&lt;/h2&gt;
&lt;p&gt;To better understand CNN's and the impact of the application of &lt;em&gt;convolutional layers&lt;/em&gt;  I created two models in order to make some comparisons; a &lt;em&gt;multi-layer peceptron&lt;/em&gt; and a convolutional variant of the model. A multi layer perceptron (MLP) is an artificial neural network that can be used to learn complex patterns in data. Here's some sample python code which defines an MLP using tensorflow:&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
```
# Define a simple Multi Layer Perceptron model
model_mlp = models.Sequential([
    layers.Dense(64*64, activation='relu', input_shape=(128 * 128,)),
    layers.Dense(4, activation='softmax')
])
```
&lt;/div&gt;

&lt;p&gt;If you aren't familiar with tensorflow don't sweat it. For now, the point is that this code defines a neural network with three layers; an input layer (which is capable of processing 128 X 128 pixel images), a &lt;em&gt;hidden activation layer&lt;/em&gt;, and an output layer with 4 units (enabling classification into 4 categories). &lt;/p&gt;
&lt;p&gt;MLP's can be enhanced through the addition of convolutional layers in the network architecture. Here's some code which enhances the basic MLP with a convolutional layer. &lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em; font-size:smaller"&gt;
```
# Define a CNN model
model_cnn_4 = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(4, activation='softmax')
])
```
&lt;/div&gt;

&lt;p&gt;The convolutional layer is defined with set of 32 3 X 3 filters. The filters are convolved across the input to generate &lt;em&gt;feature maps&lt;/em&gt;. Randomly determined initially, the filter values are updated over the course of training (through &lt;em&gt;backpropagation&lt;/em&gt;) -- enabling the system to settle into a state that optimizes classification for the training set. In other words, convolutional layers enable the system to extract features from the input which can enhance learning analogous to the ways in which we as humans perceive and learn!&lt;/p&gt;
&lt;h2&gt;Testing the Models&lt;/h2&gt;
&lt;p&gt;In order to test the models I created a data set based on four classes (drawn from the four suits represented in decks of playing cards).&lt;/p&gt;
&lt;p&gt;&lt;img src='/images/convolution/card_suit_exemplars.png' 
     width='300px'
/&gt;&lt;/p&gt;
&lt;p&gt;To create the data set I took the four exemplars (shown above) and applied basic data augmentation techniques. I introduced variability using geometric rotations and translations, adding varying degrees of blur, and injecting random noise. Here are four examples (one from each class) drawn from a set of 80 items generated to train the model. &lt;/p&gt;
&lt;p&gt;&lt;img src='/images/convolution/card_suit_examples.png' 
     width='300px'
/&gt;&lt;/p&gt;
&lt;p&gt;The following graphs show the results of training and the training benefit obtained through convolution. &lt;/p&gt;
&lt;p&gt;&lt;img src='/data/cnn_training_2.png' 
     width='300px'
/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; Model accuracy and loss obtained over twenty training epochs for the MLP and CNN models. &lt;/p&gt;
&lt;p&gt;Figure 2 shows the training results obtained over 20 training epochs with the two models. The graphs represent model accuracy (left hand side) and loss (on the right). The model accuracy is a reflection of how accurate the model classification is across the data-set (i.e., the proportion of correct classifications). Loss is a representation of error. It's a measure of how far the model's predicted output deviates from the actual target output.&lt;/p&gt;
&lt;p&gt;So what these learning curves show is how convolutional layers can enhance learning in neural networks. Both models learn the data set -- that is, both improve in accuracy over the course of training. But the convolutional model achieves much greater accuracy than the simpler MLP. Also, the convolutional layer allows the model to settle into an more optimal state more quickly as shown by the loss curves. So there you have it. The mechanics of the convolutional neural network.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this blog post I've explored the mathematics of convolution in order to better understand it's application to machine learning. Starting with its definition as a continuous integral applied to two functions and considering its discrete counterpart I worked through a simple example in order to understand the mathematical concept. I then extended the discussion to consider convolution applied to 2D matrices (used ubiquitously in image processing). Finally, I provided a very simple comparison between a multi-layer perceptron model and one augmented with convolutional layers in order to see the benefit of using convolution to define CNN's. Hopefully, this post will help to deepen understanding of the building blocks of neural networks and encourage further exploration.&lt;/p&gt;
&lt;h1&gt;Appendix 1: My Naive Pass at Convolution -- An Exercise in Algorithm Implementation&lt;/h1&gt;
&lt;p&gt;This is just a naive python implementation -- an exercise solely intended to get those synapses firing. But, again, my philosophy is that in the same way doing push-ups enables you to exercise your muscles implementing algorithms enables you to exercise your brain. &lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em;" &gt;
```
def nn_convolve( a, b ) :
  '''
  My naive implementation of convolution ...
  '''
  b_flipped = np.flip( b )
  convolution = []
  start = len(b) - 1
  stop  = len(b)
  for i in range( len(a) + len(b) - 1 ) :
    k = 0
    j_range = range ( start, stop )
    for j in  j_range  :
      k += a[ i - (len(b)-1) + j ] * b_flipped[j]
    if start &gt; 0 :
      start -= 1
    if i &gt;= len( a )-1 :
      stop -= 1
    convolution.append(k)
  return np.array( convolution )
```
&lt;/div&gt;

&lt;p&gt;And some tests&lt;/p&gt;
&lt;div style="background-color: #555; color: #ffe; padding:0.5em;" &gt;
```
a = np.array( [1, 2, 3] )
b = np.array( [2, 3, 4] )
print( nn_convolve ( a, b ) )
a3 = np.array( [1, 2, 3, 4, 1, 2, 3, 4] )
b2 = np.array( [0.1, 0.5] )
print( nn_convolve ( a3, b2 ) )
print( np.convolve ( a3, b2 ) )
```
&lt;/div&gt;

&lt;p&gt;The key highlights regarding the solution are that it (1) flips the kernel and then computes the sum of element-wise multiplications as you slide the kernel across the signal (again, the essence of convolution).&lt;/p&gt;
&lt;h1&gt;Appendix 2: Exploring the numpy Implementation&lt;/h1&gt;
&lt;p&gt;For the truly intrepid, it may well be worth studying the python numpy implementation of the &lt;a href="https://numpy.org/doc/2.0/reference/generated/numpy.convolve.html"&gt;convolve function&lt;/a&gt;. The implementation is quite a bit more complex than our naive version, because (1) it is optimized for large arrays by using FFT to calculate the convolution, and (2) it is implemented in C for performance. At the time of this writing I determined that the implementation uses a python wrapper (&lt;a href="https://github.com/numpy/numpy/blob/v2.0.0/numpy/_core/numeric.py#L782-L878"&gt;numeric.py&lt;/a&gt;) and calls low-level C++ functions in the &lt;a href="https://github.com/numpy/numpy/blob/v2.0.0/numpy/_core/src/multiarray/multiarraymodule.c"&gt;multi-array module&lt;/a&gt; as illustrated in the following diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src="/diagrams/numpy_convolve.drawio.svg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;font size="smaller"&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; High level architecture of the numpy convolution implementation. Essentially, the 'convolve' function defined in &lt;em&gt;numeric.py&lt;/em&gt; calls a low-level C implementation defined in &lt;em&gt;multiarraymodule.c&lt;/em&gt;. Note: if you want to click directly into the source code try opening the diagram in a new tab. You should then be able to click the links...&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;The heart of the algorithm's implementation lies in &lt;code&gt;_pyarray_correlate&lt;/code&gt; since convolution is mathematically equivalent to cross-correlation (except for the reversal of the filter/kernel). Additional functionality (e.g., determining whether FFT optimization is warranted, flipping the kernel, checking for error conditions on function arguments) are added for &lt;em&gt;convolution&lt;/em&gt;.&lt;/p&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;h2&gt;Numpy&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://numpy.org/doc/2.0/reference/generated/numpy.convolve.html"&gt;numpy.convolve&lt;/a&gt; &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Encyclopedic entries on neural networks&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;Convolutional Neural Network&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Multilayer_perceptron"&gt;Multi-Layer Perceptron&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Visualizing Convolution&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=KuXjwB4LzSA&amp;amp;list=WL&amp;amp;t=392s"&gt;3Blue1Brown&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog"></category><category term="math"></category><category term="convolution"></category><category term="operator"></category><category term="convolutional neural network"></category><category term="neural networks"></category></entry><entry><title>What does Looking at Imagery with text-based AI Tell us about Creativity?</title><link href="https://dr-nick-nagel.github.io/blog/creative-ai.html" rel="alternate"></link><published>2024-08-21T00:00:00-04:00</published><updated>2024-08-21T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-08-21:/blog/creative-ai.html</id><summary type="html">&lt;p&gt;Some thoughts as we start delving into general AI ...&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Backdrop&lt;/h1&gt;
&lt;p&gt;So I had an interesting morning which involved working with AI and I'm kind of excited about it, so, figured it was "blog-worthy". First, a bit of background. I've had long-standing interests in many things, two of which include &lt;em&gt;scalable vector graphics&lt;/em&gt; (SVG) for illustration and, of course, &lt;em&gt;artificial intelligence&lt;/em&gt;. Lately, I've been working on my long neglected personal website which has been sorely in need of an update for many years, and using the hot new LLM's (Large Language Models) to flesh out some technical details. &lt;/p&gt;
&lt;p&gt;The fascinating thing about LLM's is that the architectural principle on which they're built is exquisitely simple. Essentially it boils down to a probabilistic model that predicts possible continuations given a prompt-generated discourse context. Such systems appear to be exhibiting intelligent linguistic behavior based on nothing more than an "educated guess" as to how to continue the next sentence fragment of a discourse. The key to understanding how these systems work is that the output is not deterministic -- it's not generated by explicit rules expressed in program code. Instead, it's a stochastic process enabling the system to &lt;em&gt;learn&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;Let's pause for a moment and consider just what this implies. I've had a long-standing interest in the nature of consciousness going back to my adolescence. In college I made it a formal study. But after reading Plato, Aristotle, Descartes and all the rest, and Dennett's &lt;em&gt;Consciousnesses Explained&lt;/em&gt;, I was left with the vague dissatisfaction that, really, very little in these works actually explain consciousness at all. It wasn't until I got to grad school and started working with the then nascent mathematics of neural networks that I really started to understand the implications of the building blocks of consciousnesses. &lt;/p&gt;
&lt;p&gt;Fast-forward to the present and the stunning rise of applied AI. Maybe I should say re-emergence. Broadly speaking, research into AI has gone through many incarnations since it's inception. Here I'm referring to the application of neural-network architectures and, more recently, transformers which have completely disrupted how we think and do business in the information age. &lt;/p&gt;
&lt;p&gt;The implications of so many discussions around AI bubbling up into public awareness, are that these systems have crossed a tipping point. Simple mathematical principles applied in the context of exponentially increasing computational power have given rise to systems exhibiting emergent properties which we might call &lt;em&gt;creativity&lt;/em&gt; in people. For the moment, anyway, let's limit the scope of the discourse to creativity.&lt;/p&gt;
&lt;h1&gt;Putting Creativity to the Test&lt;/h1&gt;
&lt;p&gt;So given that backdrop, I wanted to go ahead and put the creativity of these systems to the test. Historically, philosophers and scientists have long debated whether computers, with all their proficiency at number crunching, could ever be creative in the way that humans can. For example, in 1950 Turing advanced the concept of the behavioral test for computer intelligence. John Searle later proposed the "Chinese Room Argument" to suggest that computers, even if it seems like they do, can't really &lt;em&gt;understand&lt;/em&gt; language. &lt;/p&gt;
&lt;p&gt;So I decided to conduct a little experiment of my own. One of my site updates includes an SVG self portrait (as I said I have a long-standing interest in using SVG for illustration). As I was working on the image it occurred to me that what I'd drawn could use some more pizzazz. That got me to thinking; everybody's using AI these days to augment their prose. But what about imagery? Yes -- I'm aware that there are numerous generative AI applications that generate imagery (shameless self-promotion -- more posts on that coming soon), but I was curious about the text-based applications I've been using lately, namely ChatGPT and Google's Gemini. Also, the image-based generative AI systems don't operate on SVG. &lt;/p&gt;
&lt;p&gt;So I set about to try an "experiment" with Gemini. The conversation went something like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Nick:&lt;/strong&gt; Wanna try an experiment?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gemini:&lt;/strong&gt; Absolutely! I'm ready to experiment. Let's see what we can discover. What do you have in mind?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nick:&lt;/strong&gt; I'll send you my self portrait in SVG and I want you to try to make it more interesting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gemini:&lt;/strong&gt; Let's do it! I'm excited to see what kind of transformations we can achieve with your self-portrait. Send it over when you're ready, and we'll explore some creative options.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sadly, when I tried sending the SVG, it ended up breaking Gemini. The system just hung there and spun causing me to have to bounce my browser. &lt;/p&gt;
&lt;p&gt;Despite the initial fail, we can still make some interesting observations. By now, it's pretty well known that the dominant LLM's on the market, while really good at generating some types of discourse (given sufficiently detailed prompts), often are prone to make wild assertions while sounding extremely confident. The phenomenon's been deemed "hallucination". When I first encountered these types of assertions I thought of them as (unapologetically tongue-in-cheek) "lies". The point is that these systems don't appear quite ready to differentiate between education and deception/misrepresentation (perhaps an argument for the "absense of understanding" side of the debate). &lt;/p&gt;
&lt;p&gt;Still I was intrigued. All this &lt;em&gt;trying to get a text-based system to draw in SVG&lt;/em&gt; recalled to mind an NPR story I heard a while back. It was a segment on "This American Life" where David Kestenbaum was interviewing a Microsoft engineer working on ChatGPT around the time of its big public release. He was very excited about the insights that were emerging based on interactions with the system. Part of the interview included a discussion of how the  engineer hit on the notion of testing whether ChatGPT 4 could "draw". Given that the text-based LLM's can't really draw per se, the engineer took the same approach. He tried to get ChatGPT to draw a unicorn using TikZ (a LaTeX package used to create vector graphics -- kind of similar to SVG). &lt;/p&gt;
&lt;p&gt;So I fired up ChatGPT and asked it to draw a unicorn in SVG. And this is what I got...&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/chat_uni.svg"  /&gt;&lt;/p&gt;
&lt;p&gt;It's actually not too different from what the engineer described the as the TikZ output. The system attempted to portray a unicorn using shapes, paths and colors available to it in the mathematical markup language it could use to generate its output. Does that imply we can say, "This is what ChatGPT 'thinks' a unicorn is"?&lt;/p&gt;
&lt;h1&gt;My Key Insights&lt;/h1&gt;
&lt;p&gt;So what can we conclude from these little experiments? I mean, to me, the drawing's pretty lame. 'Looks more like a pig than a unicorn. And if you ask ChatGPT to get more creative it pretty much gives you back only slight variations on the theme. Same shapes, same colors, same unicorn features. I was hoping for something more like this...&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/inkscape_unicorn.svg", width=300  /&gt;&lt;/p&gt;
&lt;p&gt;Or even this ...&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/unicorn-mlp.svg", width=300  /&gt;&lt;/p&gt;
&lt;p&gt;Still, what's interesting to me about all this is what emerges from these systems based on nothing more than base variations on connectionist architectures and a few variations on activation rules, loss functions and optimizers. What's neat to me is how, given these atomic building blocks, the system at least &lt;em&gt;appears&lt;/em&gt; to have developed an internal representation, a &lt;em&gt;mental model&lt;/em&gt; if you will, of what a unicorn is supposed to be. The system has never explicitly been programmed or "told" how to draw a unicorn. And yet it seems to be creative enough to express its "understanding" using the languages at it's disposal.&lt;/p&gt;
&lt;p&gt;So does all this amount to a definitive answer to the creativity question? For now I'll leave it to the reader to decide. But what's most certain is the debate over the emergent properties of creativity in automated information processing systems has never been more salient. Researchers and practitioners involved in the creation of AI systems have identified stages of AI development ranging from &lt;em&gt;narrow&lt;/em&gt; to &lt;em&gt;general&lt;/em&gt; to &lt;em&gt;super&lt;/em&gt;. I've also heard a lot of (rightful) concern around the ethics surrounding the deployment of AI applications. Many artists and creative types express grave concerns over their potential displacement by creative (general?) AI systems. &lt;/p&gt;
&lt;p&gt;All that being said, I don't think it's debatable that we are very deep into the early stages of the emergence of general artificial intelligence with everything that that implies. I firmly believe that we are well down the road to understanding how to architect and create systems capable of general intelligence. But beyond that, I feel that exploring and understanding the internal representations of such systems can provide valuable knowledge and insights leading to deeper understanding of the nature of &lt;em&gt;our own awareness&lt;/em&gt;. 
And while I completely acknowledge the need to get ahead of the eight-ball with regard to the ethical deployment and utilization of these systems, I, for one, am keen to continue the exploration!&lt;/p&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;"Greetings, People of Earth." This American Life. WBEZ Chicago, 23 June 2023.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Searle, J. R. (1980). Minds, brains, and programs. Behavioral and Brain Sciences, 3(3), 417-457.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Turing, A. M. (1950). Computing machinery and intelligence. Mind, 59(236), 433-460.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Special Thanks&lt;/h1&gt;
&lt;p&gt;Immense gratitude to &lt;a href="https://pixabay.com/users/openclipart-vectors-30363/"&gt;OpenClipart-Vectors on pixabay&lt;/a&gt; for open use of the human generated unicorn art.&lt;/p&gt;</content><category term="Blog"></category><category term="philosophy"></category><category term="artificial intelligence"></category><category term="creativity"></category><category term="consciousness"></category><category term="art"></category><category term="SVG"></category><category term="vector graphics"></category></entry><entry><title>Simple Advice</title><link href="https://dr-nick-nagel.github.io/blog/simple-advice.html" rel="alternate"></link><published>2024-08-01T00:00:00-04:00</published><updated>2024-08-01T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-08-01:/blog/simple-advice.html</id><summary type="html">&lt;p&gt;Here's some sound advice...&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last week at the closing of one of my kung fu classes my sifu offered up a final lesson for the session. The teaching resonated deeply with me so I want to take the opportunity to share it here. It's summarized in three simple maxims: (1) &lt;em&gt;don't overthink it&lt;/em&gt;, (2) &lt;em&gt;don't over analyze it&lt;/em&gt;, and (3) &lt;em&gt;don't compare&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;As I thought about these words I knew exactly what Sifu meant in the context of class. I've been studying Wa Lum Tum Toi (northern mantis style kung fu) for decades. Over the years I've seen many students come and go. The system is hard work, there's a steep learning curve and it takes a toll on your body. But you get out of it what you put into it. If you train hard your body responds and you find yourself thinking more clearly, physically reacting more sharply, and possessed of a serenity that manifests in all aspects of life. &lt;/p&gt;
&lt;p&gt;The reason for the lesson that night was that we have a mix of students of varying degrees. Some have been at it longer than others and the range of ages in the class is from youthful high-schoolers to retirees in their seventies! With such a mix of talent, experience, and physical constraints, it's inevitable that you see differences in performance of the system forms -- the exercises that comprise the kung fu curriculum. And that's what prompted the lesson. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Don't overthink it.&lt;/em&gt; Free your mind to act. When learning something new (a new kung fu form, for example) it's easy to fall prey to overthinking things. Some of the forms in our system literally can take up to 15 minutes to complete and involve hundreds of moves. The thing is, all the moves can be broken down into basic chunks that form the core of the system. And the way you learn these building blocks is through practice and repetition. There's no magical, easy way in. Learning kung fu, or learning anything else for that matter, takes effort. From kung fu to art to mathematics, if you are not afraid to put in the effort you eventually get to a place where you no longer have to think. Given enough practice, the moves come naturally and seem effortless. So in following the path to excellence, don't overthink things. You'll struggle at first -- it's inevitable.  But if you keep at it, eventually you'll simply flow like water. In the words of Bruce Lee: "be water".&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Don't over analyze it.&lt;/em&gt; This second maxim is calling upon us to break out of the tendency toward over analysis. It might seem paradoxical coming from an information scientist -- someone swimming daily in the tools and techniques of quantitative analytics -- but there is an art to analysis. We need to analyze processes enough to achieve understanding, to streamline and optimize, but only just enough and not too much more. In kung fu we analyze scenarios to understand force and kinematics at a visceral level. But, again, you reach a point where you can't spend all your time over analyzing a situation. So analysis is great and necessary. But the teaching is to be careful about &lt;em&gt;over&lt;/em&gt;analyzing a situation which can lead to inaction. If you're in a sparring match, and your opponent throws a kick, you just need to block it in the simplest way. Sometimes you just need to act! Programming, though different from kung fu, is the same. It's far too easy to worry whether your solution is eloquent enough, or uses the latest and greatest language feature, when more often than not the simplest function gets the job done in the most usable and maintainable way.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Don't compare&lt;/em&gt;. It is this last point that, perhaps, resonated most deeply with me that evening. But what does it mean, really? If you've ever had an interest in pursuing some activity -- say, art, for example -- and become intimidated in comparing your work to others then you might start to get the idea. As I mentioned, there are many students in my kung fu class ranging widely in age and body type. And everyone has different reasons for coming to class. If a novice falls into the trap of comparing their performance to someone who's been at it for years it'd be easy to get discouraged and depressed. I feel that societal demands, wrongly, often lead to this kind of thinking in many facets of life. From earliest childhood on we're constantly being compared and evaluated against others, when in fact each individual has unique strengths and talents that they can bring to the table, given half a chance.  &lt;/p&gt;
&lt;p&gt;The beauty of this wisdom is it obviously doesn't just apply to kung fu. To me, these teachings are profoundly relevant to many spheres of life. Reflecting on the principles I'm reminded of a phrase I first learned from a student of mine when I taught java programming for Sun Microsystems many years ago; "paralysis by analysis". I love this phrase because it so aptly describes a tendency we all fall prey to, especially in this age of information overflow. With so much in our environment vying for our attention, it's too easy to spend all our time over analyzing a situation and get nothing done. In software engineering and project planning, the tendency toward over analysis is a major consideration in the dialectic between "cascade" approaches to project planning verses "agile" methodologies. But that's a major discussion which I'll set aside for another time. &lt;/p&gt;
&lt;p&gt;Comparing ourselves and our performance against others is a trap. Unfortunately though, it's a pitfall too easy to fall into given the competitive demands of our environment. Unnecessarily comparing ourselves and our performance to others can harm the ego in many ways. One is over inflation. For whatever reason, it's all too common to observe the cognitive bias in many individuals who tend to overestimate their abilities and accomplishments. I'll never forget walking into the office one morning to hear a newly hired young programmer proclaim; "I'm a genius! I wrote a script months ago that I just re-used this morning!" 'Turns out the script amounted to a glorified file-copy on a linux system. But my point is to try to be self-aware enough to have pride in one's strengths but have enough humility to recognize one's limits. In my opinion, one doesn't proclaim oneself a genius. Genius is recognized by others. &lt;/p&gt;
&lt;p&gt;But the other side of the coin also holds true. While true that there's a tendency in some to overestimate their competence there are tendencies in others to underestimate their capabilities and achievements. As a lover of art, for example, I've recently discovered the joy of painting with oils. But attending a recent art studio session (part of my continuous efforts toward lifelong learning) I became intimidated seeing the masterful work produced by other artists in attendance. But, again, at a subsequent session, another artist offered the same simple advice as my kung fu sifu; "don't compare your work to others". Every artist has their own style -- that's what makes you an artist. Who can compare Van Gogh to Leonardo Da Vinci? Frank Frazetta is vastly inspirational to me as an artist and yet spent much time drawing comic books (which, again, is another topic worthy of more elaborate discussion). But, say what you will about the merits of comics as a form of art, one of his original paintings (an illustration for a pulp magazine) recently sold for over five million dollars. &lt;/p&gt;
&lt;p&gt;My point is that it's often useless to attempt to judge our own efforts and achievements by way of comparison against others. Recently, I've been hearing the term "imposter syndrome" along with concern over its concomitant features of self-doubt, perfectionism and anxiety. It seems that it's become increasingly easy to lower one's self-estimation in our new information age with increasing demands for workplace and social comparison. Instead, I feel it's increasingly important not to get discouraged upon seeing amazing work and the achievements of others and feeling that our own creative efforts fall short in comparison. Instead I'll have to ask that you please allow me the cliche; whatever you may end up doing in life, whatever path you may walk, just がんばってね . Do your best. &lt;/p&gt;</content><category term="Blog"></category><category term="philosophy"></category></entry><entry><title>Setting up to Blog with Pelican</title><link href="https://dr-nick-nagel.github.io/blog/pelican-themes.html" rel="alternate"></link><published>2024-07-29T00:00:00-04:00</published><updated>2024-07-29T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-07-29:/blog/pelican-themes.html</id><content type="html">&lt;p&gt;Most critical Pelican Docs out of the gate...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.getpelican.com/en/3.6.2/quickstart.html"&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.getpelican.com/en/latest/themes.html"&gt;Themes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.getpelican.com/en/latest/pelican-themes.html"&gt;pelican-themes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.getpelican.com/en/latest/settings.html"&gt;Settings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Process&lt;/p&gt;
&lt;p&gt;RESUME HERE...&lt;/p&gt;</content><category term="Blog"></category><category term="pelican"></category><category term="themes"></category><category term="setup"></category></entry><entry><title>Using LaTeX with Pelican</title><link href="https://dr-nick-nagel.github.io/blog/latex-pelican.html" rel="alternate"></link><published>2024-07-25T00:00:00-04:00</published><updated>2024-07-25T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2024-07-25:/blog/latex-pelican.html</id><summary type="html">&lt;p&gt;If you have an interest in maching learning and want to write about it, odds are at some point you'll want to add in some math. So how do you do that with Pelican?&lt;/p&gt;</summary><content type="html">&lt;p&gt;As a passionate blogger of many years I've increasingly had a desire to formalize ideas I'm working on using mathematics. After all, as I always say, "Math is truth". That said, it's hard to express mathematical notation in digital media. Or rather I should say it's hard if you can't use LaTeX : ) .&lt;/p&gt;
&lt;p&gt;So when I recently upgraded my blogging system (now using pelican) one of the first things I had to figure out was how to support LaTeX. A little research turned up (as always) not just one but &lt;em&gt;multiple&lt;/em&gt; approaches ranging from modding templates to using a number of plugins. After thinking about the alternatives, it turns out that the simplest, downest, dirtiest approach (for me) was to add a simple script to my theme. The approach is quick, easy, least touch and robust. &lt;/p&gt;
&lt;h1&gt;Method&lt;/h1&gt;
&lt;p&gt;The method uses &lt;em&gt;MathJax&lt;/em&gt;. MathJax is a javascript display engine for mathematics that just plain works in all browsers. To use it simply:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;include the following script in you HTML &lt;em&gt;header&lt;/em&gt; element for your page template:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
&amp;lt;script type="text/javascript" 
           async
           src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"&gt;&amp;lt/script&gt;
&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Add the following script in the same area:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
&amp;lt;script&gt;
MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
};
&amp;lt;/script&gt;
&lt;/pre&gt;

&lt;p&gt;And bang! you're done. Easy peasy.&lt;/p&gt;
&lt;p&gt;Following I have a few tests that show how easy it is to use LaTeX with these changes...&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Same line rendering: $\color{red}{f(x) = x^2}$ . The math should render in-lined.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Same line rendering: $f(x) = x^2$ . The math should render in-lined.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TEST: if I had $1,000,000.00, I would buy you a house... (you should see the dollar character '\$')&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Block Rendering. Remember the activation function?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
\sigma \left( \sum a_i w_{ij} + b_j \right)
$$&lt;/p&gt;
&lt;p&gt;Update &lt;/p&gt;
&lt;p&gt;Ran into an issue with latex rendering using alignment. Often I need to align around the equal sign as in:&lt;/p&gt;
&lt;pre&gt;
$$
\begin{align}
P ( Hyp | Event ) &amp;= \frac {P ( Event | Hyp ) \times P ( Hyp ) } { P ( Event ) } \\
                  &amp;= \frac {0.99 \times 0.25 } { 0.50 } \\
                  &amp;= 0.495
\end{align}
$$
&lt;/pre&gt;
&lt;p&gt;'Problem is, in  such cases Pelican will perform entity substitution on the the '&amp;amp;' (i.e., '&amp;amp;' --&amp;gt; &amp;amp;amp;)  causing the alignment to fail. To work around the issue, wrap the latex in a div like so...&lt;/p&gt;
&lt;pre&gt;
&amp;lt;div&gt;
$$
\begin{align}
P ( Hyp | Event ) &amp;= \frac {P ( Event | Hyp ) \times P ( Hyp ) } { P ( Event ) } \\
                  &amp;= \frac {0.99 \times 0.25 } { 0.50 } \\
                  &amp;= 0.495
\end{align}
$$
&amp;lt;/div&gt;
&lt;/pre&gt;

&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.mathjax.org/"&gt;MathJax&lt;/a&gt; .&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog"></category><category term="math"></category><category term="LaTeX"></category><category term="equations"></category></entry><entry><title>Wah Lum Tam Tui Kung Fu</title><link href="https://dr-nick-nagel.github.io/blog/kung_fu.html" rel="alternate"></link><published>2010-12-20T00:00:00-05:00</published><updated>2010-12-20T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2010-12-20:/blog/kung_fu.html</id><summary type="html">&lt;p&gt;Kung fu motion capture.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Kung Fu Moves&lt;/h1&gt;
&lt;p&gt;Back when I was visiting Aizuwakamatsu Japan (会津若松市) I had the good fortune to be invited to participate in a motion capture session with &lt;a href="https://www.nowhere.co.jp/en"&gt;Eyes Japan&lt;/a&gt;. Don't get me wrong -- my kung fu is not something to brag about. But what I loved about the session was the &lt;em&gt;concept&lt;/em&gt; of capturing kung fu moves in general-- for posterity. That is, by it's very nature, kung fu is not something that can be easily recorded in a book. There's no standard, easy way to transcribe the movements. When one makes a study of kung fu it has to be passed down through direct observation. In that sense it's kind of like an oral tradition -- you have to hear and learn the stories to hand them forward from one generation to the next. &lt;/p&gt;
&lt;p&gt;What I liked about the mo' cap was that I had a sense that the kung fu moves could be recorded for posterity. The tradition I study is &lt;a href="https://www.wahlum.com/history/"&gt;Wah Lum Tam Tui&lt;/a&gt; (northern mantis style) kung fu. Kung Fu itself has a long history going back a thousand years. I've been studying it myself for more years than I care to count. I engage in kung fu for the physical exercise, but also for  mental and spiritual discipline. Kung fu exercises not just body but also mind. &lt;/p&gt;
&lt;p&gt;The motion capture effort was quite valuable to me inasmuch as that it serves as a sort of record -- a visual log or diary -- of a long standing and beautiful cultural tradition. &lt;/p&gt;
&lt;hr /&gt;
&lt;div style="text-align:center"&gt;
    &lt;iframe id="nnkf_1"
        src="https://www.youtube.com/embed/0YXxwUQdcKw?si=DbPVxPpfzhmZYAb9"
        width='350' 
        height='300'
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" 
        allowfullscreen
    &gt;&lt;/iframe&gt;
    &lt;div&gt;Broad Sword Form&lt;/div&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;div style="text-align:center"&gt;
    &lt;iframe id="nnkf_1"
        src="https://www.youtube.com/embed/8A9r576AmM4?si=V2tYsIWTbrvMMcur"
        width='350' 
        height='300'
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" 
        allowfullscreen
    &gt;&lt;/iframe&gt;
    &lt;div&gt;Cane Form&lt;/div&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;div style="text-align:center"&gt;
    &lt;iframe id="nnkf_1"
        src="https://www.youtube.com/embed/tUnsxu-qDH0?si=jXMfqcueQoQyeo61"
        width='350' 
        height='300'
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" 
        allowfullscreen
    &gt;&lt;/iframe&gt;
    &lt;div&gt;Sixteen Hands&lt;/div&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;div style="text-align:center"&gt;
    &lt;iframe id="nnkf_1"
        src="https://www.youtube.com/embed/vBZnRE4TtZM?si=HrAq7lK-2XjF0tPm"
        width='350' 
        height='300'
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" 
        allowfullscreen
    &gt;&lt;/iframe&gt;
    &lt;div&gt;Kicks&lt;/div&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;div style="text-align:center"&gt;
    &lt;iframe id="nnkf_1"
        src="https://www.youtube.com/embed/_5-N0r8pU9E?si=p-OMnlqhLzmoeHKU"
        width='350' 
        height='300'
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" 
        allowfullscreen
    &gt;&lt;/iframe&gt;
    &lt;div&gt;Sul Yee Ma&lt;/div&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;div style="text-align:center"&gt;
    &lt;iframe id="nnkf_1"
        src="https://www.youtube.com/embed/UVTzhLqAbqY?si=eR2roArGKv4792YQ"
        width='350' 
        height='300'
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" 
        allowfullscreen
    &gt;&lt;/iframe&gt;
    &lt;div&gt;Mo' Sword&lt;/div&gt;
&lt;/div&gt;

&lt;h1&gt;Pole Dance&lt;/h1&gt;
&lt;p&gt;While visiting Eyes Japan, they happened to be capturing a different type of motion than the kung fu. But it was fun to watch! So I included it here &lt;code&gt;:)&lt;/code&gt; ...&lt;/p&gt;
&lt;hr /&gt;
&lt;div style="text-align:center"&gt;
    &lt;iframe id="nnkf_1"
        src="https://www.youtube.com/embed/jsLFA7zqKDI?si=MxPgCiTA6xfftfuK"
        width='350' 
        height='300'
        title="YouTube video player" 
        frameborder="0" 
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
        referrerpolicy="strict-origin-when-cross-origin" 
        allowfullscreen
    &gt;&lt;/iframe&gt;
    &lt;div&gt;Pole Dance&lt;/div&gt;
&lt;/div&gt;</content><category term="Blog"></category><category term="kung fu"></category><category term="wah lum tam tui"></category><category term="northern"></category><category term="mantis"></category><category term="style"></category><category term="martial arts"></category><category term="pole dance"></category></entry><entry><title>SVG Animation</title><link href="https://dr-nick-nagel.github.io/blog/svg-anim.html" rel="alternate"></link><published>2010-01-20T00:00:00-05:00</published><updated>2010-01-20T00:00:00-05:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2010-01-20:/blog/svg-anim.html</id><summary type="html">&lt;p&gt;Animating SVG&lt;/p&gt;</summary><content type="html">&lt;style&gt;
  #cheshire-container {
      position: fixed;
      /* width: 20vw; */
      /* max-width: 172px; */
      /* height: auto; */
      z-index: 1000;
      transition: top 0.3s ease-out, left 0.3s ease-out; /* Smooth transition on resize */
  }
&lt;/style&gt;

&lt;script src="/svg/loadsvg.js"&gt;&lt;/script&gt;

&lt;script&gt;
  const URL = "/svg/ChesireCat_Anim.svg";

  document.addEventListener("DOMContentLoaded", function () {

      fetch(URL)  // Adjust the path if needed
          .then(response =&gt; response.text())
          .then(svgText =&gt; {
              let parser = new DOMParser();
              let svgDoc = parser.parseFromString(svgText, "image/svg+xml");
              let svgElement = svgDoc.documentElement;

              // Create a floating container div
              let container = document.createElement("div");
              container.id = "cheshire-container";
              container.appendChild(svgElement);
              document.body.appendChild(container); // Append to &lt;body&gt; so it's fixed

              function positionCat() {
                  let contentBox = document.querySelector("main.content");
                  if (!contentBox) return;

                  let rect = contentBox.getBoundingClientRect();
                  let padding = 10; // Offset from the edge

                  container.style.top = `${(rect.top-105) + window.scrollY + padding}px`;
                  container.style.left = `${(rect.right+40) + window.scrollX - container.offsetWidth - padding}px`;
              }

              positionCat(); // Position initially
              window.addEventListener("resize", positionCat); // Update on resize
          })
          .catch(error =&gt; console.error("Error loading Cheshire Cat SVG:", error));

          loadSvg( "/svg/my_N.svg", "my_n" );
  });
&lt;/script&gt;

&lt;style&gt;
#n_wrap {
    width:  120px;
    height: 160px;
    border: inset 4px rgb(200, 200, 200);
    margin-left: auto;
    margin-right: auto
}
&lt;/style&gt;

&lt;div id="n_wrap"&gt;
  &lt;svg id="my_n"
    width="120px"
    height="160px"
  &gt;&lt;/svg&gt;
&lt;/div&gt;

&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;If you are reading this page then you may have noticed the grinning cat on the upper corner of the content box. On the surface the drawing may seem nothing special -- yet another borrowing from Lewis Carroll's ubiquitously famous &lt;em&gt;Alice in Wonderland&lt;/em&gt;. But go ahead and try moving your mouse cursor over the creature and you'll see it perform its famous magic. But what's interesting about this little web-graphic is what lies beneath the surface -- the technology I used to create it -- namely SVG. &lt;/p&gt;
&lt;p&gt;SVG (for Scalable Vector Graphics) is perhaps one of the most underrated standards available for WWW development today. Let me explain what I mean. First, what exactly is SVG? SVG is a markup language (derived from XML) for representing vector graphics. As such, it is a text-based format for describing images that can be rendered cleanly because the underlying representation is not a bitmap, but instead is a mathematical description of the features composited to form the image. As an open Web standard SVG can be used in conjunction with other web standards including HTML, CSS, and javascript to create beautiful and engaging user experiences. &lt;/p&gt;
&lt;p&gt;By now, many readers of this blog are likely familiar with web standards like HTML, CSS, and javascript. But surprisingly (to me anyway) far fewer folks are familiar with SVG. To my delight, however, I expect that will soon change. The reasons are that: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Quality browsers are increasingly providing greater support for the SVG standard, and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quality low cost (even free) tools are presently emerging geared specifically toward SVG content creation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So while SVG has been neglected for a long time, it will soon be blossoming and coming into its own as a valuable addition to browser-based presentation languages. So, with that in mind, the purpose of this post is to explore one particularly cool and interesting aspect of SVG -- animating content.&lt;/p&gt;
&lt;h1&gt;Why the Cheshire Cat?&lt;/h1&gt;
&lt;p&gt;I'm presenting this little trick here for folks getting their feet wet and exploring some of what the rich SVG standard has to offer. The techniques used to create the fading cat illusrate some  key points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;SVG, used in concert with HTML, CSS and Javascript raises web-based presentation to new heights. Imagine doing curves in pure, standards compliant mark-up (as opposed to using kludgy ugly workarounds like blending images with text and substituting images for parsable text).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SVG can be directly and easily woven into xHTML documents using XML namespaces (a truly beautiful concept).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And given one and two above, SVG nicely completes pure, standards-based graphics design for the WWW with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the capability to specify shapes and curves using WWW standards (thus rounding out CSS), and &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the capability to provide animated content without the need for non-standard plug-ins.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Technique&lt;/h1&gt;
&lt;p&gt;OK enough of the pre-amble. Let's get to the meat of it; how does Cheshire do his trick?&lt;/p&gt;
&lt;h2&gt;Creating the Art&lt;/h2&gt;
&lt;p&gt;First, we have to draw him. Although simple shapes and objects could, in principle, be handcrafted in SVG, to do any reasonably complex art you really need a good tool. As of this writing, the best SVG tool out there is &lt;a href="http://www.inkscape.org"&gt;Inkscape&lt;/a&gt;. And as it turns out Inkscape is open-source and available for free. So there's no excuse for you not to download your own executable and start drawing today ...&lt;/p&gt;
&lt;p id='note_1'&gt;When you draw Ches (or whatever other image you want to apply the fade effect to) make sure to divide him up into layers &lt;sup&gt;&lt;a href="#en_1"&gt;1&lt;/a&gt;&lt;/sup&gt;. In this case, I have two separate layers; one for the grin, and one for everything else. You'll see why in a bit.&lt;/p&gt;

&lt;p id='note_2'&gt;Once you have the image, you'll have to do a bit of tweaking to get the animation&lt;sup&gt;&lt;a href="#en_2"&gt;2&lt;/a&gt;&lt;/sup&gt;. So, *after* you've worked all the kinks out of your drawing comes the next stage in the workflow: adding the animation.&lt;/p&gt;

&lt;p&gt;To add the animation, you'll need to manually edit the SVG XML. Inkscape has it's own on-board XML editor, but I prefer to close out of Inkscape and just do the manual editing in a good text editor. I like &lt;a href="http://www.textpad.com/"&gt;Text-pad&lt;/a&gt; on Windows but, of course, any text editor will work just fine.&lt;/p&gt;
&lt;h2&gt;Creating the Effect&lt;/h2&gt;
&lt;p&gt;There a different ways to animate SVG. One is to use javascript to access and manipulate SVG DOM elements. But here I'm going to focus on techniques built into the SVG specification itself. Specifically, I'll be looking SVG (SMIL) animation elements. That is, SVG includes a number of animation tags from the SMIL standard. A full exposition is beyond the scope of this article, but you can see the &lt;a href="http://www.w3.org/TR/SVG11/animate.html"&gt;spec itself&lt;/a&gt; for the complete set (and as an added benefit it's great bed-side reading -- if you suffer from insomnia!). &lt;/p&gt;
&lt;p&gt;Short of going through the exhaustive list, getting started with the elements is really easy. A lot can be accomplished simply by applying the &lt;code&gt;animate&lt;/code&gt; tag like so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Add the x-link namespace. SVG decouples animation instructions from
    their targets. To link the instructions to their target you'll use
    &lt;code&gt;xlink:href&lt;/code&gt; attributes, hence the need of the namespace. Inkscape
    doesn't include the namespace by default (as of version 0.46
    anyway) so you'll have to add it to the Inkscape generated SVG. Add
    it to the SVG root element as illustrated below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;svg
   ...
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   ...
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:xlink="http://www.w3.org/1999/xlink"
   width="172"
   height="102"
   id="svg2"
   ...
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code fragment shows the SVG root element with the xlink
namespace attribute I added. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, you'll have to tweak the Inkscape generated code a bit.
    Locate the 'g' element (the SVG "layer") that will be the target
    of the animation. Change it to a 'symbol' element.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;symbol
   inkscape:groupmode="layer"
   id="layer4"
   inkscape:label="body"
   style="display:inline"
   sodipodi:insensitive="true"
   opacity="1"&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, add the opacity attribute as shown above. This is the
parameter you'll manipulate to achieve the animation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, add the animation instructions. Insert the animation tags
    right after the 'symbol' as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;use xlink:href="#layer4" &amp;gt;
  &amp;lt;animate attributeName="opacity"
           begin="mouseover"
           restart="whenNotActive"
           dur="4s"
           values="1; 0; 1" /&amp;gt;
&amp;lt;/use&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 'use' element specifies the target of the animation using the
xlink href attribute. The 'animate' tag holds a number of
attributes representing animation parameters. 'attributeName' is
the parameter to modify and can be used to move, rotate, and scale
objects and, in this case, affect transparency. 'begin' specifies
an event to trigger the animation (and can specify a time as well).
'restart' tells the browser only to start the animation under
specific conditions. The 'duration' is a timeline for the
animation, and the 'values' are the parametric values of the
animation target over that duration.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p id='note_3'&gt;It's encouraging to me to see browsers ramping up support for SVG. As of the time of this writing any modern Web browsers worth their salt&lt;sup&gt;&lt;a href="#en_3"&gt;3&lt;/a&gt;&lt;/sup&gt; provide at least some support for rendering SVG.&lt;/p&gt;
&lt;p&gt;Hopefully, the little trick I've presented here is enough to pique folks' interest and curiosity in SVG as a viable addition to web content development.&lt;/p&gt;
&lt;div class="admonition note"&gt;
&lt;p class="admonition-title"&gt;Update 2025&lt;/p&gt;
&lt;p&gt;When I wrote this blog post waaay back in 2010 the web-development landscape was a lot different than it is today. At the time, web devs were constantly doing 3X the work necessary to create presentation code in order to support web browsers that failed to support WWW standards. Since then, things have changed and bucking the standards has become the exception to the rule, not the norm. &lt;/p&gt;
&lt;p&gt;Meanwhile support for SVG has grown continually to the point that it has indeed become integral to WWW content development. Not just to create icons and simple graphics but to create beautiful and inspired works of art. &lt;/p&gt;
&lt;p&gt;The approach to SVG animation I originally took here -- using SMIL tags to create effects -- has been debated off an on in recent times (with some arguing for the use of CSS3 over SMIL). But the fact is SMIL tags remain part of the SVG standard and are supported in nearly all modern &lt;a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Element/animate#browser_compatibility"&gt;standards-compliant browsers&lt;/a&gt;. And I was simply delighted when I went to integrate this post with my newly updated blog environment. All I had to do was grab one file and everything worked fine out of the box. That's the real magic of SMIL 15 years later. Untouched, and it still works. No worrying about JavaScript frameworks breaking, no outdated dependencies, just pure SVG doing its thing. That's the kind of robustness you can build an artistic legacy on! &lt;/p&gt;
&lt;p&gt;So for me, the bottom line is this. For simple animations I favor SMIL elements which can be used to accomplish numerous effects. For more complex simulations, game development and creating dynamic SVG artworks I'll employ javascript. As a passionate SVG evangelist returning to work with this powerful standard I remain very excited! &lt;/p&gt;
&lt;/div&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.w3.org/TR/SVG11/animate.html"&gt;The SVG Specification&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://inkscape.org"&gt;Inkscape&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Endnotes&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;p id="en_1"&gt; SVG doesn't have the concept of layers per se but Inkscape implements graphics layers by mapping the concept onto SVG &lt;em&gt;groups&lt;/em&gt; (i.e., &lt;code&gt;g&lt;/code&gt; elements). &lt;a href="#note_1"&gt;&amp;uarr;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;p id="en_2"&gt; As of this writing Inkscape does not support SVG animation. But what do you want for free, eh? Send them some money (developers have to eat too you know) and then you can complain ... &lt;a href="#note_2"&gt;&amp;uarr;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;p id="en_3"&gt; As of this writing the latest version of Firefox (3.6) supports only a subset of SVG features. So while you may see the SVG graphic, you may not see the animated fade effect if you are using Firefox. You can see the full effect using Google Chrome. I.E. and Safari are another story...&lt;a href="#note_3"&gt;&amp;uarr;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="blog"></category><category term="animation"></category><category term="svg"></category><category term="scalable vector graphics"></category><category term="art"></category><category term="artwork"></category><category term="alice"></category><category term="wonderland"></category><category term="cartoon"></category><category term="chesire cat"></category><category term="smil"></category><category term="artworks"></category><category term="framework"></category><category term="Inkscape"></category><category term="Illustrator"></category><category term="Adobe"></category></entry><entry><title>このガイジンの冒険は Bandai -san</title><link href="https://dr-nick-nagel.github.io/blog/bandai.html" rel="alternate"></link><published>2009-10-29T00:00:00-04:00</published><updated>2009-10-29T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2009-10-29:/blog/bandai.html</id><summary type="html">&lt;p&gt;Adventures in paragliding: Jumping off of Bandai san!&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Paragliding in Bandai&lt;/h1&gt;
&lt;p&gt;This Gaijin will always remember visiting Japan. Definitely a major highlight of my existance. While there I had the good fortune to be invited to try my hand at paragliding (I'd never done it before). &lt;/p&gt;
&lt;div style="width:400px"&gt;

&lt;object width="400" 
        height="300" &gt;
  &lt;param name="src" value="/video/My_Turn.MP4" /&gt;
  &lt;param name="autoplay" value="true" /&gt;
  &lt;embed src="/video/My_Turn.MP4" 
         width="400" 
         height="300" 
         autoplay="true"&gt;&lt;/embed&gt;
&lt;/object&gt;

&lt;p style='text-align:center'&gt;Yup that's me up there. Gliding off of Bandai San in Inawashiro, Japan. What fun!&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;The place was Bandai -san -- A mountain near Azuwakamatsu. I was visiting the University of Aizu to collaborate on work related to the &lt;em&gt;Immersive Education Initiative&lt;/em&gt;. &lt;/p&gt;
&lt;p&gt;Bandai -san is a beautiful volcanic peak located in Bandai-town, and Kitashiobara village, in Yama-Gun, Fukushima prefecture.&lt;/p&gt;
&lt;div style="border: inset 4px grey; width: 400px"&gt;

    &lt;iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d214527.83975608827!2d139.88054177215298!3d37.534688032225525!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x5f8aa565471aad81%3A0xb6ecffcff8edabe5!2sMount%20Bandai!5e0!3m2!1sen!2sus!4v1743024265697!5m2!1sen!2sus" width="400" height="380" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"&gt;&lt;/iframe&gt;

&lt;/div&gt;

&lt;p&gt;Visiting the onsen there and biking around the region were definitely highlights of my visit. My thanks to Michael Cohen and Jun Yamadera for providing me the opportunity to jump off the mountain!&lt;/p&gt;</content><category term="Blog"></category><category term="paragliding"></category><category term="adventure"></category><category term="travel"></category><category term="bandai"></category><category term="mountain"></category><category term="aizuwakamatsu"></category><category term="japan"></category><category term="inawashiro"></category></entry><entry><title>Paddling Cape to Cape: On Crossing Massachusetts Bay by Sea Kayak</title><link href="https://dr-nick-nagel.github.io/blog/cape2cape.html" rel="alternate"></link><published>2007-10-08T00:00:00-04:00</published><updated>2007-10-08T00:00:00-04:00</updated><author><name>Nick Nagel</name></author><id>tag:dr-nick-nagel.github.io,2007-10-08:/blog/cape2cape.html</id><summary type="html">&lt;p&gt;Adventures in paddling: crossing Massachusetts Bay by sea kayak&lt;/p&gt;</summary><content type="html">&lt;style&gt;

body::-webkit-scrollbar {
    display:none;
}

body {
    font-family: "Times New Roman", Times, serif;
    overflow-y: scroll;

    .content {
        overflow-y: scroll;

        div.ImageRack
        {
          position: relative;
          left:          50%;
          margin-left:-400px;
          width:       800px;
          text-align: center;
        }
        div.ImageRack img
        {
          border:solid black 1px;
        }

    }

    .content::-webkit-scrollbar {
        display:none
    }

    h2 {
        margin: -20px;
        margin-bottom: 20px;
        padding: 20px;
        background-color: #fafafa;
        font-family: "", Arial, sans-serif;
        border-bottom: solid #222 1px;
        text-align: center;
        font-size: 14pt;
    }

    h2 a:link,
    h2 a:visited {
        text-decoration: none;
        color: #002;
    }

}

.content

div.heading {
    text-align:center;
    font-weight:bold;
    font-size:18pt
}
&lt;/style&gt;

&lt;div class='heading' &gt;
~ I ~
&lt;/div&gt;

&lt;p&gt;What can I say? It simply had to be done. Chris Thomas and I had talked
about it for years. I even recall a colleague from years gone by, David
Gow, mentioning it as some sort of remote objective when I first got
introduced to sea kayaking (more years ago, now, than I care to count).
But for whatever reason, this simply had to be the year that we paddled
our sea kayaks from cape to cape; Cape Ann Massachusetts (Gloucester) to
Province Town, Cape Cod.&lt;/p&gt;
&lt;p&gt;Whenever I chat about it with non-seakayakers they sort of do a double
take, look at me like I'm crazy, and then &lt;em&gt;tell&lt;/em&gt; me that I'm crazy. I
never claimed otherwise.&lt;/p&gt;
&lt;p&gt;But to any sea kayaker ~~ any &lt;em&gt;real&lt;/em&gt; sea kayaker ~~ the idea of
crossing Massachusetts Bay (a 47 mile odyssey over open ocean and
completely out of sight of land) is as natural as a hiker wanting to
summit their favorite mountain, or an athlete wanting to run a marathon.&lt;/p&gt;
&lt;p&gt;Don't get me wrong ~~ this was no walk in the park. First of all, to
our knowledge, crossing Massachusetts Bay by sea kayak had never been
done before. At least, I know of no written record of the feat. If
anyone reading this can lay claim to having paddled cape to cape before
us please let me know. But that's beside the point anyway. Certainly
many paddlers have logged much more time on a journey and achieved far
greater feats. I even know of at least one intrepid paddler that has
kayaked the entire US eastern sea-board. But, to my knowledge, there is
no record of a continuous crossing from Cape Cod to Cape Ann, one point
of land to the other, a crossing which ended up taking us about 13 and a
half hours to complete.&lt;/p&gt;
&lt;p&gt;The planning alone was difficult ~~ never mind the execution. You
can't just pick a day and say, "Let's paddle across Massachusetts Bay
today". So many factors have to fall into place to make that happen. I
think the smartest thing we did was choose a two-week window in early
fall and say we have to be ready to go at any minute within that
time-frame ~~ weather permitting. And that right there is the key
phrase; &lt;em&gt;weather permitting&lt;/em&gt;. You don't want to get caught out on the
open ocean, 20-30 miles from the nearest point of land, in a sea kayak,
and have a storm come up. Or even just sustained 20-mile-an-hour winds
for that matter! When you're sitting there at water level and making
headway completely under your own steam, every stroke is critical, and
wind and current can change a pleasant excursion into a tragic nightmare
in short order. Luckily for us, that didn't happen. On Saturday,
October 6th, 2007 The sea Gods smiled upon us. We had fair winds (little
to none), fair temperatures (summer-like), and fair seas (very calm).
And we were off; Chris Thomas, Will Means, Doug Millen and myself ~~
paddling from cape to cape.&lt;/p&gt;
&lt;p&gt;In addition to knowing the weather, we had to be able to navigate.
Navigation is critical for a journey like this. When you are completely
out of site of land it is very easy to veer off course. And when your
target is a small point of land 40 miles away you don't want that to
happen. Miss Cape Cod and your next stop is the Bahamas! But thanks to
this miricle of modern navigational science, navigation was a breeze. We
plotted our way points in advance and all we had to do was follow the
little arrow and maintain a straight course. Thank you GPS!&lt;/p&gt;
&lt;div class="ImageRack"&gt;
    &lt;a href="images/CapeToCape.jpg" target="_blank"&gt;
      &lt;img alt="Insert Image" width="200" height="200" src="/images/cape_kayakin/CapeToCape.jpg" style="margin:10px;" /&gt;
    &lt;/a&gt;
    &lt;a href="images/CapeToCapeLocator.jpg" target="_blank"&gt;
      &lt;img alt="Insert Image" width="200" height="200" src="/images/cape_kayakin/CapeToCapeLocator.jpg" style="margin:10px;" /&gt;
    &lt;/a&gt;
    &lt;a href="images/CapeToCapeTrack2.jpg" target="_blank"&gt;
      &lt;img alt="Insert Image" width="200" height="200" src="/images/cape_kayakin/CapeToCapeTrack2.jpg" style="margin:10px;" /&gt;
    &lt;/a&gt;
    &lt;p&gt;These images show the course we took. Left: naughtical map. Center: GPS way points. Right: actual course as determined by GPS.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Originally, we were going to have a chase boat ~~ a power boater was
going to follow us just in case of emergency. That put some of the folks
I was chatting with about this before-hand at ease. But it didn't end
up happening. Our chase boat couldn't follow in the end, so we just set
out ourselves in two two-person sea kayaks ~~ the Tango One and Tango
Two.&lt;/p&gt;
&lt;div class="Image"&gt;
    &lt;img alt="INSERT IMAGE" src="/images/cape_kayakin/CapeToCape003.jpg" /&gt;
    &lt;p&gt;The Tango 1 and Tango 2.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;And so it was that at approximately three o'clock in the morning on a
calm Saturday in the early fall we loaded up our boats and set off from
Stage Fort Park at the head of Gloucester Harbor. Set off for Province
Town cape cod.&lt;/p&gt;
&lt;div class='heading' &gt;
~ II ~
&lt;/div&gt;

&lt;p&gt;The early part of the paddle was eerie. Setting off in the wee hours of
the morning meant, essentially, that we were in the dark. Don't get me
wrong ~~ it was beautiful to look up and see the stars overhead. But
as the lights of the shoreline receded into the nighttime mist behind us
and the rhythm of the small waves lapping at the sides of the boat
settled in, the magnitude of what we were undertaking really started to
hit home.&lt;/p&gt;
&lt;p&gt;But we soon got accustomed to the darkness, I turned off my GPS, and set
a course by the stars. What a treat. Getting to experience first-hand
what the ancient mariners must have done, as they set about exploring
their world in the days before electronics were standard issue on almost
any sea going vessel.&lt;/p&gt;
&lt;p&gt;It wasn't so dark for long though. Almost immediately after setting out
the lights of a huge offshore rig (Keyspan energy laying a natural gas
pipeline or some such) started to loom in the distance. It seemed to
take us forever to reach that rig, but approaching it was awe-inspiring.
Here were we were, in these tiny kayaks, paddling past this vessel of
gargantuan proportions (I couldn't even count how many stories high
this thing stood off the water). The lights of the rig blazed out of the
night like the lights of a city block ~~ lighting up the sky and
blotting out the stars. As we passed the rig, a party ship shot by
behind us with a more colorful display of lights. And moving at a pretty
good clip! Images of Huckleberry Finn and Jim the slave getting run over
by a Mississippi steamboat came to mind.&lt;/p&gt;
&lt;p&gt;But that was about as much big boat traffic that we saw. Even much later
in the journey when we crossed the major shipping lanes into Boston we
didn't see another vessel larger than a fishing boat, despite earlier
concerns.&lt;/p&gt;
&lt;p&gt;Not long after we passed that rig and it too receded into the distance
dawn began to break. More than one person since has asked me to describe
what it was like to experience that sunrise.&lt;/p&gt;
&lt;div class="Image"&gt;
    &lt;img alt="INSERT IMAGE" src="/images/cape_kayakin/CapeToCape005.jpg" /&gt;
    &lt;p&gt;Dawn breaks on the ocean&lt;/p&gt;
  &lt;/div&gt;

&lt;p&gt;Well it was beautiful for sure. But what was so much more amazing to me
was being in a such a rhythm with the sea, and paddling on through that
sunrise, and on into the day, and seeing the sun ~~ not just as a
snapshot ~~ but rather watching it climb into the sky and on overhead
over the course of the day. It wasn't just the moment of the sunrise
~~ the in-between time that happens between night and day. It was the
totality and continuity of the paddling experience without the everyday
distractions of corporate life and work that most impressed me.&lt;/p&gt;
&lt;p&gt;And on we went. The seconds blended into minutes, the minutes into
hours. The Sun continued it's climb into the heavens and the waves
continued incessantly lapping at our sides.&lt;/p&gt;
&lt;div class="Image" style="display:inline-block;margin-left:auto;margin-right:auto;width:400px"&gt;
    &lt;img alt="INSERT IMAGE" width=400px src="/images/cape_kayakin/CapeToCape006.jpg" /&gt;
    &lt;p&gt;Paddlin'!&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;A slide show reflecting the experience would be funny; "Here we are on
the ocean paddling. Here we are paddling some more. And here's another
shot of ocean. And here's some more water". Really, most of the
journey was like this.&lt;/p&gt;
&lt;p&gt;By now, we were completely out of sight of land as we were for most of
the crossing. Around mid morning we stopped for our first significant
break.&lt;/p&gt;
&lt;div class="Image"&gt;
    &lt;img alt="INSERT IMAGE" src="/images/cape_kayakin/DSCF8995.jpg" width="400px" /&gt;
    &lt;p&gt;Break time.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Doug got a little worried when Chris decided to go for a swim (it
wasn't exactly 100% intentional) ~~ and a bit more worried when the
radio decided to dive in too (we only had two radios among us all, our
only contact in the event of an emergency). And when we opened our aft
hatch and found the bulkhead full of water, we started to get a bit more
worried still. But, luckily, we had enough stuff in dry bags in the rear
bulkhead to displace some of the water, so with some heavy duty pumping
Doug was able to empty it out.&lt;/p&gt;
&lt;p&gt;And so, with one of our radios sitting at a depth of 100 feet, a slow
leak in the aft bulkhead, and morale ever so slightly compromised ~~
we pressed on. No more breaks for a while!&lt;/p&gt;
&lt;p&gt;Not long after, with the sun climbing a bit higher into the sky we saw a
fishing boat heading our way. He pulled up alongside us ~~ obviously
intrigued at the prospect of seeing a small group of sea kayakers
paddling along 20 nautical miles out to sea! I had to chuckle inwardly
at his bemused expression when we told him we were in the middle of
paddling our kayaks cape to cape. That would be pretty much our only
contact with another mariner until we reached the waters of Cape Cod.&lt;/p&gt;
&lt;div class='heading' &gt;
~ III ~
&lt;/div&gt;

&lt;p&gt;Around about half way through our expedition we started to feel the
strain. I felt it in my left wrist, where the repetitive motion and
strain of the paddling started working my tendons. So I had to
continually adjust my stroke to avoid injury. Later I'd feel it in my
lower back as well. The constant rotation and stress of sitting in the
cockpit for 14 hours will inevitably take its toll.&lt;/p&gt;
&lt;p&gt;But just as we were starting to feel the strain, physically, it
happened. We were in the midst of crossing the shipping lanes (with not
a ship in sight) when we started chatting about the possibility seeing
whales. We knew we'd be passing over Stellwagen bank for much of the
crossing and we also knew that area is frequented by whales
(Massachusetts has some boats that'll take tourists out to actually see
them in the wild). So we'd been expecting, hoping, maybe that we might
catch a view of them on the trip. Will started joking around, making low
rumbling noises to "call" out to the whales.&lt;/p&gt;
&lt;p&gt;Well, there must've been something in the translation ~~ I'm gonna
call him whale whisperer from now on ~~ because not long after he
sounded off we saw them. Chris was, I think, the first to spot them
breaching on the horizon. It was a small group ~~ maybe a family of
three humpbacks feeding. We all got very exited very quick over this. It
was the most amazing experience to be sitting that close to the water
and seeing these magnificent animals approach. We were very happy as we
watched them break the surface. There's one breaching. Look at flukes
as that one sounds. Look, you can see the gaping mouth on that one as it
drinks in its briny soup.&lt;/p&gt;
&lt;style&gt;
  .video-container {
    display: inline-block;
    overflow: hidden;  
    width: 320px;       
    height: 250px;      
    margin: 0;         
    padding: 0;      
    border: inset 4px grey  
  }

  .video-container object, .video-container embed {

    margin: 0;         
    padding: 0;        
  }
&lt;/style&gt;

&lt;div class='.video-container'&gt;
    &lt;object width="320" height="250"&gt;
      &lt;param name="src" value="/video/whales_firefox.mp4" /&gt;
      &lt;param name="autoplay" value="false" /&gt;
      &lt;param name="loop" value="false" /&gt;
      &lt;param name="controller" value="true" /&gt;
      &lt;embed src="/video/whales_firefox.mp4" 
             width="320" 
             height="250" 
             autoplay="false" 
             loop="false" 
             controller="true" /&gt;
    &lt;/object&gt;
&lt;/div&gt;

&lt;p&gt;Thar she blows! If you play this movie, you should see 
the flukes of a whale as it dives and the
distinctive signiture of its humpback companions.&lt;/p&gt;
&lt;p&gt;Then they started to get closer. And closer. Maybe just a bit too close!
At one point they couldn't have been more than 100 yards away from us!
When we could actually &lt;em&gt;hear&lt;/em&gt; them making those same rumbling noises
Will had been making. Truth be told we started to get a bit nervous! I mean,
here's an animal the size of a city bus ~~ bigger even ~~
approaching you as you sit in your narrow little kayak rocking on the
waves. You have to respect that. At one point I said to Doug who was
eagerly trying to get some good shots of the whales; "you might wanna
put the camera away and get ready to brace!".&lt;/p&gt;
&lt;p&gt;But 100 yards or so was about as close as they got. Eventually they went
their way and we continued on ours. The experience of paddling with
whales gave us our second wind and on we went. We saw a lot more
wildlife along the way ~~ breaking the surface of water. We saw
schools of tuna ~~ huge fish leaping clean out of the water. At one
point we saw a pod of dolphins porposing along at great speed with some
purpose evident that we would just not fathom. And birds. A couple of
them quite strange ~~ looking like a cross between a pelican and an
albatross.&lt;/p&gt;
&lt;p&gt;And, we saw the shark. It was pretty funny really. I just looked over at
Chris and Will at one point and saw a couple of dorsal fins breaking the
surface. Dolphins maybe? No, wait, they're not porposing. Then,
suddenly I realized ~~ it wasn\'t two animals, but rather it was one!
The first fin was the dorsal fin and the second was the tale fin of a
shark! That animal must've exceeded 10 feet in length if it was an
inch! We suspect it might've been a blue shark, but can't rule out the
possibility it was a mako. But all he did was cross our path and move
along his way ~~ in his endless search for his next meal.&lt;/p&gt;
&lt;div class='heading' &gt;
~ IV ~
&lt;/div&gt;

&lt;p&gt;Not long afterward we reached the fishing grounds off of cape cod. When
we had to paddle through a fleet of fishing boats (anchored just out of
sight of land) we knew we were reaching the end of our journey. The last
8 miles were the most grueling. The sun was high in the sky beating down
on us ~~ raising sweat on my brow and causing the sunscreen to sting
my eyes. The mild tendonitis started to flare up again. And the pace,
which had been a steady 3.5 ~ 3.7 knots for most of the trip began to
slow.&lt;/p&gt;
&lt;p&gt;But then, through the late afternoon haze, we saw the faintest shadow of
a coastline and we knew we'd made it. The cape seemed just to pop out
at us, and soon we began to make out details like the small tower in
Province Town a monument visible just over Race Point beach. We were
there. 13 odd hours and 41 nautical miles (47 statute) and we were
there.&lt;/p&gt;
&lt;div class="Image" style="display:inline-block;margin-left:auto;margin-right:auto;width:400px"&gt;
    &lt;img alt="INSERT IMAGE" src="/images/cape_kayakin/CapeToCape031.jpg" /&gt;
    &lt;p&gt;Land Ho'!&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Rounding the point and paddling to the takeout was probably the most
grueling part of the trip. But we'd made it! And that simple fact gave
us the energy to put forth that final burst of speed as we paddled up to
the shore and touched land for the first time in 14 hours. There were
many people on the beach, enjoying a lazy afternoon on the Cape on that
warm fall day. The sun was sinking now ~~ approaching the horizon on
Cape Cod bay.&lt;/p&gt;
&lt;div class="Image" style="display:inline-block;margin-left:auto;margin-right:auto;width:400px"&gt;
    &lt;img alt="INSERT IMAGE" src="/images/cape_kayakin/CapeToCape037.jpg" /&gt;
    &lt;p&gt;Landing at Race Point&lt;/p&gt;
  &lt;/div&gt;

&lt;p&gt;A couple of guys staggered up to us ~~ it's not every day you see two
21 foot 2 man kayaks pull up onto the beach I guess.&lt;/p&gt;
&lt;p&gt;"Where'd you guys pull in from" they asked? "Oh, we just took off
from Cape Ann. Gloucester you know." They were incredulous at first.
"Naw ~~ you're kidding right?" But then they saw the conviction in
our eyes. They sensed the aura you exude when you've just paddled 41
miles across the open sea. And they new we'd done it.&lt;/p&gt;
&lt;div class='heading' &gt;
~ Epilogue ~
&lt;/div&gt;

&lt;p&gt;My buddy Bill Schoolcraft picked us up down there at Race Point beach
not long after we arrived. The sun was setting by then ~~ simply
beautiful. We were a bit surprised at how great we felt. Sure it was a
long paddle, and it tested our endurance at the end. But we felt
surprisingly good as we packed up our gear and loaded up the van to head
for home.&lt;/p&gt;
&lt;div class="Image"&gt;
    &lt;img alt="INSERT IMAGE" src="/images/cape_kayakin/CapeToCape053.jpg" /&gt;
    &lt;p&gt;Sunset over Cape Cod Bay.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We were all excited as we headed out from Province Town that evening.
Full of our adventure. The blackness of the morning launch was far
behind us now. The uncertainty we'd felt was gone ~~ replaced by the
accomplishment of having paddled cape to cape.&lt;/p&gt;
&lt;div class='heading' &gt;
~ Links ~
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.nspn.org/forum/index.php?showtopic=4025"&gt;Chris's entry in NSPN Paddler's forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nspn.org/forum/index.php?showtopic=4039"&gt;Will's entry in the forum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/assets/Cape2CapebyWill.pdf"&gt;Will's Writeup (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nekayaking.com/trips/capetocape.htm"&gt;New Enland Kayaking (Doug's Account)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Blog"></category><category term="kayak"></category><category term="adventure"></category><category term="travel"></category><category term="paddling"></category><category term="whales"></category><category term="cape anne"></category><category term="cape cod"></category><category term="massachusetts"></category><category term="sea kaying"></category></entry></feed>